{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"About","n":1},"1":{"v":"\nSoftware Engineer @ LG Electronics\n\n\n## Certificates\n\nWhat|When\n----|----\n데이터분석 전문가 (ADP) - 필기 |2023.03.24\nDatabricks Certified Data Engineer Associate (Version 2) |2022.12.10\nAcademy Accreditation - Databricks Lakehouse Fundamentals |2022.11.30\nDatabricks Certified Associate Developer for Apache Spark 3.0 - Python |2022.11.27\n빅데이터 분석기사 |2022.07.15\n데이터분석 준전문가 (ADsP) |2021.09.24\nMicrosoft Azure Data Scientist Associate |2021.08.06\nMicrosoft Azure AI Fundamentals |2021.07.24\nMicrosoft Azure Fundamentals |2021.07.16\n\n## Baekjoon Online Judge\n- https://www.acmicpc.net/user/lovil","n":0.135}}},{"i":2,"$":{"0":{"v":"Dev","n":1}}},{"i":3,"$":{"0":{"v":"Web","n":1}}},{"i":4,"$":{"0":{"v":"Notion과 Next.js로 웹사이트 만들기","n":0.5},"1":{"v":"\nhttps://github.com/transitive-bullshit/nextjs-notion-starter-kit\n\nhttps://gongheung.com","n":1}}},{"i":5,"$":{"0":{"v":"CORS(Cross Origin Resource Sharing)","n":0.5},"1":{"v":"\n## What\n클라이언트 웹 애플리케이션이 다른 도메인에 있는 리소스와 상호 작용할 수 있도록 접속을 허용하는 것\n\n## Why\n현대의 모든 브라우저는 인터넷 초창기의 크로스 사이트 요청 위조(CSRF) 문제를 해결하기 위해 *동일 오리진 정책*을 구현함\n- 동일 오리진 정책: 클라이언트는 클라이언트의 URL과 동일한 오리진의 리소스로만 요청을 보낼 수 있음; 즉, 클라이언트 URL의 프로토콜, 포트 및 호스트 이름은 모두 클라이언트에서 요청하는 서버와 일치해야 함\n\n동일 오리진 정책은 매우 안전하지만 실제 사용 사례에는 유연하지 않음\n\n크로스 오리진 리소스 공유(CORS)는 동일 오리진 정책을 확장하여 외부의 서드 파티의 승인 하에 오리진 밖의 리소스를 공유하도록 함\n\n## How\n서버 측에서는 허용된 오리진 목록에 오리진을 추가하여 사용자에게 액세스 권한을 부여\n\n### 프로세스\n1. 브라우저는 요청에 오리진 헤더에 오리진을 추가\n2. 서버는 오리진 헤더를 확인하고 응답시 요청된 데이터와 함께 Access-Control-Allow-Origin 헤더로 응답\n3. 브라우저는 액세스 제어 요청 헤더를 확인한 후 반환된 데이터를 클라이언트 애플리케이션과 공유\n\n## Reference\nhttps://aws.amazon.com/ko/what-is/cross-origin-resource-sharing/","n":0.089}}},{"i":6,"$":{"0":{"v":"(TBC)","n":1},"1":{"v":"\n\nFlink 스트림 프로세싱 프레임워크\nhttps://www.samsungsds.com/kr/insights/flink.html\n\nNAVER DEVIEW 2021\nhttps://deview.kr/2021/sessions\n\nDatabricks Academy 예제\nhttps://adb-892793297169985.5.azuredatabricks.net/?o=892793297169985#notebook/4447473277740608/command/4447473277740609\nhttps://files.training.databricks.com/distributions/lakehouse-with-delta-lake-deep-dive/summit-2022-06/\n\n\n\nhttps://files.training.databricks.com/distributions/databricks-lakehouse-overview/summit-2022-06/Databricks-Lakehouse-Overview.pdf\n\nhttps://files.training.databricks.com/distributions/databricks-lakehouse-overview/summit-2022-06/\n\njupyter notebook remote 접속 허용 설정\nhttps://teddylee777.github.io/python/jupyternotebook-remote-%EC%A0%91%EC%86%8D%ED%97%88%EC%9A%A9%ED%95%98%EA%B8%B0\n\nhttps://github.com/awesome-selfhosted/awesome-selfhosted\n\n백준 10986 나머지 합\nhttps://zoosso.tistory.com/550\n\nhttps://tarra.tistory.com/entry/%EB%B0%B1%EC%A4%80-15810%EB%B2%88-%ED%92%8D%EC%84%A0-%EA%B3%B5%EC%9E%A5-python\n\n이진 탐색 트리\nhttps://nomad-programmer.tistory.com/376\n\n백준 1300 K번째 수\nhttps://cocoon1787.tistory.com/292\n\n백준 18877\nhttps://devbelly.tistory.com/93\n\n백준 1285 동전 뒤집기\nhttps://chanho0912.tistory.com/43\n\n\nhttps://www.speedcoder.net/lessons/py/1/","n":0.2}}},{"i":7,"$":{"0":{"v":"Software Engineering","n":0.707}}},{"i":8,"$":{"0":{"v":"Theory","n":1},"1":{"v":"\n\n## 제어 역전 \n\n### What\n\n- **IoC**(Inversion of Control)\n- 프로그래머가 작성한 프로그램이 재사용 라이브러리의 흐름 제어를 받게 되는 소프트웨어 디자인 패턴\n- 전통적인 프로그래밍에서 흐름은 프로그래머가 작성한 프로그램이 외부 라이브러리의 코드를 호출해 이용\n- 제어 역전이 적용된 구조에서는 외부 라이브러리의 코드가 프로그래머가 작성한 코드를 호출\n\n### Why\n\n- 작업을 구현하는 방식과 작업 수행 자체를 분리\n- 모듈을 제작할 때, 모듈과 외부 프로그램의 결합에 대해 고민할 필요 없이 모듈의 목적에 집중할 수 있음\n\n\n### References\n- https://ko.m.wikipedia.org/wiki/%EC%A0%9C%EC%96%B4_%EB%B0%98%EC%A0%84\n\n\n## 의존관계 역전 원칙\n\n### What\n\n- 객체 지향 프로그래밍에서 소프트웨어 모듈들을 분리하는 특정 형식을 지칭\n- 이 원칙을 따르면, 상위 계층(정책 결정)이 하위 계층(세부 사항)에 의존하는 전통적인 의존 관계를 반전(역전)시킴으로써 상위 계층이 하위 계층의 구현으로부터 독립되게 할 수 있음\n- 이 원칙은 '상위와 하위 객체 모두가 동일한 추상화에 의존해야 한다'는 객체 지향적 설계의 대원칙을 제공\n    - 첫째, 상위 모듈은 하위 모듈에 의존해서는 안된다. 상위 모듈과 하위 모듈 모두 추상화에 의존해야 한다.\n    - 둘째, 추상화는 세부 사항에 의존해서는 안된다. 세부사항이 추상화에 의존해야 한다.\n\n### References\n- https://ko.m.wikipedia.org/wiki/%EC%9D%98%EC%A1%B4%EA%B4%80%EA%B3%84_%EC%97%AD%EC%A0%84_%EC%9B%90%EC%B9%99","n":0.082}}},{"i":9,"$":{"0":{"v":"Security","n":1}}},{"i":10,"$":{"0":{"v":"SSL/TLS","n":1},"1":{"v":"\n## What\nSSL(Secure Sockets Layer)과 TLS(Transport Layer Security)는 데이터 전송 중 보안과 데이터 무결성을 확보하기 위해 설계된 암호화 프로토콜이다. 웹 브라우저와 서버 간에 전송되는 데이터를 암호화하여 중간자 공격(man-in-the-middle attack)이나 데이터 스니핑(eavesdropping) 같은 공격을 방지하게 된다.\n\n## How\n최초 연결 설정 시에 서버와 클라이언트 간에 '암호화 키 교환'이 이루어진다. 이 키는 세션 동안 데이터 암호화에 사용되며, 세션이 종료되면 폐기된다.\n\n1. **핸드쉐이크**: \n이 과정에서 클라이언트와 서버는 암호화를 위한 파라미터를 교환하며, 통신에 사용할 암호화 방식을 결정한다.\n2. **서버 인증**:\n핸드쉐이크 과정 중, 서버는 클라이언트에게 공개 키가 포함된 인증서를 제공한다. 이 인증서는 보통 신뢰할 수 있는 제3자인 인증기관(CA)[^1]에 의해 서명된다.\n3. **세션 키 생성**:\n클라이언트는 이 공개 키를 사용하여 임의의 세션 키를 암호화하고 서버로 보낸다.\n4. **세션 키 복호화**:\n서버는 자신만이 알고 있는 개인 키를 사용하여 암호화된 세션 키를 복호화한다. 이제 클라이언트와 서버 모두 동일한 세션 키를 갖게 된다.\n5. **암호화된 통신**:\n이 세션 키는 후속 통신의 대칭키 암호화에 사용된다. 클라이언트와 서버는 이 키를 사용하여 데이터를 암호화 및 복호화한다.\n\n[^1]: CA(Certificate Authority)는 디지털 인증서의 발급, 관리, 저장, 회수 그리고 만료 관리를 수행하는 신뢰할 수 있는 제3자 조직이다.","n":0.079}}},{"i":11,"$":{"0":{"v":"Authentication vs Authorization","n":0.577},"1":{"v":"\n## What\n\n### 인증(authentication)\n사용자가 누구인지 확인하는 단계 (예: 로그인)\n\n### 인가(authorization)\n인증을 통해 검증된 사용자가 애플리케이션 내부의 리소스에 접근할 때 사용자가 해당 리소스에 접근할 권리가 있는지를 확인하는 과정 (예: 로그인한 사용자가 특정 게시판에 접근해서 글을 보려고 하는 경우 게시판 접근 등급을 확인해 접근을 허가하거나 거부하는 것)\n\n## How\n- 일반적으로 사용자가 인증 단계인 로그인에 성공하면 애플리케이션 서버는 응답으로 사용자에게 토큰(token)을 전달하는데, 이 토큰 안에는 인가 내용을 포함하고 있음\n- 사용자가 사용자가 리소스에 접근하면서 토큰을 함께 전달하면 애플리케이션 서버는 토큰을 통해 권한 유무 등을 확인해 인가를 수행","n":0.112}}},{"i":12,"$":{"0":{"v":"Programming","n":1}}},{"i":13,"$":{"0":{"v":"Vim","n":1},"1":{"v":"\n\nFind and Replace in Vim<br>https://linuxize.com/post/vim-find-replace/\n","n":0.447}}},{"i":14,"$":{"0":{"v":"Functional Programming","n":0.707},"1":{"v":"\n\n## 특징\n\n- PURE\n- DECLARATIVE\n  - DECLARE 'WHAT' THE DESIRED RESULT IS\n- IMMUTABLE\n  - Object.freeze\n  - Immutable.js\n- FIRST CLASS FUNCTIONS\n- CLOSURES (ENCAPSULATION)\n```js\nconst createAdder = (x) => {\nreturn (y) => x + y;\n};\n\nconst add3 = createAdder(3);\n\nadd3(2) === 5;\nadd3(3) === 6;\n```\n- CURRYING\n```js\n  const add = x => y => x + y;\n  \n  function add(x) {\n  \treturn function(y) {\n  \t\treturn x + y;\n  \t};\n  }\n```\n\n## 특징 (2)\n\n- Declarative pattern\n  - Imperative programs spend lines of code describing the specific steps used to achieve the desired results — the flow control: How to do things.\n  - Declarative programs abstract the flow control process, and instead spend lines of code describing the data flow: What to do. The how gets abstracted away.\n  - Array Functions : for devlarative pattern\n    - filter\n    - find\n    - mao\n    - reduce\n    - every\n    - some\n\n\n## 핵심 개념\n\n### Pure function\n  - Given the same inputs, always returns the same output, and .. 참조 투명성 (Referential transparency)\n  - Has no side-effects .. (Side-effect free)\n### Function composition\n  - the process of combining two or more functions in order to produce a new function or perform some computation\n  - Function Chaining\n### Avoid shared state\n  - Shared state\n    - any variable, object, or memory space that exists in a shared scope, or as the property of an object being passed between scopes\n### Avoid mutating state\n  - Immutability\n    - An immutable object is an object that can’t be modified after it’s created. Conversely, a mutable object is any object which can be modified after it’s created.\n    - Immutable Libraries\n      - Seamless-immutable\n      - Immutable JS\n### Avoid side effects\n  - A side effect is any application state change that is observable outside the called function other than its return value. \n  - Haskell and other functional languages frequently isolate and encapsulate side effects from pure functions using monads.\n\n## How?\n\n### 고계함수\n- A higher order function is any function which takes a function as an argument, returns a function, or both.\n\n## Libraries to support FP\n\n- lodash\n- RamdaJS\n- RxJS\n\n\n## 교재\n\n- https://github.com/luijar/functional-programming-js\n\n## Articles\n\n- http://hughfdjackson.com/javascript/why-curry-helps/","n":0.056}}},{"i":15,"$":{"0":{"v":"Snippets","n":1},"1":{"v":"\n\nSnippet\n\n```js\n{\n    \"user\": \"hughfdjackson\",\n    \"posts\": [\n        { \"title\": \"why curry?\", \"contents\": \"...\" },\n        { \"title\": \"prototypes: the short(est possible) story\", \"contents\": \"...\" }\n    ]\n}\n\nvar curry = require('curry')\n\n// curry는 교재 p34, p129\n// get :: String -> Object -> String\nvar get = R.curry((property, object) => object[property])\n// map :: Function -> Object -> Object (?)\nvar map = R.curry((fn, value) => value.map(fn))\n\nfetchFromServer()\n\t.then(JSON.parse)\n\t.then(get('posts'))\n\t.then(map(get('title'))\n\n\nfetchFromServer()\n.then(R.prop('posts'))\n.then(R.map(R.props('title')))\n.then(element => console.log(element))\"\n```\n\n## dragons.js\n```js\n  \"const R = require('ramda')\n  \n  let dragons = [\n    { name: 'fluffykins', element: 'lightning'},\n    { name: 'noomi',      element: 'lightning'},\n    { name: 'karo',       element: 'fire'},\n    { name: 'doomer',     element: 'timewarp'},\n  ]\n  \n  // let hasElement = (element, obj) => obj.element === element\n  // let lightDragons = dragons.filter(x => hasElement('lightning', x))\n  \n  // let hasElement = R.curry((element, obj) => obj.element === element)\n  // let lightDragons = dragons.filter(hasElement('lightning'))\n  \n  let hasElement = R.propEq('element', 'lightning')\n  let lightDragons = R.filter(hasElement, dragons)\n  \n  console.log(lightDragons)\n```\n\n## curry\n```js\n  getOnClick = (index) => (event) => message = buttons[index]\n```\n\n## 일급시민\n```js\n  const multiply = (x, y) => x + y;\n  \n  function add(x, y) {\n  \treturn x + y;\n  }\n  \n  const addAlias = add;\n  \n  const evens = [1, 2, 3].map(n => n * 2);\n```\n\n## closure\n```js\n  const add = x => y => x + y;\n  \n  function add(x) {\n  \treturn function(y) {\n  \t\treturn x + y;\n  \t};\n  }\n  \n  // ----\n  \n  const createAdder = (x) => {\n  \treturn (y) => x + y;\n  };\n  \n  const add3 = createAdder(3);\n  \n  add3(2) === 5;\n  add3(3) === 6;\n  // closure는 이렇게 RECALL 가능\n  \n  // ----\n  \n  const request = (options) => {\n  \treturn fetch(options.url, options)\n  \t\t.then(res => res.json());\n  };\n  \n  const usersPromise = request({\n  \turl: '/users',\n  \theaders: { 'X-Custom': 'mykey' }\n  });\n  \n  const tasksPromise = request({\n  \turl: '/tasks',\n  \theaders: { 'X-Custom': 'mykey' }\n  });\n  \n  ==>\n  \n  const createRequester = (options) => {\n  \treturn (otherOptions) => {\n  \t\treturn request(Object.assign({}, options, otherOptions));\n  \t};\n  };\n  \n  const customRequest = createRequester({\n  \theaders: { 'X-Custom': 'mykey' }\n  });\n  \n  const usersPromise = customRequest({ url: '/users' };\n  const tasksPromise = customRequest({ url: '/tasks' });\n```\n\n## partial application\n```js\n  const add = (x, y) => x + y;\n  \n  const add3 = partial(add, 3);\n  \n  add3(2) === 5;\n  \n  \n  const request = (defaults, options) => {\n  \toptions = Object.assign({}, defaults, options);\n  \n  \treturn fetch(options.url, options)\n  \t\t.then(res => res.json());\n  };\n  \n  const customRequest = partial(request, {\n  \theaders: { 'X-Custom': 'mykey' }\n  });\n  \n  const usersPromise = customRequest({ url: '/users' };\n  const tasksPromise = customRequest({ url: '/tasks' });\n```\n\n## curry\n```js\n  const request = defaults => options => {\n  \toptions = Object.assign({}, defaults, options);\n  \n  \treturn fetch(options.url, options)\n  \t\t.then(res => res.json());\n  };\n```\n\n## Shoping Cart\n```js\n  const map = fn => arrray => array.map(fn);\n  const multiply = x => y => x * y;\n  const pluck = key => object => object[key];\n  \n  const discount = multiply(0.98);\n  const tax = multiply(1.0925);\n  \n  const customRequest = request({\n  \theaders: { 'X-Custom': 'mykey' }\n  });\n  \n  customRequest( { url: '/cart/items' })\n  \t.then(map(pluck('price')))\n  \t.then(map(discount))\n  \t.then(map(tax));\n  \n  customRequest( { url: 'cart/items' })\n  \t.then(map(compose(tax, discount, pluck('price')));\n```\n\n## Recursion\n```js\n  const factorial = (n) => {\n  \tif (n < 2) {\n  \t\treturn 1;\n  \t}\n  \t\n  \treturn n * factorial(n - 1);\n  };\n  \n  OPTIMIZABLE\n  \n  const factorial = (n, accum = 1) => {\n  \tif (n < 2) {\n  \t\treturn accum;\n  \t}\n  \n  \treturn factorial(n - 1, n * accum);\n  };\n  \n  factorial(4);\n  4 * factorial(3);\n  4 * 3 * factorial(2);\n  4 * 3 * 2 * factorial(1);\n  4 * 3 * 2 * 1;\n  4 * 3 * 2;\n  4 * 6;\n  24;\n```\n\n## curryRight\n```js\n  greaterThanOrEqual = (a, b) => a >= b\n  const greaterThanOrEqualTo = _.curryRight(greaterThanOrEqual)\n  \n  let thirtyDaysAgo = (new Date()).getTime() - (86400000 * 30)\n  const within30Days = greaterThanOrEqualTo(thirtyDaysAgo)\n```\n\n## Utils\n```js\n  const getTime = (dateString) => (new Date(dateString)).getTime()\n  \n  greaterThanOrEqual = (a, b) => a >= b\n  const greaterThanOrEqualTo = _.curryRight(greaterThanOrEqual)\n  \n  let thirtyDaysAgo = (new Date()).getTime() - (86400000 * 30)\n  const within30Days = greaterThanOrEqualTo(thirtyDaysAgo)\n```\n\n## Pure function\n```js\n  <참조 투명성>\n  const add = (x, y) => x + y;\n  \n  add(2, 3) === 5;\n  add(2, 3) === 5;\n  add(2, 3) === 5;\n  \n  <IMPURE>\n  let name = 'Jeremy';\n  \n  const getName = () => name;\n  \n  const setName = (newName) => {\n  \tname = newName;\n  };\n  \n  const printUpperName = () => {\n  \tconsole.log(name.toUpperCase());\n  };\n  \n  //----\n  \n  function doubleNumbers(numbers) {\n  \tconst doubled = [];\n  \tconst l = numbers.length;\n  \n  \tfor (let i = 0; i < l; i++) {\n  \t\tdoubled.push(numbers[i] * 2);\n  \t)\n  \n  \treturn doubled;\n  }\n  \n  doubledNumber([1, 2, 3]); // [2, 4, 6]\n  \n  // ----\n  \n  function doubleNumbers(numbers) {\n  \treturn numbers.map(n => n * 2);\n  }\n  \n  doubleNumbers([1, 2, 3]); // [2, 4, 6]\n```\n\n## Immutable\n```js\n  <MUTABLE>\n  \n  const hobbies = [\n  \t'programming',\n  \t'reading',\n  \t'music'\n  ];\n  \n  const firstTwo = hobbies.splice(0, 2);\n  \n  console.log(firstTwo); // ['programming', 'reading']\n  \n  console.log(hobbies); // ['music']\n  \n  ==>\n  \n  const const hobbies = Object.freeze([\n  \t'programming',\n  \t'reading',\n  \t'music'\n  ]);\n  \n  const firstTwo = hobbies.splice(0, 2); // TypeError\n  \n  // ----\n  \n  class Point {\n  \tconstructor(x, y) {\n  \t\tthis.x = x;\n  \t\tthis.y = y;\n  \t}\n  \t\n  \tmoveBy(dx, dy) {\n  \t\tthis.x += dx;\n  \t\tthis.y += dy;\n  \t}\n  }\n  \n  const point = new Point(0, 0);\n  \n  point.moveBy(5, 5);\n  point.move(-2, 2);\n  \n  console.log([point.x, point.y]); // [3, 7]\n  \n  ==>\n  \n  const createPoint = (x, y) => Object.freeze([x, y]);\n  \n  const movePointBy = ([x, y], dx, dy) => {\n  \treturn Object.freeze([x + dx, y + dy]);\n  };\n  \n  let point = createPoint(0, 0);\n  \n  point = movePointBy(point, 5, 5);\n  point = movePointBy(point, -2, 2);\n  \n  console.log(point); [3, 7]\n```\n","n":0.033}}},{"i":16,"$":{"0":{"v":"Ramda.js","n":1},"1":{"v":"\n\n### Examples\n\n```js\nR.map(([k, v]) => global[k] = v, R.toPairs(R));\n```\n\n\n### ramda_intro.js\n```js\n  const R = require('ramda')\n  const fs = require('fs')\n  \n  const readFileSync = path => fs.readFileSync(path, { encoding: 'utf-8' })\n  \n  const readData = R.pipe(\n    R.converge(R.concat, [\n      R.pipe(  \n        R.replace(/.*\\//, ''),\n        R.replace(/\\..*/, ''),\n        R.concat('date\\t'),\n        R.concat(R.__, '\\n')\n      ),\n      readFileSync\n    ])\n  )\n  \n  R.pipe(\n    R.map(R.pipe(\n      readData,\n      R.split('\\n'),\n      R.map(R.pipe(\n        R.split('\\t'),\n        R.view(R.lensIndex(1))\n      )),\n      R.converge(R.map, [\n        R.pipe(R.head, R.objOf),\n        R.tail\n      ])\n    )),\n    R.transpose,\n    R.map(R.mergeAll),\n    console.log\n  )(['./btc.csv', './eth.csv'])\n  \n  \n  R.pipe(\n    R.map(R.pipe(\n      readData,\n      R.split('\\n'),\n      R.map(R.pipe(\n        R.split('\\t'),\n        R.take(2),\n      )),\n    )),\n    R.transpose,\n    R.map(R.pipe(\n      R.flatten,\n      R.uniq,\n      R.map(R.ifElse(isNumeric, toNumber, R.identity)),\n      R.map(R.ifElse(R.head, R.toUpper, R.identity))\n    )),\n    console.log\n  )(['./btc.csv', './eth.csv'])\n```\n\n### ramda_lesson01.js\n```js\n  const R = require('ramda');\n  R.map(([k, v]) => global[k] = v, R.toPairs(R));\n  \n  const permissions = {\n    'group1-perm1': true,\n    'group1-perm2': false,\n    'group2-perm1': false,\n    'group2-perm2': true,\n    'perm3': true,\n    'perm4': false\n  };\n  \n  // {\n  //   group1: [\n  //     { value: 'group1-perm1', checked: true, 'label': 'perm1' }]\n  //   ]\n  // }\n  \n  const GROUP_LEN = 6;\n  const KEYS = ['value', 'checked', 'label'];\n  const inGroup = str => str.indexOf('-') === GROUP_LEN;\n  const getLabel = compose(last, splitAt(GROUP_LEN + 1));\n  const addLabel = chain(append, compose(ifElse(inGroup, getLabel, identity), head));\n  const convert = compose(zipObj(KEYS), addLabel);\n  \n  const groupName = ifElse(compose(inGroup, prop('value')),\n    compose(head, splitAt(GROUP_LEN), prop('value')),\n    () => 'general'\n  );\n  \n  const fn = compose(groupBy(groupName), map(convert), toPairs);\n  \n  console.log(fn(permissions));\n```\n\n### ramda_lesson04.js\n```js\n  const R = require('ramda');\n  \n  const perms = { \n    \"group1\": [ \n      { \"value\": \"group1-perm1\", \"checked\": true, \"label\": \"perm1\" },\n      { \"value\": \"group1-perm2\", \"checked\": false, \"label\": \"perm2\" }\n    ],\n    \"group2\": [ \n      { \"value\": \"group2-perm1\", \"checked\": false, \"label\": \"perm1\" },\n      { \"value\": \"group2-perm2\", \"checked\": true, \"label\": \"perm2\" } \n    ],\n    \"general\": [ \n      { \"value\": \"perm3\", \"checked\": true, \"label\": \"perm3\" },\n      { \"value\": \"perm4\", \"checked\": false, \"label\": \"perm4\" },\n      { \"value\": \"perm5\", \"checked\": true, \"label\": \"perm5\" } \n    ] \n  };\n  \n  const fn = R.compose(\n    R.mergeAll,\n    R.map(p => ({ [p.value]: p.checked })),\n    R.flatten, \n    R.map(R.nth(1)), \n    R.toPairs);\n  \n  console.log(JSON.stringify(fn(perms)));\n```\n\n```\n- BTCETH\n  \"[ [ 'DATE', 'BTC', 'ETH' ],\n    [ 'JUL 26, 2018', 8176.85, 472.33 ],\n    [ 'JUL 25, 2018', 8379.66, 479.91 ],\n    [ 'JUL 24, 2018', 7716.51, 451.14 ],\n    [ 'JUL 23, 2018', 7414.71, 459.44 ],\n    [ 'JUL 22, 2018', 7417.8, 462.44 ],\n    [ 'JUL 21, 2018', 7352.72, 450.68 ],\n    [ 'JUL 20, 2018', 7467.4, 469.31 ],\n    [ 'JUL 19, 2018', 7378.2, 480.63 ],\n    [ 'JUL 18, 2018', 7315.32, 500.84 ],\n    [ 'JUL 17, 2018', 6739.65, 480.08 ],\n    [ 'JUL 16, 2018', 6357.01, 450.43 ],\n    [ 'JUL 15, 2018', 6272.7, 435.88 ],\n    [ 'JUL 14, 2018', 6247.5, 434.51 ],\n    [ 'JUL 13, 2018', 6235.03, 430.74 ],\n    [ 'JUL 12, 2018', 6396.78, 446.5 ],\n    [ 'JUL 11, 2018', 6330.77, 434.52 ],\n    [ 'JUL 10, 2018', 6739.21, 476.16 ],\n    [ 'JUL 09, 2018', 6775.08, 488.88 ],\n    [ 'JUL 08, 2018', 6857.8, 492.07 ],\n    [ 'JUL 07, 2018', 6668.71, 474.06 ],\n    [ 'JUL 06, 2018', 6638.69, 474.36 ],\n    [ 'JUL 05, 2018', 6599.71, 467.29 ],\n    [ 'JUL 04, 2018', 6550.87, 464.15 ],\n    [ 'JUL 03, 2018', 6596.66, 475.39 ],\n    [ 'JUL 02, 2018', 6380.38, 453.82 ],\n    [ 'JUL 01, 2018', 6411.68, 455.24 ],\n    [ 'JUN 30, 2018', 6214.22, 436.21 ],\n    [ 'JUN 29, 2018', 5898.13, 422.59 ],\n    [ 'JUN 28, 2018', 6153.16, 442.29 ],\n    [ 'JUN 27, 2018', 6084.4, 432.24 ] ]\"\n```\n\n### countBy\n```js\n  const fn = R.compose(\n    R.map(R.compose(R.flatten, R.over(R.lensIndex(0), R.split(',')))),\n    R.toPairs,\n    R.countBy(R.identity)\n  )\n  console.log(fn(data))\n  \n  const data = [\n    [\"0722\", \"ID_1\"],\n    [\"0722\", \"ID_3\"],\n    [\"0722\", \"ID_1\"],\n    [\"0723\", \"ID_3\"],\n    [\"0723\", \"ID_2\"],\n    [\"0724\", \"ID_1\"],\n    [\"0724\", \"ID_3\"],\n    [\"0724\", \"ID_2\"],\n    [\"0724\", \"ID_1\"],\n    [\"0724\", \"ID_3\"]\n  ]\n```\n\n### Ramda Tutorial\n  - \"https://github.com/knowthen/ramdatutorial\"\n","n":0.041}}},{"i":17,"$":{"0":{"v":"Favorates","n":1},"1":{"v":"\n## Open Source\n\n||What|Where\n-|----|-----\nFunctional Programming in JavaScript | | https://github.com/luijar/functional-programming-js\nRamda.js| A practical functional library for JavaScript programmers | https://ramdajs.com/\nramda.py | Python clone of Ramda.js | https://github.com/slavaGanzin/ramda.py\nOSlash  | Functors, Applicatives, And Monads in Python | https://github.com/dbrattli/OSlash","n":0.171}}},{"i":18,"$":{"0":{"v":"OS","n":1}}},{"i":19,"$":{"0":{"v":"Ubuntu","n":1}}},{"i":20,"$":{"0":{"v":"Pip","n":1},"1":{"v":"\n```bash\npip --version\n\npip install --upgrade pip\n\npip list -v\n```","n":0.378}}},{"i":21,"$":{"0":{"v":"How to fix pip path?","n":0.447},"1":{"v":"\n## 문제\n`pip`로 패키지를 설치 후 실행 시 패키지를 찾지 못함\n\n## 관찰\n`pip install`로 패키지 설치 시 아래와 같은 로그 확인\n```\nInstalling collected packages: pakage-abc\n  WARNING: The script package-abt is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nSuccessfully installed package-abc-1.2.3\n```\n\n## 해결\n```bash\necho \"export PATH=\\\"/home/ubuntu/.local/bin:\\$PATH\\\"\" >> ~/.bashrc && source ~/.bashrc\n```\n\n## References\n- https://askubuntu.com/questions/1242234/in-ubuntu-20-04-how-to-fix-pip-pat","n":0.126}}},{"i":22,"$":{"0":{"v":"Mac","n":1},"1":{"v":"\n* How to combine images into one pdf file on a Mac\n    - https://www.howtogeek.com/247879/how-to-combine-images-into-one-pdf-file-on-a-mac/\n\n* Use Preview to combine PDFs on your Mac\n    - https://support.apple.com/en-us/HT202945\n\n* 원격 컴퓨터가 Mac에 접근하도록 허용하기\n    - https://support.apple.com/ko-kr/guide/mac-help/mchlp1066/mac\n\n* How to Install and Use Wget on Mac\n    - https://www.fo\n\n* Error: EACCES: permission denied, access '/usr/local/lib/node_modules'\n    - https://stackoverflow.com/questions/48910876/error-eacces-permission-denied-access-usr-local-lib-node-modules","n":0.143}}},{"i":23,"$":{"0":{"v":"Linux","n":1}}},{"i":24,"$":{"0":{"v":"Ops","n":1}}},{"i":25,"$":{"0":{"v":"Kubernetes","n":1},"1":{"v":"\n쿠버네티스는 컨테이너(예: 도커)화된 애플리케이션이 실행되는 여러 머신을 하나의 리소스 풀로 취급하여 애플리케이션의 배포, 확장 및 관리를 자동화하는 오픈소스 컨테이너 오케스트레이션 프레임워크다. 서비스 디스커버리, 로드 밸런싱, 자동 복구, 볼링 업데이트 등의 기능을 제공한다.\n\n## 주요 기능\n\n- **리소스 관리**: 여러 머신을 CPU, 메모리, 스토리지 볼륨을 묶어 놓은 하나의 리소스 풀로 취급함 \n- **스케줄링**: 컨테이너를 실행할 머신을 선택\n    - 스케줄링은 기본적으로 컨테이너의 리소스 요건 및 노드별 리소스 상황에 따라 결정됨\n    - 또 유사성(affinity)을 찾아내 여러 컨테이너를 같은 노드에 배치하거나, 반대로 반유사성(anti-affinity)을 발견하여 컨테이너를 다른 노드에 옮김\n- **서비스 관리**: \n    - 마이크로서비스에 직접 매핑되는 서비스를 명명하고 버저닝\n    - 정상 인스턴스를 항상 적정 개수만큼 가동시키고 요청 부하를 인스턴스에 고루 분산\n    - 서비스를 롤링 업데이트(rolling update)[^1]하는 기능도 있어서 구 버전으로 바로 롤백할 수 있음\n\n> 쿠버네티스는 원하는 개수의 서비스 인스턴스를 실행하라고 지시하면 서비스 인스턴스나 머신이 깨지더라도 항상 서비스 인스턴스별 개수가 원하는 만큼 실행되도록 유지합니다.\n\n## 쿠버네티스 아키텍처\n\n- 쿠버네티스는 머신 클러스터에서 실행되며, 클러스터는 대부분 소수의(보통 하나의) 마스터(master)와 하나 이상의 노드(node)로 구성됨\n- 마스터는 클러스터를 관장, 노드는 하나 이상의 파드(pod)를 실행하는 워커\n- 파드는 여러 컨테이너로 구성된 쿠버네티스의 배포 단위\n\n### 마스터\n마스터는 다음 컴포넌트를 실행\n- **API 서버**: kubectl CLI에서 사용하는 서비스 배포/관리용 REST API\n- **etcd**: 클러스터 데이터를 저장하는 키-값 NoSQL DB\n- **스케줄러**: 파드를 실행할 노드를 선택\n- **컨트롤러 관리자**: 컨트롤러를 실행\n    - 컨트롤러는 클러스터가 원하는 상태가 되도록 제어함\n    - 가령 복제 컨트롤러(replication controller)는 서비스 인스턴스가 적정 개수만큼 실행되도록 인스턴스를 시동/중지\n\n### 노드\n노드는 다음 컴포넌트를 실행\n- **큐블릿**: 노드에서 실행된느 파드를 생성/관리\n- **큐브 프록시(kube-proxy)**: 여러 파드에 부하를 분산하는 등 네트워킹을 관리\n- **파드**: 애플리케이션 서비스\n\n## 쿠버네티스 핵심 개념\n쿠버네티스는 객체라는 핵심 개념만 잘 이해하면 효과적으로 활용할 수 있음\n\n### 파드(pod)\n- 쿠버네티스의 기본 배포 단위\n- IP 주소, 스토리지 볼륨을 공유하는 하나 이상의 컨테이너로 구성됨\n- 파드의 컨테이너와 파드가 실행하는 노드, 둘 중 하나는 언제라도 깨질 수 있기 때문에 파드는 일시적(ephemeral)임\n\n### 디플로이먼트(deployment): 파드의 선언형 명세\n- 항상 파드 인스턴스(서비스 인스턴스)를 원하는 개수만큼 실행시키느 컨트롤러\n- 롤링 업데이트/롤백 기능이 탑재된 버저닝을 지원\n\n### 서비스(service)\n- 클라이언트에 안정된 정적 네트워크 위치를 제공\n- 인프라에서 제공된 서비스 디스커버리 형태를 따름\n- IP 주소와 이 주소로 해석되는 DNS명이 할당된 서비스는 TCP/UDP 트래픽을 하나 이상의 파드에 고루 분산\n- IP 주소, DNS명은 오직 쿠버네티스 내부에서만 접근할 수 있음\n\n### 컨피그맵(ConfigMap)\n- 하나 이상의 애플리케이션 서비스에 대한 외부화 구성이 정의된 이름-값 쌍의 컬렉션\n- 파드 컨테이너의 데피니션은 컨테이너 환경 변수를 정의하기 위해 컨피그맵을 참조\n\n## References\n크리스 리처드슨, 마이크로서비스 패턴, 길벗\n\n[^1]: 파드(pod) 인스턴스를 점진적으로 새로운 것으로 업데이트하여 디플로이먼트(deployment) 업데이트가 서비스 중단 없이 이루어질 수 있게 하는 것","n":0.052}}},{"i":26,"$":{"0":{"v":"Pod","n":1},"1":{"v":"\n### What\n\n- 쿠버네티스에서 컨테이너 애플리케이션의 기본 단위; 컨테이너를 표현하는 최소 단위\n- 1개 이상의 컨테이너를 실행하는 컨테이너 그룹\n- 클러스터의 노드에 배포되어 컨테이너 단위로 실행됨\n- Pod 리소스는 클러스터 내에서 접근 가능한 고유의 IP를 가짐\n- kubectl run(CLI) 명령 또는 YAML 파일을 이용해 생성\n\n### Pod 생성 w/ YAML\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-yml\nspec:\n  containers:\n  - name: my-nginx-container\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n      protocol: TCP\n```\n\n#### Pod 정의 구성 요소\n\n구성 요소 | 설명\n---|---\napiVersion  | 쿠버네티스 api의 버전을 가리킴\nkind        | 어떤 리소스 유형인지 결정 (파드, 레플리카컨트롤러, 서비스 등)\nmetadata    | 파드와 관련된 이름 네임스페이스, 라벨 및 그 밖의 정보\nspec        | 컨테이너 볼륨 등의 정보\nstatus      | 파드의 상태, 각 컨테이너의 설명 및 상태, 파드 내부의 IP 및 그 밖의 기본 정보 등\n","n":0.094}}},{"i":27,"$":{"0":{"v":"Kubectl","n":1},"1":{"v":"\n* Node 확인\n```shell\n$ kubectl get node -o wide\n```\n\n\n* Pod 생성 및 실행\n```shell\n-- 하나만\n$ kubectl run nginx-pod --image=ngnix\n\n-- 여러 개\n$ kubectl create deployment dpy-nginx1 --image=nginx\n$ kubectl run my-first-nginx --image=nginx --replicas=2\n```\n\n* Pod 확인\n```shell\n$ kubectl get pods\n$ kubectl get pods -o wide\n$ kubectl get deployment\n$ kubectl get pod,deploy\n```\n\n* Yaml 파일 실행\n```shell\n$ kubectl create -f dpy-nginx2.yaml\n$ kubectl apply -f dpy-nginx.yaml\n$ kubectl edit deploy dpy-nginx2\n```\n\n* Pod 삭제\n```shell\n$ kubectl delete pod --all\n$ kubectl delete deployment --all\n$ kubectl delete svc --all\n$ kubectl delete deployment,pod,svc --all\n```\n\n* 모니터링\n```shell\n$ watch kubectl get pods -o wide\n```","n":0.108}}},{"i":28,"$":{"0":{"v":"Controller","n":1},"1":{"v":"\n### 종류\n\n- Relication Controller\n- Replica Set\n- Deployment","n":0.378}}},{"i":29,"$":{"0":{"v":"Cluster","n":1},"1":{"v":"\n## Rancher로 클러스터 설정하기\n\n### 멀티 노드를 가진 단일 클러스터\n\n* Master node\n```shell\n# Master node\n$ cd ~\n$ sudo apt updated\n\n$ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\" \\\nserver --cluster-init \\\n--disable traefik \\\n--disable metricks-server \\\n--node-name master1 --docker\" \\\nINSTALL_K3S_VERSION=\"v1.20.0-rc4+k3s1\" sh -s -\n\n# node-token과 IP 확인\n$ sudo cat /var/lib/rancher/k3s/server/node-token\n$ kubectl get node master1 -ojsonpath=\"{.status.addresses[0].address}\"\n\n# Worker node\nNODE_TOKEN=... \nMASTER_IP=...\n\n$ curl -sfL https://get.k3s.io | K3S_URL=https://$MASTER_IP:6443 \\\nK3S_TOKEN=$NODE_TOKEN \\\nINSTALL_K3S_EXEC=\"--node-name m1-worker1 --docker\" \\\nINSTALL_K3S_VERSION=\"v1.20.0-rc4+k3s1\" sh -s -\n```\n\n* kubectl 명령을 사용하기 위한 환경 설정\n```shell\n$ mkdir ~/.kube\n$ sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/confifg\n$ sudo chown -R $(id -u):$(id -g) ~/.kube\n$ echo \"export KUBECONFIG=~/.kube/config\" >> ~/.bashrc\n$ source ~/.bashrc\n```\n\n* 클러스터 확인\n```shell\n$ kubectl cluster-info\n$ kubectl get node -o wide\n```\n\n* 클러스터 환경 삭제\n```shell\n# Master\n$ /usr/local/bin/k3s-uninstall.sh\n\n# Workers\n$ /usr/local/bin/k3s-agent-uninstall.sh\n```\n\n### Another master node\n```shell\n# Master node의 token과 IP\n$ NODE_TOKEN=... \n$ MASTER_IP=...\n\n$ curl -sfL https://get.k3s.io | K3S_TOKEN=$NODE_TOKEN \\\nINSTALL_K3S_EXEC=\" \\\nserver --server https://$MASTER_IP:6443 \\\n--disable traefik \\\n--disable metrics-server \\\n--node-name master2 --docker\" \\\nINSTALL_K3S_VERSION=\"v1.20.0-rc4+k4s1\" sh -s -\n```","n":0.086}}},{"i":30,"$":{"0":{"v":"Deployment vs Release","n":0.577},"1":{"v":"\n배포(Deployment)와 릴리즈(Release)는 소프트웨어 개발 및 운영에서 주요 단계로 사용되는 용어이지만, 그들 사이에는 중요한 차이점이 있음.\n\n## 배포(Deployment)\n- 배포는 특정 소프트웨어 버전의 실행 가능한 코드를 특정 환경에 설치하는 프로세스를 의미 (예: 개발, 스테이징, 프로덕션 환경 등에 코드를 배포할 수 있음)\n- 이 과정은 자동화될 수 있으며, [[dev.ops.cicd-pipeline]]의 일부로 통합될 수 있음\n- 배포의 주요 목표는 새로운 코드 또는 수정된 코드를 실행 환경에 가져오는 것임\n\n## 릴리즈(Release)\n- 릴리즈는 소프트웨어의 특정 버전이 고객 또는 사용자에게 제공되어 사용할 수 있게 되는 시점을 나타냄\n- 배포된 코드가 프로덕션 환경에서 실제로 사용자에게 활성화될 준비가 되었을 때 릴리즈라고 함\n- 릴리즈 관리는 종종 피처 플래그, 피처 코글 또는 A/B 테스팅과 같은 전략을 사용하여 특정 기능이나 변경 사항을 점진적으로 사용자에게 노출시키는 데 사용됨\n\n\n> 배포는 코드가 실행 환경에 설치되는 것을 의미하며, 릴리즈는 해당 코드가 최종 사용자에게 사용 가능하게 되는 시점을 의미합니다.\n\n코드를 프로덕션 환경에 배포할 수 있지만, 모든 사용자에게 즉시 노출되지 않을 수 있음. 일부 사용자에게만 노출시키거나, 특정 조건 하에서만 활성화하는 전략을 사용할 수 있음. 이런 전략들은 릴리즈 관리의 일부임.","n":0.081}}},{"i":31,"$":{"0":{"v":"CI/CD 파이프라인","n":0.707},"1":{"v":"\n## CI(Continuous Integration; 지속적 통합)\n코드가 주기적으로 빌드 및 테스트되어 중앙의 저장소에 통합되는 프로세스. 이를 통해 코드 변경에 대한 오류를 빠르게 감지하고 수정할 수 있음.\n\n## CD(Continuous Deployment/Delivery; 지속적 배포/전달)\n- 개발된 소프트웨어를 사용자에게 안정적으로 빠르게 제공하는 프로세스\n- 지속적 전달은 준비된 코드를 프로덕션 준비 상태로 만들며, 지속적 배포는 프로덕션에 자동으로 배포하는 단계까지 포함함\n\n## CI/CD 파이프라인의 중요성\n- 빠르게 피드백을 받아 개발 품질 향상\n- 배포 프로세스의 자동화를 통한 인력 및 시간 절감\n- 빈번하고 안정적인 배포를 통해 사용자에게 빠른 가치 제공\n\n## CI/CD 파이프라인에서 자주 사용되는 도구들\n- 버전 관리: Git, GitHub, Bitbucket\n- 지속적 통합: Jenkins, Travis CI, CircleCI, GitLab CI\n- 컨테이너화 & 오케스트레이션: Docker, Kubernetes\n- 배포 자동화: Ansible, Puppet, Chef, Terraform\n- 모니터링 & 로깅: Grafana, Prometheus, ELK Stack (Elasticsearch, Logstash, Kibana)","n":0.094}}},{"i":32,"$":{"0":{"v":"Network","n":1}}},{"i":33,"$":{"0":{"v":"VPN(Virtual Private Network, 가상 사설망)","n":0.447},"1":{"v":"\n## What\n- 두 컴퓨터 간에 암호화된 통신 경로를 설정하여 정보 흐름을 양방향으로 안전하게 보호\n\n## Why\n- 기업에서는 직원들이 집에서 일하거나 통신 네트워크 보안을 신뢰할 수 없는 국가에서 일할 수 있게 하고자 사용\n- 개인 사용자는 VPN을 사용하여 개방형 와이파이를 제공하는 카페나 다른 장소에서 더 안전하게 작업할 수 있음","n":0.147}}},{"i":34,"$":{"0":{"v":"RTP(Real-time Transport Protocol)","n":0.577},"1":{"v":"\nRTP(Real-time Transport Protocol)는 실시간 데이터 전송, 특히 오디오와 비디오 스트리밍에 주로 사용되는 프로토콜이다.\n\n## Jitter\n\n### Summary\nJitter는 RTP 패킷들 사이의 도착 시간의 변화량을 의미한다. RTP를 사용하는 VoIP, 비디오 컨퍼런싱, 스트리밍 미디어 등의 실시간 애플리케이션에서 중요한 지표로 간주되며, 네트워크의 품질을 평가하는 데 사용된다.\n\n### What\nRTP의 데이터 흐름은 실시간 특성 때문에 일반적으로 일정한 간격으로 패킷이 전송된다. 그러나 실제 네트워크 환경에서는 다양한 요인으로 인해 패킷 간의 간격이나 지연 시간이 일정하지 않을 수 있다. 이러한 변동성을 **Jitter**라고 한다.\n\n> Jitter는 네트워크에서 패킷의 도착 간격의 변동을 나타낸다. 다시 말해, RTP 패킷들 사이의 도착 시간의 변화량을 의미한다.\n\n실시간 통신에서는 데이터가 일정한 간격으로 도착하는 것이 중요하다. Jitter가 발생하면 오디오나 비디오 스트림에 끊김이나 지연이 발생할 수 있다.\n\n### Why\n\n1. 네트워크의 혼잡: 패킷들이 라우터나 스위치에서 큐 대기하는 시간이 변동될 때 발생한다.\n2. 다양한 경보를 통한 패킷 전송: 패킷들이 다양한 라두트를 거치며 전송될 때 각 경로의 지연 시간이 다를 수 있다.\n3. 네트워크 장비의 성능: 라우터나 스위치의 처리 능력에 따라 패킷의 전송 속도가 달라질 수 있다.\n\n### How\n\n1. Jitter Buffer: 수신 측에서 jitter buffer를 사용하여 일정한 간격으로 데이터를 출력할 수 있게 한다. 패킷이 일찍 도착하면 잠시 대기하게 하고, 늦게 도착하면 버퍼에서 다음 패킷을 가져와서 출력한다. 하지만 너무 큰 지터 버퍼는 전체적인 지연을 증가시킬 수 있으므로 적절한 크기 설정이 중요하다.\n2. 좋은 품질의 네트워크 장비 사용: 패킷 처리 성능이 좋은 장비를 사용하여 네트워크의 지연과 혼잡을 최소화한다.\n3. QoS(Quality of Service) 설정: RTP 트래픽에 우선 순위를 부여하여 네트워크 혼잡 시에도 RTP 패킷의 전송을 보장한다.","n":0.067}}},{"i":35,"$":{"0":{"v":"Language","n":1}}},{"i":36,"$":{"0":{"v":"Python","n":1}}},{"i":37,"$":{"0":{"v":"SQLAlchemy","n":1},"1":{"v":"\n## SQLAlchemy ORM (Object Relational Mapper)\n\n### What\n- 파이썬에서 데이터베이스를 쉽고 효율적으로 다룰 수 있는 환경을 제공하는 도구\n\n### How\n- 파이썬과 데이터베이스를 연결해 상위 레벨에서 SQL을 사용할 수 있게 해줌\n    - 파이썬의 클래스와 클래스의 객체를 관계형 데이터베이스의 테이블과 그 테이블의 레코드로 연관지음\n\n### Note\n- 만약 저수준의 SQL로 데이터베이와 직접 상호 작용해야하면 SQLAlchemy Core의 SQL Expression Language를 사용할 수 있음\n\n## SQLAlchemy 사용법\n\n### SQLAlchemy 설치\n```shell\npip install SQLAlchemy\n```\n\n### 데이터베이스에 접속\n```python\nfrom sqlalchemy.orm import scoped_session, sessionmaker, relationship, backref\n\n# 데이터베이스에 접속하기 위해 데이터베이스 엔진을 추상화한 Engine 객체를 얻는다\nengine = sa.create_engine('mysql+pymysql://root:password@34.239.103.110/testdb') #'sqlite:///site.db'\n\n# 연결된 데이터베이스를 다루기 위한 세션 객체를 얻는다.\n# 이 세션을 이용해 데이터베이스 쿼리, 커넥션, 트랜잭션 등 데이터베이스와 관련된 모든 작업을 처리한다. bind 인자에 연결할 데이터베이스 엔진을 설정한다.\nsession = scoped_session(sessionmaker(bind=engine))\n```\n\n### Base 클래스 만들기\n```python\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n```\n\n### Model 만들기\n```python\nimport sqlalchemy as sa\n\n# users 테이블을 아래와 같이 모델링한다\nclass User(Base):\n    # User 클래스와 매핑될 물리적인 데이터베이스 테이블명\n    __tablename__ = 'user'\n    id = sa.Column(sa.Integer, primary_key=True)\n    username = sa.Column(sa.String(45), unique=True, nullable=False)\n    email = sa.Column(sa.String(80), unique=True, nullable=False)\n    password = sa.Column(sa.String(45), nullable=False)\n    \n    # 객체에 어떤 값이 들어있는지 출력 (디버깅, 로깅 용)\n    def __repr__(self):\n            return f\"User('{self.user_name}', '{self.email}')\"\n```\n\n### 데이터베이스 테이블 생성\n```python\n# 앞서 초기화한대로 데이터베이스 테이블을 생성한다.\nBase.metadata.create_all(engine)\n```\n\nSQLAlechemy에서 자동으로 아래와 같은 데이터베이스 테이블 스키마를 새성하는 SQL 문을 실행한 것처럼 테이블을 만들어준다.\n\n```sql\nCREATE TABLE users(\nid INTEGER NOT NULL,\nusername VARCHAR(50),\nemail VARCHAR(80),\npassword VARCHAR(55),\nPRIMARY KEY (id),\nUNIQUE (username)\n);\n```\n\n## References\n- 주성식, 홍성민, 파이썬 웹 프로그래밍, 위키북스\n- https://docs.sqlalchemy.org/en/latest/orm/examples.html\n\n## Note\n아래 자료도 추가 확인할 것\n- https://www.sqlalchemy.org/\n- http://flask-sqlalchemy.pocoo.org/2.3/\n","n":0.068}}},{"i":38,"$":{"0":{"v":"Problem Solving Tips","n":0.577},"1":{"v":"\n## 입력 처리\n```python\n# 다음을 고할 것\n# 숫자인지, 문자인지\n# 음수\n# 정수 여부\n# 정렬 여부\n# 입력 값이 없는 경우\n# 입력 리스트 내부를 직접 조작해도 되는지 ~ immutable\n# 대소문자 구분\n\nif not strs:\nif len(s) < 2:\nif s == s[::-1]\n```\n\n## 리스트 초기화\n```python\nanswer = [0] * len(in)\n```\n\n## 최댓값/최솟값\n```python\nmx = -sys.maxsize\nmn = sys.maxsize\n\nmx = float('-inf')\nmn = float('inf')\n```\n\n## 문자\n```python\nchar.isalnum()\nchar.lower()\nchar.isdigit()\nchar.isnumeric()\n\n# isdecimal() ⊆ isdigit() ⊆ isnumeric()\n# https://stackoverflow.com/questions/44891070/whats-the-difference-between-str-isdigit-isnumeric-and-isdecimal-in-python\n```\n\n## 문자열\n```python\nstrs.lower()\nstrs.reverse() ~ strs[:] = strs[::-1]\n```\n\n## Matrix\n```python\nany(target in row for row in matrix)\n```\n\n## 정규식\n```python\nimport re\n\ns = re.sub('[^a-z0-9]', '', s)\nword = re.sub(r'[^\\w]', ' ', paragraph)\n```\n\n## Counter\n```python\nimport collections\n\ncounts = collections.Counter(words)\n\nreturn counts.most_common(1)[0][0]\n또는 sorted(counts.items(), key=lambda x: x[1])\n```\n\n## List ~ 문자열\n```python\nstrs.pop(0)  # O(n)\nstrs.pop()\n\nlst.index(val)\n\nmax(lst, key=len)\n\nresult = int(''.join(str(e) for e in a))\nresult = int(''.join(map(str, a))\n\nnext_elements = elements[:] # 값을 참조가 아닌 복사\nnext_elements.remove(e)\n\npath + [candidates[i]]\n\nnew_list = [12, 3].copy\n```\n\n## Deque\n```python\nimport collections\n\nstrs: Deque = collections.deque()\nstrs.popleft()  # O(1)\n```\n\n## Two pointers\n```python\nleft, right = 0, len(s) - 1\nwhile left < right:\n  # do something\n  s[left], s[right] = s[right], s[left]\n  left += 1\n  right -= 1\n\n# 06 longest palindromic substring\n# 08 trapping rain water\n# 09 3sum\n# 30 longest-substring-without-repeating-characters\n# 63 sort-colors\n# 67 intersection-of-two-arrays\n# 76 minimum-window-substring\n\n# Runner\nslow = fast = head\nwhile fast and fast.next:\n  fast = fast.next.next\n  slow = slow.next\n\n# 13 palindrome-linked-list\n# 58 sort-list\n```\n\n## 정렬\n```python\ns.sort(key=lambda x: (x.split()[1], x.split()[0]))\ns.sort(key=lambda x: (x.split()[1:], x.split()[0]))\n\n''.join(sorted(s))\n```\n\n## Dict\n \ncollections.defaultdict(int)\ncollections.defaultdict(list)\n\n## 예외 처리\n- stack에서 pop 전에 stack이 비었는지 확인\n  if not stack or table[char] != stack.pop(): return False\n\n\n## Stack\n```python\nstack = []\ntop = stack[-1]\n\nstack = collections.deque([root])\nwhile stack:\n  node = stack.pop()\n  if node:\n    ...\n```\n\n## Queue\n```python\nqueue = collections.deque([root])\nwhile queue:\n  node = queue.popleft()\n  if node:\n    ...\n```\n\n## 우선순위 큐\n```python\nimport heapq\n\nheap = []\nfor lst in lists:\n  heapq.heappush(heap, (lst.val, lst))\n\nwhile heap:\n  elem = heapq.heappop(heap)\n\nfreqs = collections.Counter(nums)\nfreqs_heap = []\n\nfor f in freqs:\n  heapq.heappush(freqs_heap, (-freqs[f], f))\n\ntopk = []\nfor _ in range(k):\n  topk.append(heapq.heappop(freqs_heap)[1])\n\nlist(zip(*collections.Counter(nums).most_common(k)))[0]\n```\n\n## 해시 테이블\n```python\nfreqs = {}\nfreqs[char] += 1\nif char in freqs:\n  count += freqs[char]\n\nfreqs = collections.defaultdict(int)\n\nfreqs = collections.Counter(S)\n```\n\n## Set\n```python\ns = set()\ns.add(val)\nif sth in s:\n```\n\n## Sliding Window\n\n06 longest-palindromic-substring\n30 longest-substring-without-repeating-characters\n\n\n## 백트래킹\n```python\nresults = []\ndef dfs(index, path):\n  # 끝까지 탐색하면 백트래킹\n  if len(path) == len(digits):\n    results.append(path)\n    return\n\n  for i in range(index, len(digits)):\n    for j ...\n      dfs(i + 1, path + j)\n    \n# 33 letter-combination-of-a-phone-number\n# 34 permutations\n# 35 combinations\n# 44 longest-univalue-path\n# 45 invert-binary-tree\n# 46 merge-two-binary-trees\n```\n\n## 그래프\n```python\ngraph = collections.defaultdict(list)\nfor a, b in sorted(tickets):\n  graph[a].append(b)\n```\n\n## 최단 거리\n\n## 트리\n```python\nmid = len(nums) // 2\nnode = TreeNode(nums[mid])\nleft = bst(nums[:mid])\nright = bst(nums[mid + 1:])\n```\n\n## 분할 정복\n50 convert-sorted-array-to-binary-search-tree\n54 construct-binary-tree\n58 sort-list\n\n## Set\n```python\nresult = set()\nresult.add(a)\n```","n":0.052}}},{"i":39,"$":{"0":{"v":"Math","n":1},"1":{"v":"\n`math.prod` is a new function (from Python 3.8).\n\n```python\nimport math\n\nnumbers = [1, 2, 3, 4]\n\nprint(math.prod(numbers))\n```\n\nIf you want to have a more compatible code, you should use the old way:\n\n```python\nfrom functools import reduce\nimport operator\n\nprint(reduce(operator.mul, [1,2,3,4], 1))\n```","n":0.171}}},{"i":40,"$":{"0":{"v":"OS","n":1},"1":{"v":"\n## 디렉토리 내의 파일 리스트 조회\n\n```python\ndef get_files(directory: str) -> Iterator[str]:\n    return (file for file in os.listdir(directory))\n```","n":0.25}}},{"i":41,"$":{"0":{"v":"Logger (TBD)","n":0.707},"1":{"v":"\n```python\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# 로그 레벨 확인\nprint(logger.getEffectiveLevel())\nprint(logger.isEnabledFor(logging.WARNING)) # True\nprint(logger.isEnabledFor(logging.DEBUG)) # False\n\n# 로그 레벨 변경\nlogger.setLevel(logging.DEBUG)\nprint(logger.getEffectiveLevel())\nprint(logger.isEnabledFor(logging.DEBUG)) # True\n\n# Literal 사용\nprint(logging.getLevelName(\"DEBUG\"))\nlogger.setLevel(\"INFO\")\n\n```","n":0.236}}},{"i":42,"$":{"0":{"v":"How to access the top element in heapq without deleting (popping) it python?","n":0.277},"1":{"v":"\nFrom docs python, under heapq.heappop definition, it says:\n\n**To access the smallest item without popping it, use heap[0].**\n\nIt says smallest, because it is a min heap. So the item at the top will be the smallest one.\n\n```python\nimport heapq\n\npq = []\n\nheapq.heappush(pq,5)\nheapq.heappush(pq,3)\nheapq.heappush(pq,1)\nheapq.heappush(pq,2)\nheapq.heappush(pq,4)\n\nprint(\"element at top = \",pq[0])\nprint(\"check the heapq : \", pq)\n```\n\nResult\n```python\nelement at top =  1\ncheck the heapq :  [1, 2, 3, 5, 4]\n```\n\n### References\n- https://stackoverflow.com/questions/64246136/how-to-access-the-top-element-in-heapq-without-deleting-popping-it-python","n":0.127}}},{"i":43,"$":{"0":{"v":"Functools","n":1},"1":{"v":"\n## 부분 적용\n\n```python\ndef filterBy(filter_fn):\n    return functools.partial(filter, filter_fn)\n```\n@FunctionalProgramming","n":0.378}}},{"i":44,"$":{"0":{"v":"Python으로 file 다루기","n":0.577},"1":{"v":"\n## Text file 읽기\n\n```python\nwith open('file.txt', encoding='utf8') as f:\n    for lines in f:\n        print(line.strip())\n```\n\n## References\n- https://pythontutorial.net/python-basics/python-read-text-file/","n":0.258}}},{"i":45,"$":{"0":{"v":"JavaScript","n":1}}},{"i":46,"$":{"0":{"v":"문자열 다루기","n":0.707},"1":{"v":"\n##\n```javascript\nfunction convertToFillInTheBlank(sentence) {\n    const regex = /{{([^}]+)}}/g;\n    const matches = sentence.match(regex);\n\n    if (!matches) {\n        return sentence;\n    }\n\n    const components = sentence.split(regex).map((item, index) => {\n        if (matches.includes(`{{${item}}}`)) {\n            return `<span>key={${index}} solution={${item}}</span>`;\n        }\n        return item;\n    });\n\n    return components.toString();\n}\n\nconst data = '(Genesis 1:1) In the beginning God created the {{heavens}} and the {{earth}}.'\n\n\nconst lineComponents = line.split(regex).map((item, index) => {\n    if (matches.includes(`{{${item}}}`)) {\n    return <Blank key={index} solution={item} />;\n    }\n    return item;\n});\n```","n":0.122}}},{"i":47,"$":{"0":{"v":"날짜 다루기","n":0.707},"1":{"v":"\n## 현재 시각 가져오기\n```javascript\nconst currentDate = new Date();\n```\n\n## Datetime을 Date 포맷으로 바꾸기\n```javascript\n// Format the date as 'YYYY-MM-DD'\nconst formattedDate = currentDate.toISOString().slice(0, 10);\n```\n\n## 날짜를 해당 주의 월요일 날짜로 변경\n```javascript\nfunction convertDate(dateString) {\n  const date = new Date(dateString);\n  const dayOfWeek = date.getDay();\n\n  // If the date is already a Sunday, subtract 7 days\n  if (dayOfWeek === 0) {\n    date.setDate(date.getDate() - 7);\n  } else {\n    date.setDate(date.getDate() - dayOfWeek);\n  }\n\n  return date.toISOString().slice(0, 10);\n}\n\n// Example usage\nconst date1 = '2023-09-18';\nconst convertedDate1 = convertDate(date1);\nconsole.log(convertedDate1); // Output: 2023-09-17\n\nconst date2 = '2023-09-25';\nconst convertedDate2 = convertDate(date2);\nconsole.log(convertedDate2); // Output: 2023-09-24\n\nconst date3 = '2023-10-02';\nconst convertedDate3 = convertDate(date3);\nconsole.log(convertedDate3); // Output: 2023-10-01\n```","n":0.102}}},{"i":48,"$":{"0":{"v":"Java","n":1}}},{"i":49,"$":{"0":{"v":"Troubleshooting","n":1},"1":{"v":"\n## Error occurred during initialization of VM\n\n- 자바가 32비트일 경우 램을 1기가까지만 할당 가능하다!\n- https://kin.naver.com/qna/detail.nhn?d1id=2&dirId=20411&docId=344517490\n\n\n## Determining whether a Java installation is 32 bit or 64 bit\n\n$ java –version\n- If 64 bit is running you’ll get a message like:\n```\njava version \"1.6.0_18\" \nJava(TM) SE Runtime Environment (build 1.6.0_18-b07) \nJava HotSpot(TM) 64-Bit Server VM (build 16.0-b13, mixed mode)\n```\n- For 32 bit, the message is:\n```\njava version \"1.6.0_21\" \nJava(TM) SE Runtime Environment (build 1.6.0_21-b07) \nJava HotSpot(TM) Client VM (build 17.0-b17, mixed mode, sharing)\n```\n\n- https://wiki.scn.sap.com/wiki/display/SCM/Determining+whether+a+Java+installation+is+32+bit+or+64+bit\n","n":0.113}}},{"i":50,"$":{"0":{"v":"Java8 함수형 프로그래밍과 Java9","n":0.5},"1":{"v":"\n\nTop 10 Easy Performance Optimisations in Java\nhttps://blog.jooq.org/2015/02/05/top-10-easy-performance-optimisations-in-java/\n\n10 Subtle Mistakes When Using the Streams API\nhttps://blog.jooq.org/2014/06/13/java-8-friday-10-subtle-mistakes-when-using-the-streams-api/\n\nComparator가 Functional Interface가 맞는가?\n\nhttps://stackoverflow.com/questions/23721759/functionalinterface-comparator-has-2-abstract-methods\n\n? super T 와 ? extends T의  차이점\nhttps://stackoverflow.com/questions/2827585/what-is-super-t-syntax\n\nLambda와 Garbage Collection\nhttps://stackoverflow.com/questions/27212730/are-lambdas-garbage-collected\n\n\nJava 9 module example\nhttps://www.logicbig.com/tutorials/core-java-tutorial/modules/quick-start.html\n\nJava 9 New Features\nhttp://www.baeldung.com/new-java-9\n\nJava9 Examples\nhttps://github.com/eugenp/tutorials/tree/master/core-java-9\n\nOracle Java SE Support Roadmap\nhttp://www.oracle.com/technetwork/java/javase/eol-135779.html","n":0.164}}},{"i":51,"$":{"0":{"v":"Advanced","n":1},"1":{"v":"\n```java\npublic void doIt() {\n    Supplier < Consumer <?> > v1 = () -> a -> {}; \n}","n":0.243}}},{"i":52,"$":{"0":{"v":"Git","n":1},"1":{"v":"\n## Config\n\n```\n$ git config --list --show-origin\n\n$ git config --global user.name \"John Doe\"\n$ git config --global user.email johndoe@example.com\n```","n":0.243}}},{"i":53,"$":{"0":{"v":"Front-End","n":1}}},{"i":54,"$":{"0":{"v":"React","n":1},"1":{"v":"\n- React로 Angular의 Tour of Heroes 개발\n    - https://github.com/coryhouse/react-tour-of-heroes/\n\n- 리액트 네이티브로 안드로이드 앱 개발하기의 장단점\n    - https://academy.realm.io/kr/posts/react-native-android-pros-cons/\n\n\n```js\nconst myData = [].concat(this.state.data)\n\t.sort((a, b) => a.item > b.item)\n\t.map((item, i) => \n\t\t<div key={i}> {item.id} {item.time}{item.description}</div>\n)\n```","n":0.18}}},{"i":55,"$":{"0":{"v":"Chakra UI","n":0.707},"1":{"v":"\n## Width full for mobile screen\n\n```javascript\nimport { Box } from \"@chakra-ui/react\";\n\nconst MyComponent = () => {\n  return (\n    <Box width={{ base: \"100%\", md: \"50%\", lg: \"25%\" }}>\n      {/* Your content goes here */}\n    </Box>\n  );\n};\n```","n":0.169}}},{"i":56,"$":{"0":{"v":"jQuery","n":1},"1":{"v":"\n## What\n\n- JavaScript library\n    - HTML document traversal and maniupulation\n    - event handling\n    - animation\n    - Ajax\n        - simplifies JavaScript programming\n\n## Function 사용법\n\n```js\n// [1] \n$(document).ready(function() {\n\t...\n});\n-> window.onload = function() {}과 같은 기능\n\n// [2]\n$(function() {\n\t...\n});\n-> 그냥 함수를 실행하라는 의미\n\n// [3] \n(function($) {\n\t...\n})(jQuery);\n-> jQuery를 사용할 때 $의 의미를 jQuery에서 사용하겠다는 의미; 함수로 $를 매개변수로 넘기는 것.\n```\n\n## Etc\n- 크롬 개발자모드 콘솔에서 jQuery 사용하는 방법\n    - https://gomcine.tistory.com/entry/%ED%81%AC%EB%A1%AC-%EA%B0%9C%EB%B0%9C%EC%9E%90%EB%AA%A8%EB%93%9C-%EC%BD%98%EC%86%94%EC%97%90%EC%84%9C-jQuery-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95","n":0.126}}},{"i":57,"$":{"0":{"v":"Angular","n":1},"1":{"v":"\n## 개요\n\n- Angular란?\n  - Google이 만든 오픈소스 기반의 다이나믹 웹 애플리케이션 프레임워크\n  - 프론트엔드 코드를 단순하고 명료하게 하기 위해 HTML, CSS 그리고 JavaScript를 하나로 조합하는 방법을 제공\n  - 2014년 10월에 처음 소개되어 2018년 6월 기준으로 v6.0.4까지 릴리즈됨\n- Angular에서 사용하는 언어는 TypeScript\n  - Microsoft에서 개발한 자바스크립트의 확장된 언어\n  - JavaScript ES6의 기능에 엄격한 타입 체크와 객체지향적 기능을 추가\n  - 브라우저는 TypeScript를 해석할 수 없기에, TSC라는 transpiler를 이용하여 TypeScript를 JavaScript로 변환하여 배포 필요\n- Angular 아키텍처\n  \"https://angular.io/guide/architecture\"\n  - Component를 이용하여 Template과 Service를 관리\n    - Template은 View 자체 (UI 화면; HTML)\n    - Component는 View를 제어 (class; TypeScript)\n      \"- Components are the fundamental building blocks of Angular applications. They display data on the screen, listen for user input, and take action based on that input.\n      - Components shouldn't fetch or save data directly and they certainly shouldn't knowingly present fake data. They should focus on presenting data and delegate data access to a service.\"\n    - Service는 애플리케이션의 공통 로직 (class; TypeScript)\n      - database access, http 통신\n      - Services are a great way to share information among classes that don't know each other. \n  - 상세 내용은 별도로 학습 필요; 이게 Angular의 핵심\n- 개발을 위해 Angular CLI를 이용해야 함\n  - 예를 들어, 새로운 class를 하나 만드려고 할 때 IDE에서 생성하는 것이 아니라 CLI(Command Line Interface)를 통해 만들어야 함\n    - Angular는 class의 skeleton code를 생성해주고,  \n    - 빌드에 관한 설정까지 자동으로 처리해 줌 (Goood!) \n- 실습\n  - Angular CRUD application \n  - Tour of Heroes application\n    - https://infoscis.github.io/2017/07/21/angular-tutorial-introduction/\n- 기타\n  - 요새 핫한 웹 개발 프레임워크로 구글의 Angular, 페이스북의 React, 그리고 Angular와 React를 짬뽕해놓은 Vue.js라는 넘들이 있음\n    - 각각의 프레임워크가 책 한 권 분량\n  - 각각의 프레임워크가 장단점이 있기에 웹 서비스 개발 시 프레임워크를 선정 후 해당 프레임워크 위에서 개발\n    - 규모가 있는 개발팀에서 대규모 웹 서비스 개발에는 Angular가 주로 사용된다고 함\n      - 인천공항에 돌아다니는 LG 로봇의 관제 시스템도 Angular로 개발했다는 썰이.. (by LG CNS)\n      - 스타트업에서는 페이스북이 만든 React가 더 널리 사용됨\n  - Angular는 배우는 데 러닝 커브가 있으나 단계를 넘어서면 React보다 생산성이 더 좋다고들 함\n    - 진입 장벽이 높은 이유는 1) Angular Architecture와 구성 요소들, 그리고 각 요소들의 관계를 이해해야 하고 2) 비동기 HTTP 처리를 위해 Reactive 기술을 이해해야 하며, 3) 무엇보다 TypeScript로 개발을 해야 함\n    - 생산성이 좋아지는 이유는 Angular가 Architecture를 꽉 붙잡고 개발에 필요한 API들을 대부분 제공하기에, 개발자는 주어진 가이드만 따라서 로직만 개발하면 되기 때문\n      - 예를 들어, 설정, 빌드 및 배포 등 개발자가 귀찮아하는 부분은 Angular가 알아서 척척~\n      - 또한, 자체 라이브러리가 빵빵하므로 별도의 3rd party 라이브러리를 갖다 쓸 필요가 없음\n  - 개인적으로는, SmartRM에 Angular를 선호하지는 않으나 그렇다고 다른 대안은 없음;;\n    - 일단 TypeScript가 친숙하지 않고 \n    - SmartRM이 정형화된 대규모 서비스를 제공하는 단계도 아니며\n    - 향후 유지 보수를 위해 개발팀 전원이 Angular를 따로 학습해야 하는, 배보다 배꼽이 더 큰 문제가 있음\n\n\n## 참고 자료\n\n- Official https://angular.io/guide/quickstart\n- Architecture https://angular.io/generated/images/guide/architecture/overview2.png\n- 강의 자료\nhttps://blog.naver.com/vega2k\nhttps://github.com/vega2k/spring-boot-angular\nhttps://github.com/vega2k/angular-user-form-app\nhttps://github.com/vega2k/Angular6-toh-app\n\n- Angular style gude\nhttps://angular.io/guide/styleguide\n\n- TypeScript guidelie\nhttps://github.com/Microsoft/TypeScript/wiki/Coding-guidelines\n\n- ES6 Promise 객체\nhttps://developer.mozilla.org/ko/docs/Web/JavaScript/Reference/Global_Objects/Promise\n\n- RxJS Observable 객체\nhttp://xgrommx.github.io/rx-book/index.html\n\n- Learn rxjs\nhttps://www.learnrxjs.io/\n\n- Understanding Lettable Operators\nhttps://blog.angularindepth.com/rxjs-understanding-lettable-operators-fe74dda186d3\n\n- Understaning Subject Objects\nhttps://medium.com/@luukgruijs/understanding-rxjs-subjects-339428a1815b\n​\n- angular 6 version \nhttps://blog.angular.io/version-6-of-angular-now-available-cc56b0efa7a4\n​\n- User 관리 : angular - boot 연동 (Angular Form) 예제\nhttps://github.com/vega2k/userApp-boot-angular\n\n- Angular 6 Example App + Angular CLI + Angular Material + Docker + Angular Example Library\nhttps://github.com/Ismaestro/angular6-example-app\n\n- Angular CLI\nhttps://github.com/angular/angular-cli/wiki\n```\nnpm install -g @angular/cli\n\nng new my-project\ncd my-project\nng serve\n```\n\n- angular6-example-app\nhttps://github.com/Ismaestro/angular6-example-app\n\n- 책\nhttps://www.amazon.com/ng-book-Complete-Angular-Nathan-Murray/dp/1985170280/ref=sr_1_3?s=books&ie=UTF8&qid=1531925325&sr=1-3&keywords=ng-book&tag=chrome-extensions-20&dpID=51Vld%252BPmHwL&preST=_SY291_BO1,204,203,200_QL40_&dpSrc=srch\n\nhttp://www.yes24.com/24/goods/58054234","n":0.044}}},{"i":58,"$":{"0":{"v":"Development Environment","n":0.707}}},{"i":59,"$":{"0":{"v":"Tool","n":1}}},{"i":60,"$":{"0":{"v":"Video Share Code","n":0.577},"1":{"v":"\n* Remote > SSH\n    - https://code.visualstudio.com/docs/remote/ssh\n\n* Visual Studio Code Server\n    - https://code.visualstudio.com/docs/remote/vscode-server","n":0.289}}},{"i":61,"$":{"0":{"v":"Data Structure & Algotirhm","n":0.5}}},{"i":62,"$":{"0":{"v":"Tree","n":1},"1":{"v":"\n## 트리를 순회하는 방법은 무엇인가?\n\n```python\n# 임의의 노드에서 깊이 우선 탐색을 시작하는 것이다.\n\ndef dfs(curr, prev):\n\tfor next in adj[curr]:\n\t\tif next != prev:\n\t\t\tdfs(next, curr)\n\ndfs(x, 0) # 노드 x에서 탐색을 시작. 이전 노드가 없으므로 prev는 0.\n```\n\n## 서브트리의 노드 수\n\n```python\n# 동적 계획법\ndef dfs(curr, prev):\n \tcounts[curr] = 1\n \tfor next in adj[curr]:\n \t\tif next == prev:\n \t\t\tcontinue\n \t\tdfs(next, curr)\n \t\tcounts[curr] += count[next]\n```\n#DynamicProgramming\n\n","n":0.134}}},{"i":63,"$":{"0":{"v":"Java Example","n":0.707},"1":{"v":"\n## BST\n\n```java\nInteger last_printed = null;\nboolean checkBST(TreeNode n) {\n  if (n == null) return true;\n  \n  // 왼쪽을 재귀적으로 검사\n  if (!checkBST(n.left)) return false;\n  \n  // 현재 노드 검사\n  if (last_printed != null && n.data <= last_printed) {\n    return false;\n  }\n  last_printed = n.data;\n\n  // 오른쪽을 재귀적으로 검사\n  if (!checkBST(n.right)) return false;\n\n  return true; // 검사 통과\n}\n\nboolean checkBST(TreeNode n) {\n  return checkBST(n, null, null);\n}\n\n// Root에서 내려가면서 해당 노드가 가질 수 있는 값을 제한\n// 최솟값과 최댓값을 아래로 전달\n// TC: O(N), SC: O(logN)\nboolean checkBST(TreeNode n, Integer min, Integer max) {\n  if (n == null) return true;\n\n  if ((min != null && n.data <= min) || (max != null && n.data > max)) {\n    return false;\n  }\n\n  // 왼쪽으로 분기하면 max를, 오른쪽으로 분기하면 min을 갱신\n  if (!checkBST(n.left, min, n.data) || !checkBST(n.right, n.data, max)) {\n    return false;\n  }\n\n  return true;\n}\n\n\n// 정렬된 배열을 BST로 만들기\n\nvoid makeTree(int[] a) {\n  root = makeTreeR(a, 0, a.length - 1);\n}\n\nNode makeTreeR(int[] a, int start, int end) {\n  if (start > end) return null;\n  int mid = (start + end) / 2;\n  Node node = new Node(a[mid]);\n  node.left = makeTreeR(a, start, mid - 1);\n  node.right = makeTreeR(a, mid + 1, end);\n  return node;\n}\n\n// 주어진 트리가 BST인지 확인\n\nint index = 0;\nvoid inorder(Node root, int[] array) {\n  if (root != null) {\n    inorder(root.left, array);\n    array[index] = root.data;\n    index++;\n    inorder(root.right, array);\n  }\n}\n\n// 순서대로 출력\n\nvoid levelOrderPrint(Node root) {\n  if (root == null) return;\n  Queue<Node> q = new LinkedList<>();\n  q.add(root);\n  while (!q.isEmpty()) {\n    Node n = q.poll();\n    System.out.print(n.data + \" \");\n    if (n.left != null)\n      q.add(n.left);\n    if (n.right != null)\n      q.add(n.right);\n    }\n  }\n}\n\n// 역순으로 출력\n\nvoid reverseLevelOrder(Node node) {\n  Stack<Node> S = new Stack();\n  Queue<Node> N = new LinkedList();\n\n  Q.add(node);\n\n  while (!Q.isEmpty()) {\n    Node n = Q.peek();\n    S.push(n);\n\n    if (n.right != null)\n      Q.add(n.right);\n    if (n.left != null)\n      Q.add(n.left);\n  }\n\n  while (!S.empty()) {\n    Node n = S.peek();\n    System.out.print(n.data + \" \");\n    S.pop();\n  }\n}\n\n// 층별로 출력\n\nvoid levelOrderPrint(Node root) {\n  if (root == null) return;\n  Queue<Node> q = new LinkedList<>();\n  q.add(root);\n  q.add(null);\n  while (!q.isEmpty()) {\n    Node n = q.poll();\n    if (n == null) {\n      if (!q.isEmpty()) {\n        q.add(null);\n        System.out.println();\n      }\n    } else {\n      if (n.left != null)\n        q.add(n.left);\n      if (n.right != null)\n        q.add(n.right);\n      System.out.print(n.data + \" \");\n    }\n  }\n}\n```\n\n## Binary Search Tree Interator\n\n```java\n// https://leetcode.com/problems/binary-search-tree-iterator/discuss/52525/My-solutions-in-3-languages-with-Stack\n\npublic class BSTIterator {\n    private Stack<TreeNode> stack = new Stack<TreeNode>();\n    \n    public BSTIterator(TreeNode root) {\n        pushAll(root);\n    }\n/** @return whether we have a next smallest number */\n    public boolean hasNext() {\n        return !stack.isEmpty();\n    }\n/** @return the next smallest number */\n    public int next() {\n        TreeNode tmpNode = stack.pop();\n        pushAll(tmpNode.right);\n        return tmpNode.val;\n    }\n    \n    private void pushAll(TreeNode node) {\n        for (; node != null; stack.push(node), node = node.left);\n    }\n}\n```\n\n## 트리의 높이\n```java\nint height(Node root) {\n  if (root == null) return 0;\n  int lh = height(root.left);\n  int rh = height(root.right);\n  if (lh > rh) return lh + 1;\n  return rh + 1;\n}\n```","n":0.047}}},{"i":64,"$":{"0":{"v":"Sief (TBD)","n":0.707},"1":{"v":"\n```python\n# 소수가 아닌 수를 지워가는 과정\n# def mark_sieve(first, last, factor):\n#   *first = False\n#   while last - first > factor:\n#     first = first + factor\n#     *first = False\n\n\ndef sief0(n):\n  sieve = [0] * (n + 1)\n  \n  for x in range(2, n + 1):\n    if sieve[x]: continue\n    for u in range(2*x, n + 1, x):\n      sieve[u] = 1\n\n  print(sieve[2:])\n\ndef sief1(n):\n  def mark_sieve(first, last, factor):\n    sieve[first] = False\n    while last - first > factor:\n      first = first + factor\n      sieve[first] = False\n\n  sieve = [True] * (n + 1)\n  for x in range(2, n + 1):\n    # if sieve[x]: continue\n    mark_sieve(x, n + 1, x)\n  print(sieve[2:])\n\nsief0(10)\nsief1(10)\n```","n":0.098}}},{"i":65,"$":{"0":{"v":"References","n":1},"1":{"v":"\n백준 알고리즘 중급\nhttps://code.plus/course/57\n\nhttps://inner-game.tistory.com/\n\nhttps://blog.naver.com/iamjune88/222436916116","n":0.577}}},{"i":66,"$":{"0":{"v":"Math","n":1}}},{"i":67,"$":{"0":{"v":"소인수분해 후 각 소인수의 지수의 합 구하기","n":0.378},"1":{"v":"\n## 문제\n$1 \\times 2 \\times 3 \\times ··· \\times 10$을 소인수분해하면 $2^x \\times 3^y \\times 5^z \\times 7$이다. 이때 자연수 $x, y, z$에 대하여 $x+y+z$의 값을 구하시오.\n\n## 풀이\n주어진 수를 소인수분해 후 각 소인수의 지수를 모두 합산한다.\n\n```python\nfrom collections import Counter\n\nnum = 1\nfor n in range(2, 11):\n    num *= n\n\nfactors = Counter(prime_factorization(num))\n\nx = factors[2]\ny = factors[3]\nz = factors[5]\n\nprint(x + y + z)\n```\n\n소인수분해 코드는 아래를 참고한다.\n![[dev.dsa.math.prime_factorization]]\n\n## References\n문제 0131, 쎈 중등 수학 1-1, (주)좋은책신사고","n":0.12}}},{"i":68,"$":{"0":{"v":"소수(Prime number)","n":0.707},"1":{"v":"\n소수(prime number)는 **에라토스테네스의 체** 알고리즘으로 구한다.\n\n```python\ndef sieve_of_eratosthenes(n):\n    \"\"\"Return a list of all prime numbers less than n.\"\"\"\n    sieve = [True] * n\n    sieve[0], sieve[1] = False, False  # 0과 1은 소수가 아니다.\n    \n    for i in range(2, int(n ** 0.5) + 1):\n        if sieve[i]:\n            for j in range(i*i, n, i):\n                sieve[j] = False  # i의 배수는 모두 소수가 아니다.\n\n    return [i for i, is_prime in enumerate(sieve) if is_prime]\n\nnum = 30\nprint(sieve_of_eratosthenes(num))  # [2, 3, 5, 7, 11, 13, 17, 19, 23, 29] 출력\n```","n":0.11}}},{"i":69,"$":{"0":{"v":"소인수분해","n":1},"1":{"v":"\n```python\ndef prime_factorization(n):\n    \"\"\"Return the prime factorization of a number.\"\"\"\n    factors = []\n    divisor = 2  # 시작하는 소수\n\n    while divisor <= n:\n        if n % divisor == 0:  # n이 divisor로 나누어 떨어지면\n            factors.append(divisor)\n            n //= divisor   # n을 divisor로 나눈 몫을 새로운 n으로 설정\n        else:\n            divisor += 1\n\n    return factors\n\nnum = 56\nprint(prime_factorization(num))  # [2, 2, 2, 7] 출력\n```","n":0.13}}},{"i":70,"$":{"0":{"v":"약수의 개수가 n인 자연수 k(의 개수) 구하기","n":0.378},"1":{"v":"\n## 문제\n60보다 작은 자연수 _k_의 약수의 개수가 4일 때, _k_가 될 수 있는 수의 개수는?\n\n## 풀이\n- 60보다 작은 모든 자연수의 소인수분해의 결과를 `Counter`를 사용하여 **각 소수의 개수**를 구한다.\n- 소수의 개수가 `약수의 개수가 4`라는 조건에 부합하면 count를 추가한다.\n    - 어떤 자연수의 **약수의 개수가 4**가 되려면, 그 자연수는 **_소인수 하나의 세제곱_**(조건1) 또는 **_소인수 두 개의 곱_**(조건2)의 꼴이어야 한다.\n\n```python\nfrom collections import Counter\n\ndef solution() {\n    def cond1(factors):\n        if len(factors) == 1:\n            if list(factors.values())[0] == 3:\n                return True\n        return False\n    def cond2(factors):\n        if len(factors) == 2:\n            if sum(list(factors.values())) == 2:\n                return True\n        return False\n\n    count = 0\n    for k in range(1, 60):\n        factors = Counter(prime_factorization(k))\n        if cond1(factors) or cond2(factors):\n          count += 1\n    \n    print(count)\n}\n```\n\n소인수분해 코드는 아래를 참고한다.\n![[dev.dsa.math.prime_factorization]]\n\n## References\n문제 0127, 쎈 중등 수학 1-1, (주)좋은책신사고","n":0.092}}},{"i":71,"$":{"0":{"v":"Leetcode","n":1}}},{"i":72,"$":{"0":{"v":"Number of Islands","n":0.577},"1":{"v":"\n```python\n# https://leetcode.com/problems/number-of-islands/\n\ndef numIslands(self, grid) -> int:\n  def out_of_grid(r, c):\n    return r < 0 or r >= len(grid) or c < 0 or c >= len(grid[0])\n\n  def dfs(r, c):\n    if grid[r][c] != \"1\":\n      return\n\n    grid[r][c] = 0\"\n\n    direction = (0, 1)\n    for _ in range(4):\n      direction = (direction[1], -direction[0])\n      nr, nc = r + direction[0], c + direction[1]\n      if not out_of_grid(nr, nc):\n        dfs(nr, nc)\n\n  ans = 0\n  for r in range(len(grid)):\n    for c in range(len(grid[0])):\n      if grid[r][c] == \"1\":\n        dfs(r, c)\n        ans += 1\n\n  return ans\n```","n":0.108}}},{"i":73,"$":{"0":{"v":"Number of Enclaves","n":0.577},"1":{"v":"\n```python\n# https://leetcode.com/problems/number-of-enclaves/\n\ndef numEnclaves(self, grid: List[List[int]]) -> int:\n  \"\"\"\n  경계에 있는 cell이 1인 넘과 그넘 친구들을 0으로 변환 후,\n  grid에 남아있는 cell의 개수를 카운트한다.\n  \"\"\"\n  def out_of_grid(r, c):\n    return r < 0 or r >= len(grid) or c < 0 or c >= len(grid[0])\n\n  def dfs(r, c):\n    if out_of_grid(r, c):\n      return\n\n    if grid[r][c] != 1:\n      return\n\n    grid[r][c] = 0\n\n    direction = (0, 1)\n    for _ in range(4):\n      direction = (direction[1], -direction[0])\n      nr, nc = r + direction[0], c + direction[1]\n      dfs(nr, nc)\n\n  m, n = len(grid), len(grid[0])\n  ans = 0\n  for r in range(m):\n    dfs(r, 0)\n    dfs(r, n - 1)\n  for c in range(n):\n    dfs(0, c)\n    dfs(m - 1, c)\n\n  return sum([sum(row) for row in grid])\n```","n":0.093}}},{"i":74,"$":{"0":{"v":"LCS","n":1},"1":{"v":"\n## Longest Common Sequence\n\n```java\npublic class LCS {\n\n    int LCS(String s1, String s2) {\n        int m = s1.length();\n        int n = s2.length();\n        int[][] dp = new int[m + 1][n + 1];\n\n        for (int i = 1; i <= m; i++) {\n            for (int j = 1; j <= n; j++) {\n                if (i == 0 || j == 0) {\n                    dp[i][j] = 0;\n                } else if (s1.charAt(i - 1) == s2.charAt(j - 1)) {\n                    dp[i][j] = 1 + dp[i - 1][j - 1];\n                } else {\n                    dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]);\n                }\n            }\n        }\n\n        return dp[m][n];\n    }\n}\n```\n\n#DynamicProgramming","n":0.101}}},{"i":75,"$":{"0":{"v":"Graph","n":1},"1":{"v":"\n## What\n\n```\n개체들 사이의 일대일 관계를 시각적으로 표현하는 수학적 모델\n그래프는 vertex(꼭지점, 정점)와 edge(간선)의 집합\nG = (V, E)\n\tV: Vertex는 개체를 나타냄\n\tE: Edge는 일대일 관계를 나타냄\n\t   Edge는 vertex의 쌍으로 표현됨\n그래프의 수학적/시각적 표현\n\tV = {0, 1, 2, 3}\n\tE = {(0,2), (0,3), (1,2), (1,3)))}\nEdge 리스트를 이용한 표현\n\t4, 4 // vertex의 수, edge의 수\n\t0, 2\n\t0, 3\n\t1, 2\n\t1, 3\n\t\n용어\nvertex\n하나의 개체를 표현~~~~\nedge\nvertex와 verte의 1:1 관계 -> (0, 1)\npath\ncycle\n첫 번째 vertex와 마지막 vertex가 일치하는 단순 경로\nconnected acylic graph = tree\n```\n\n## 그래프 탐색\n- 깊이 우선 탐색\n  - 재귀-간단\n  - 다른 인접 노드를 방문하기 전에 특정한 인접 노드를 깊이 있게 탐색\n- 너비 우선 탐색\n  - 최단 경로\n\n## Graph with adjacent list and DFS\n\n```java\nimport java.util.LinkedList;\nimport java.util.Stack;\n\npublic class Graph {\n  LinkedList<Interger>[] vertex;\n\n  public Graph(int size) {\n    this.vertex = vertex;\n    vertex = new LinkedList[size];\n    for (int i = 0; i < vertex; i++) {\n      vertex[i] = new LinkedList<>();\n    }\n  }\n\n  public void addEdge(int source, int destination) {\n    vertex[source].addFirst(destination);\n  }\n\n  public void DFS() {\n    System.out.print(\"Depth First Traversal: \");\n    boolean[] visited = new boolean[vertex.length];\n    Stack<Integer> stack = new Stack<>();\n\n    for (int startIndex = 0; startIndex < vertex; startIndex++) {\n      if (visited[startIndex] == false) {\n        stack.push(startIndex);\n        visited[startIndex] = true;\n        while (stack.isEmpty() == false) {\n          int nodeIndex = stack.pop();\n          System.out.print(nodeIndex + \" \");\n          LinkedList<Integer> adjacent = vertex[nodeIndex];\n          for (int i = 0; i < adjacent.size(); i++) {\n            int dest = adjacent.get(i);\n            if (visited[dest] == false) {\n              stack.push(dest);\n              visited[dest] = true;\n            }\n          }\n        }\n      }\n    }\n    System.out.println();\n  }\n\n  public static void main(String[] args) {\n    Graph graph = new Graph(6);\n    graph.addEdge(0, 1);\n    graph.addEdge(0, 2);\n    graph.addEdge(1, 2);\n    graph.addEdge(1, 3);\n    graph.addEdge(3, 4);\n    graph.addEdge(2, 3);\n    graph.addEdge(4, 0);\n    graph.addEdge(4, 1);\n    graph.addEdge(4, 5);\n    graph.printGraph();\n    graph.DFS();\n  }\n}\n```","n":0.062}}},{"i":76,"$":{"0":{"v":"Prim (TBD)","n":0.707}}},{"i":77,"$":{"0":{"v":"Kruskal (TBD)","n":0.707}}},{"i":78,"$":{"0":{"v":"Dijkstra","n":1},"1":{"v":"\n\n## 다익스트라 알고리즘의 수도코드\n\n```python\nfunction Dijkstra(Graph, soure):\n\tdist[source] ← 0\n\t\n\tcreate vertex priority queue Q\n\t\n\tfor each vertext v in Graph:\n\t\tif v ≠ source\n\t\t\tdist[v] ← INFINITY\n\t\t\tprev[v] ← UNDEFINED\n\t\t\n\t\tQ.add_with_priority(v, dist[v])\n\t\t\n\twhile Q is not empty:\n\t\tu ← Q.extract_min()\n\t\tfor each neighbor v of u:\n\t\t\talt ← dist[u] + length(u, v)\n\t\t\tif alt < dist[v]\n\t\t\t\tdist[v] ← alt\n\t\t\t\tprev[v] ← u\n\t\t\t\tQ.decrease_priority(v, alt)\n\t\t\t\t\n\treturn dist, prev\n```\n\n## 다익스트라 알고리즘 구현 (Python)\n\n```python\n# 그래프 인접 리스트 구성\ngraph = collections.defaultdict(list)\nfor u, v, w in info:\n\tgraph[u].append((v, w))\n\n# 큐 변수: [(거리, 정점)]\nQ = [(0, K)]\n# 최단 거리 변수\ndist = collections.defaultdict(int)\n\n# 우선순위 큐 최솟값 기준으로 정점까지 최단 경로 삽입\nwhile Q:\n\td, u = heapq.heappop()\n\tif node not in dist:\n\t\tdist[u] = d\n\t\tfor v, w in graph[u]:\n\t\t\talt = d + w\n\t\t\theapq.heappush(Q, (alt, v))\t\n```\n\n## 다익스트라 알고리즘 구현 (Java)\n\n```java\nint[] dijkstra(Graph G, int s) {\n  final int[] dist = new int[G.size()];\n  final int[] pred = new int[G.size()];\n  for (int i=0; i<dist.length; i++) {\n    dist[i] = Integer.MAX_VALUE;\n    pred[i] = false;\n  }\n\n  final boolean[] visited = new boolean[G.size()];\n\n  dist[s] = 0;\n\n  for (int i=0; i<dist.length; i++) {\n    final int u = extractMin(dist, visited);\n    visited[u] = true;\n    final int[] n = G.neighbors(u);\n    for (int j=0; j<n.length; j++) {\n      final int d =  dist[u] + G.weight(u, v);\n      if (dist[v] > d) {\n        dist[v] = d;\n        pred[v] = u;\n      }\n    }\n  }\n\n  return pred;\n}\n```","n":0.071}}},{"i":79,"$":{"0":{"v":"GCD","n":1},"1":{"v":"\n\n```python\ndef gcd(a, b):\n    while b != 0:\n        a = a % b\n        a, b = b, a\n    return a\n\nprint(gcd(30, 6)) # 6\nprint(gcd(6, 30)) # 6\n```","n":0.2}}},{"i":80,"$":{"0":{"v":"Fibonacci","n":1},"1":{"v":"\n```python\nimport collections\n\ndp = collections.defaultdict(int)\n\n# Recursive\ndef fib(N):\n    if N <= 1:\n        return N\n\n    if dp[N]:\n        return dp[N]\n\n    dp[N] = fib(N - 1) + fib(N - 2)\n    return dp[N]\n\n# Bottom-up\ndef fibo(N):\n    dp[1] = 1\n\n    for i in range(2, N + 1):\n        dp[i] = dp[i - 1] + dp[i - 2]\n\n    return dp[N]\n\ndef fibonacci(N):\n    x, y = 0, 1\n    for i in range(0, N):\n        x, y = y, x + y\n    return x \n```\n\n#DynamicProgramming","n":0.119}}},{"i":81,"$":{"0":{"v":"Count Common Sequence","n":0.577},"1":{"v":"\n```python\n# Python3 program to count common\n# subsequence in two strings\n \n# return the number of common subsequence\n# in two strings\ndef CommomSubsequencesCount(s, t):\n \n    n1 = len(s)\n    n2 = len(t)\n    dp = [[0 for i in range(n2 + 1)]\n             for i in range(n1 + 1)]\n \n    # for each character of S\n    for i in range(1, n1 + 1):\n \n        # for each character in T\n        for j in range(1, n2 + 1):\n \n            # if character are same in both\n            # the string\n            if (s[i - 1] == t[j - 1]):\n                dp[i][j] = (1 + dp[i][j - 1] +\n                                dp[i - 1][j])        \n            else:\n                dp[i][j] = (dp[i][j - 1] + dp[i - 1][j] -\n                            dp[i - 1][j - 1])        \n         \n    return dp[n1][n2]\n \n# Driver Code\ns = \"ajblqcpdz\"\nt = \"aefcnbtdi\"\n \nprint(CommomSubsequencesCount(s, t))\n```\n\n## References\n- https://www.geeksforgeeks.org/count-common-subsequence-in-two-strings/\n\n#DynamicProgramming","n":0.086}}},{"i":82,"$":{"0":{"v":"Codility","n":1},"1":{"v":"\n## Lesson 1 Iterations\n\n### BinaryGap\n```python\ndef solution(N):\n\tgaps = list(map(len, format(N, 'b')))\n\tif N % 2 == 0:\n\t\tgaps = gaps[:-1]\n\treturn max(gaps)\n```\n\n## Lesson 2 Arrays\n\n### CyclicRotation\n```python\ndef solution(A, K):\n\tk = K % len(A) if len(A) > 0 else K\n\treturn A[len(A) -k:] + A[:len(A) - k]\n```","n":0.158}}},{"i":83,"$":{"0":{"v":"Binary Tree","n":0.707},"1":{"v":"\n## 이진 트리의 최대 깊이\n\n```python\ndef maxDepth(root):\n    if root is None:\n        return 0\n\n    queue = collections.deque([root])\n    depth = 0\n\n    while queue:\n        depth += 1\n        # 큐 연산 추출 노드의 자식 노드 삽입\n        for _ in range(len(queue)):\n            cur_root = queue.popleft()\n            if cur_root.left:\n                queue.append(cur_root.left)\n            if cur_root.right:\n                queue.append(cur_root.right)\n\n    # BFS 반복 횟수 == 깊이\n    return depth\n```\n\n## 이진 트리의 지름\n\n```python\ndef diameterOfBinaryTree(root):\n    def dfs(node):\n        nonlocal diameter\n\n        if not node:\n            return 0\n\n        left = dfs(node.left)\n        right = dfs(node.right)\n\n        diameter = max(diameter, left + right)\n\n        return max(left, right) + 1\n\n    diameter = 0\n    dfs(root)\n    return diameter\n```\n\n## 이진 트리의 높이\n\n```python\ndef height(root):\n \tif root is None:\n\t\treturn 0\n\treturn 1 + max(height(root.left), height(root.right)\n\t\ndef height(root):\n\tif not root: return 0\n\th = 0\n\tqueue = collections.deque([root])\n\twhile queue:\n\t\tfor _ in range(len(queue)):\n\t\t\tcurr_node = queue.popleft()\n\t\t\tif curr_node.left:\n\t\t\t\tqueue.append(curr_node.left)\n\t\t\tif curr_node.right:\n\t\t\t\tqueue.append(curr_node.right)\n\t\th += 1\n\treturn h\n```\n\n### 이진 트리의 최소 깊이\n\n```python\ndef minDepth(root):\n\tif not root:\n\t\treturn 0\n\tif root.left and root.right:\n\t\treturn min(minDepth(root.left), minDpeth(root.right)) + 1\n\telse:\n\t\treturn minDepth(root.left or root.right) + 1 def minDepth(root):\n \tqueue = collections.dqeue((root, 1))\n \twhile queue:\n \t\tnode, level = queue.popleft()\n \t\tif node:\n \t\t\tif not node.left and not node.right:\n \t\t\t\treturn level\n \t\t\telse:\n \t\t\t\tqueue.append((node.left, level + 1))\n \t\t\t\tqueue.append((node.right, level + 1))\n```\n\n## 이진 트리 순회\n- 전위(Pre-order) 순회: NLR\n- 중위(In-order) 순회: LNR\n- 후위(Post-order) 순회: LRN\n- Recursion\n```python\ndef preorder(node):\n\tif node is None:\n\t\treturn\n\tprint(node.val)\n\tpreorder(node.left)\n\tpreorder(node.right)\n\t\ndef inorder(node):\n\tif node is None:\n\t\treturn\n\tinorder(node.left)\n\tprint(node.val)\n\tinorder(node.right)\n\t\ndef postorder(node):\n\tif node is None:\n\t\treturn\n\tpostorder(node.left)\n\tpostorder(node.right)\n\tpostorder(node.val)\n```\n- Iterative\n```python\ndef traverse_preorder(root):\n    if not root:\n        return\n\n    stack = [root]\n\n    while stack:\n        node = stack.pop()\n        print(node.val, end=\" \")\n        if node.right:\n            stack.append(node.right)\n        if node.left:\n            stack.append(node.left) def traverse_inorder(root):\n    stack = []\n    node = root\n    while stack or node:\n        while node:\n            stack.append(node)\n            node = node.left\n\n        node = stack.pop()\n\n        print(node.val, end=\" \")\n\n        node = node.right\n```\n```python\n# https://www.geeksforgeeks.org/iterative-postorder-traversal-using-stack/\n\ndef postOrderIterative(root):\n    node = root\n    stack = []\n    while True:\n        while node:\n            stack.append(node)\n            stack.append(node)\n            node = node.left\n\n        if not stack:\n            break\n\n        node = stack.pop()\n\n        # 현재 노드의 첫번째 방문일때, 오른쪽으로 이동\n        if stack and stack[-1] == node:\n            node = node.right\n        # 현재 노드를 두번째 방문 했을때 출력한다.\n        else:\n            print(\"visited\", node.val)\n            node = None  # The first is by postorder using a flag to indicate whether the node has been visited or not.\ndef postOrderTraversal(root): \t\n \ttraversal, stack = [], [(root, False)]\n \twhile stack:\n \t\tnode, visited = stack.pop()\n \t\tif node:\n \t\t\tif visited:\n \t\t\t\t# add to result if visited\n \t\t\t\ttraversal.append(node.val)\n \t\t\telse:\n \t\t\t\t# post-order\n \t\t\t\tstack.append((node, True))\n \t\t\t\tstack.append((node.right, False))\n \t\t\t\tstack.append((node.left, False))\n \t\n \treturn traversal\n \n \n# The 2nd uses modified preorder (right subtree first). Then reverse the result.\n\ndef postorderTraversal(root):\n\ttraversal, stack = [], [root]\n\twhile stack:\n\t\tnode = stack.pop()\n\t\tif node:\n\t\t\t# pre-order, right-first\n\t\t\ttraversal.append(node.val)\n\t\t\tstack.append(node.left)\n\t\t\tstack.append(node.right)\n\t\n\t# reverse result\n\treturn traversal[::-1]\n```","n":0.051}}},{"i":84,"$":{"0":{"v":"Data+AI","n":1}}},{"i":85,"$":{"0":{"v":"Visualization","n":1}}},{"i":86,"$":{"0":{"v":"Seaborn","n":1},"1":{"v":"\n## Import\n\n```python\nimport seaborn as sns\n```\n\n## Distribution plot\n\n```python\nfig, (ax1, ax2) = plt.subplots(ncols=2)\nfig.set_size_inches(12, 5)\nsns.distplot(y_train, ax=ax1, bins=50)\nax1.set(title='train')\nsns.distplot(pred, ax=ax2, bind=50)\nax2.set(title='test'))\n```","n":0.25}}},{"i":87,"$":{"0":{"v":"Unsupervised Learning","n":0.707},"1":{"v":"\nhttps://jogrammer.tistory.com/351\n\nhttps://github.com/golbanghacker/lg-swedu-ml-unsupervised\n\nhttps://github.com/golbanghacker/handson-ml2/blob/master/09_unsupervised_learni","n":1}}},{"i":88,"$":{"0":{"v":"Tensorflow","n":1}}},{"i":89,"$":{"0":{"v":"Tensorflow References","n":0.707},"1":{"v":"\n![](/assets/images/tensorflow_references.png)\n\n<케라스 창시자에게 배우는 딥러닝>\n- https://github.com/rickiepar","n":0.447}}},{"i":90,"$":{"0":{"v":"REDNet","n":1},"1":{"v":"\n아래는 <시작하세요! 텐서플로 2.0 프로그래밍>의 REDNet 네트워크 예제 코드\n\n```python\n# 9.42 Segmentation을 위한 REDNet 네트워크 정의\ndef REDNet_segmentation(num_layers):\n    conv_layers = []\n    deconv_layers = []\n    residual_layers = []\n\n    inputs = tf.keras.layers.Input(shape=(None, None, 3))\n    conv_layers.append(tf.keras.layers.Conv2D(3, kernel_size=3, padding='same', activation='relu'))\n\n    for i in range(num_layers-1):\n        conv_layers.append(tf.keras.layers.Conv2D(64, kernel_size=3, padding='same', activation='relu'))\n        deconv_layers.append(tf.keras.layers.Conv2DTranspose(64, kernel_size=3, padding='same', activation='relu'))\n\n    deconv_layers.append(tf.keras.layers.Conv2DTranspose(3, kernel_size=3, padding='same', activation='softmax'))\n\n    x = conv_layers[0](inputs)\n\n    for i in range(num_layers-1):\n        x = conv_layers[i+1](x)\n        if i % 2 == 0:\n            residual_layers.append(x)\n\n    for i in range(num_layers-1):\n        if i % 2 == 1:\n            x = tf.keras.layers.Add()([x, residual_layers.pop()])\n            x = tf.keras.layers.Activation('relu')(x)\n        x = deconv_layers[i](x) \n\n    x = deconv_layers[-1](x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=x)\n    return model\n\n# 9.43 Segmentation을 위한 REDNet 네트워크 초기화 및 컴파일\nmodel = REDNet_segmentation(15)\nmodel.compile(optimizer=tf.optimizers.Adam(0.0001),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# 9.44 Segmentation을 위한 REDNet 네트워크 학습\nhistory = model.fit(train_dataset,\n                    epochs=20,\n                    steps_per_epoch=train_data_len//16, \n                    validation_data=test_dataset, \n                    validation_steps=test_data_len)\n\n\n### References\n- https://github.com/wikibook/tf2/blob/master/Chapter9.ipynb","n":0.089}}},{"i":91,"$":{"0":{"v":"Statistics","n":1}}},{"i":92,"$":{"0":{"v":"지수 평활법 (지수 이동 평균법)","n":0.447},"1":{"v":"\n## 지수 평활법 (지수 이동 평균법, EMA, Exponential Moving Average)\n\n- 과거 데이터 중 현재에 가까운 데이터에 보다 큰 비중을 주고 과거로 갈수록 웨이트가 적어지게(지수 함수적으로 감소)하는 방법\n- 이동 평균법에 비해 현재의 상태가 직저ㄴ의 상태에 강한 영향을 받거나 상태의 변동에 가급적 추종하고 싶은 경우에 사용\n- 단기적인 예측에 적합\n- 재무상의 시계열 예측이나 주가 변동 분석 등에 사용\n- 예측 값 = a x 전회 실적치 + (1 - a) x 전회 예측치 = 전회 예측치 + a x (전회 실적치 - 전회 예측치)\n  - 전회 실적 값과 예측 값의 차이에 일정한 계수 a를 곱해서 생긴 추정치를 이전 예측치에 가감함으로써 이번 전망치를 산출\n","n":0.101}}},{"i":93,"$":{"0":{"v":"SQL","n":1}}},{"i":94,"$":{"0":{"v":"Top N","n":0.707},"1":{"v":"\n## 각 카테고리의 상위 n개 추출하기\n\n```sql\n-- # 각 카테고리의 상위 n개 추출하기\nSELECT *\nFROM\n\t-- 서브 쿼리 내부에서 순위 계산하기\n    ( SELECT\n     \t  category\n     \t, product_id\n     \t, score\n     \t  -- 카테고리별로 점수 순서로 유일한 순위를 붙임\n     \t, ROW_NUMBER()\n     \t\tOVER(PARTITION BY category ORDER BY score DESC)\n     \t  AS rank\n       FROM popular_products\n     ) AS popular_products_with_ranks\n-- 외부 쿼리에서 순위 활용해 압축하기\nWHERE rank <= 3\nORDER BY category, rank\n;\n```\n\n## 각 카테고리에서 최상위 상품 추출하기\n\n```sql\n-- # 각 카테고리에서 최상위 상품 추출하기\n-- DISTINCT 구문을 사용해 중복 제거하기\nSELECT DISTINCT\n\t  category\n      -- 카테고리별로 순위 최상위 상품 ID 추출하기\n    , FIRST_VALUE(product_id)\n      OVER(PARTITION BY category ORDER BY score DESC\n           ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\n      AS product_id\nFROM popular_products\n;\n```","n":0.097}}},{"i":95,"$":{"0":{"v":"Test","n":1}}},{"i":96,"$":{"0":{"v":"Select","n":1},"1":{"v":"\n```sql\nSELECT * FROM table WHERE a IN ('1', '2', '3')\n```","n":0.316}}},{"i":97,"$":{"0":{"v":"Ranking","n":1},"1":{"v":"\n## 순위 구하기 (ROW_NUMBER, RANK, DENSE_RANK 이해)\n\n```sql\nSELECT\n    product_id\n  , score\n    -- score 순서로 유일한 순위를 붙임\n  , ROW_NUMBER() OVER(ORDER BY score DESC) AS row\n    -- 같은 순위를 허용하여 순위를 붙임\n  , RANK()       OVER(ORDER BY score DESC) AS rank\n    -- 같은 순위가 있을 때 같은 순위 다음에 있는 순위를 건너 뛰고 순위를 붙임\n  , DENSE_RANK() OVER(ORDER BY score DESC) AS dense_rank\nFROM\n    popular_products\nORDER BY\n    row\n;\n```\n","n":0.126}}},{"i":98,"$":{"0":{"v":"SQL로 날짜 다루기","n":0.577},"1":{"v":"\n## 한 주 전 날짜 구하기 (KST)\n\n```sql\nDECLARE @today VARCHAR(10)\nDECLARE @1weekago VARCHAR(10)\nSET @today = CONVERT(DATE, DATEADD(HOUR, 9, GETDATE())) -- 2022-11-23\nSET @1weekago = CONVERT(DATE, DATEADD(DAY, -6, @today)) -- 2022-11-17\nPRINT(@today)\nPRINT(@1weekago)\n```","n":0.192}}},{"i":99,"$":{"0":{"v":"Spark","n":1},"1":{"v":"\n## 중요 정보\n- 스파크는 병렬 분산 환경을 의식하지 않고 처리를 기술할 수 있는 것을 목표로 한다.\n\n\n## Terms\nterm|description\n---|---\nRDD|Resilient Distributed Dataset, 내결함성 분산 데이터셋\n\n\n## Spark Session\n```python\nspark = SparkSession.builder().getOrCreate()\n```\n\n\n## UDF\n```python\n@udf\ndef extract(x):\n    # do something\n\nextract = udf(functools.partial, extract, 'key')\n```\n\n## 브로드캐스트 변수\n- 복수의 executor가 공통으로 참조하는 데이터는 *브로드캐스트 변수*로 배포\n\n## Dataframe Source\n- https://github.com/apache/spark/blob/master/python/pyspark/sql/dataframe.py\n\n## Reference\n- 아파치 스파크 입문\n- https://github.com/spark-in-action/first-edition","n":0.137}}},{"i":100,"$":{"0":{"v":"Terms","n":1},"1":{"v":"\n용어|원어|설명|출처\n---|---|---|---\n내결함성|Fault Tolerance|결함 또는 고장이 발생해도 정상적 혹은 부분적으로 기능을 수행할 수 있는 능력|아파치 스파크 입문, p31\n스케일업||한 대의 서버에 있는 리소스를 늘리고 강화함으로써 처리 능력이 높은 환경을 마련|아파치 스파크 입문, p33\n스케일 아웃||여러 대의 서버를 통한 병렬분산처리\n병렬 처리|parrallelization|전체 연산을 잘게 나누어 동시에 처리하는 방법\n데이터 분산|data distribution|데이터를 여러 노드로 분산하는 방법\n장애 내성|fault tolerance|분산 컴포넌트의 장애에 대응하는 방법\n\n","n":0.136}}},{"i":101,"$":{"0":{"v":"Structured Streaming","n":0.707},"1":{"v":"\n## What\n\n- Structured stream에서 data stream은 계속해서 append되는 테이블로 간주됨\n\n![](https://learn.microsoft.com/en-us/azure/databricks/_static/images/getting-started/gsasg-spark-streaming-workflow.png)\n\n## Why\n\n- 이렇게 하면 stream processing model을 batch processing model처럼 처리할 수 있음\n\n![](https://learn.microsoft.com/en-us/azure/databricks/_static/images/getting-started/gsasg-spark-streaming-model.png)\n\n- 시간이 지날수록 새로운 row가 입력 테이블에 append되는 꼴이 되며 (Input), 이 테이블을 query하면 (Query), 결과 테이블도 계속 업데이트됨 (Output)\n\n## How\n\n### Initialize the stream\n```python\nstreamingInputDF = (\n  spark\n    .readStream\n    .schema(jsonSchema)\n    .option(\"maxFilesPerTrigger\", 1)\n    .format(\"json\")\n    .load(inputPath)\n)\n\nstreamCountsDF = (\n  streamingInputDF\n    .groupBy(\n      streamingInputDF.action,\n      window(streamingInputDF.time, \"1 hour\"))\n    .count()\n)\n```\n\n### Start the streaming job\n```python\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")\n    .queryName(\"counts\")\n    .outputMode(\"complete\")\n    .start()\n)\n```\n\n## Trigger\n\n### 시간 기반 트리거 간격 지정\n- Structured Streaming의 시간 기반 trigger는 기본적으로 500ms의 고정 간격 micro-batches\n- `processTime` 키워드를 사용하여 기간(예: `.trigger(processingTime='10 seconds'))`을 문자열로 지정\n\n## 증분 일괄 처리 구성\n- 원본 디렉터리의 모든 새 데이터를 단일 micro-batch로 처리하려면 `.trigger(once=True)` 옵션을 사용\n- 스트리밍 입력 크기가 큰 경우 `.trigger(availableNow=True)` 옵션 사용\n\n## Reference\n- https://learn.microsoft.com/en-us/azure/databricks/getting-started/streaming\n- https://learn.microsoft.com/en-us/azure/databricks/structured-streaming/triggers","n":0.089}}},{"i":102,"$":{"0":{"v":"클러스터 구성 최적화","n":0.577},"1":{"v":"\n## Worker 노드를 하나만 사용\n\n스파크 클러스터는 병렬 연산이 가능하고 네트워크로 연결된 머신(즉, 노드)의 집합이다. 스파크에서 데이터를 로드하면 데이터는 (row를 기준으로) 여러 파티션으로 분할되어 클러스터 노드에 고르게 분산 저장된다.\n\n이때, groupBy로 집계를 하고 join이나 union을 통해 데이터를 통합하는 등의 작업을 수행하면, 각 파티션에 나누어 저장된 데이터는 다른 파티션으로 물리적인 이동(shuffling)을 하게 된다. 이는 네트워크 및 디스크 I/O를 야기한다.\n\n따라서, 조인(join)이나 집계(aggregation) 등 광범위한 데이터 변환이 발생하는 셔플링(shuffling)이 빈번한 작업에는 가급적 적은 수의 노드로 클러스터를 구성하는 것이 좋다. 이와 반대로 간단한 ETL 작업(노드 사이에 데이터 이동이 발생하지 않고 각 노드의 파티션의 데이터만 사용하여 변환하는 작업)에는 연산에 최적화된 클러스터 구성을 시도해 볼 수 있다.\n\n\n## References\n\n- https://docs.microsoft.com/ko-kr/azure/databricks/clusters/cluster-config-best-practices","n":0.1}}},{"i":103,"$":{"0":{"v":"Spark Overview","n":0.707},"1":{"v":"\n\n### RDD\n- RDD\n    - 분산 컬렉션 자료구조로 대량의 데이터를 요소(element)로 가짐\n        - 여러 머신으로 구성된 클러스터 환경에서 분산 처리를 전제로 설계됨\n    - 내부는 `partition`이라는 단위로 나뉨\n        - partition\n            - RDD를 구성하는 객체\n            - 스파크에서는 이 파티션이 분산 처리 단위\n            - RDD를 파티션 단위로 독립해 여러 머신에서 분산 처리\n    - 변경할 수 없는 형태의 분산된 객체들의 모음\n    - executor 또는 worker node에 저장됨\n\n- RDD 처리\n    - transformation(변환)\n        - RDD를 가공하고 그 결과로 새로운 RDD를 얻는 처리\n        - shuffle(셔플)\n            - 서로 다른 파티션에 있는 같은 키를 가지는 요소의 자리를 바꾸는 것\n            - executor 간의 다대다 네트워크 통신이 필요해짐 ~ 부하가 높은 처리\n        - narrow transformation : shuffle이 발생하지 않음\n            - 하나의 `stage` 안에서의 연산\n        - wide transformation : shuffle이 수반됨\n            - 각 partition에서 계산해야 하는 데이터들이 여러 머신에 나누어져 있기 때문에\n            - shuffle이 완료된 후에 이후 연산이 시작됨\n            - `정렬`은 단순히 각 파티션에서만 정렬되는 것이 아니라 모든 레코드들이 정의된 순서에 따라 정렬되어 있어야 하므로 narrow transformation으로는 해결할 수 없음\n            \n            ![](https://images.squarespace-cdn.com/content/v1/5bce4071ab1a620db382773e/dbbdd8e6-5f2a-45f5-a232-825dca3fa816/Narrow+Transformation.png?format=500w) | ![](https://images.squarespace-cdn.com/content/v1/5bce4071ab1a620db382773e/282af54b-d7d1-427e-b602-fdb3effc59a4/Wide+Transformation.png?format=500w)\n            ---|---\n            Narrow Transformation|Wide Transformation\n            filter()<br>contains()<br>map()<br>flatMap()<br>MapPartition()<br>sample()<br>union()<br>coalesce() --> when numPartitions is reduced<br>drop()<br>cache()|orderBy()<br>repartition()<br>distinct()<br>collect()<br>cartesian()<br>intersection()<br>reduceByKey()<br>groupByKey()<br>groupBy()<br>join()\n        - transformation은 자신에게 의존하는 action이 호출되기 전까지는 실행되지 않음\n    - action(액션)\n        - RDD 내용을 바탕으로 더이상 데이터를 가공하지 않고 원하는 결과를 얻는 조작\n        - 스파크 RDD에 분산된 데이터를 RDD 밖으로 꺼내는 행위\n            - 예: 데이터를 드라이버에 되돌려 주거나 저장 시스템에 저장\n        - action은 DAG에서 leaf 노드 형태가 됨\n        - 스파크 스케줄러는 각 action마다 실행 그래프를 만들고 스파크 잡을 시작 \n\n- 스파크 클러스터 매니저\n    - 스파크 애플리케이션에서 설정한 parameter에 따라 분산 시스템에 executor들을 실행하고 분산해 주는 역할\n    - Executors are launched by the cluster manager on behalf of the driver.\n\n- 스파크 실행 엔진\n    - 연산을 위해 executor들에게 데이터를 분산해 주고 실행을 요청\n\n- 지연 평가\n    - 스파크 애플리케이션은 드라이버 프로그램이 action을 호출하기 전까지는 '아무것도' 하지 않음\n\n- Job\n    - 스파크 애플리케이션에서는 RDD 생성부터 액셔 적용까지를 통틀어 job이라는 단위로 처리\n    - 스파크 job은 하나의 최종 결과를 연산해 내는 데 필요한 RDD transformation들의 집합\n    - 각 job은 최종 RDD를 만들어 내는 데 필요한 데이터 변환(transformation)의 각 단계를 의미하는 stage들로 구성됨\n\n- Stage\n    - job의 일부분으로 하나의 executor에서 계산 가능한 task들의 집합\n    - 하나의 stage는 partition들끼리 데이터가 전송되는 일 없이 병렬로 연산이 가능한 단위\n    - shuffle이 발생할 때 시작됨\n\n- Task\n    - 한 stage 안에서 데이터의 각 partition에 대한 작업을 수행하는 단위\n\n![](/assets/images/spark_application.png)\n    - 분산 시스템에서 스파크 애플리케이션 시작하기\n\n![](https://images.squarespace-cdn.com/content/v1/5bce4071ab1a620db382773e/57966b9e-d905-4bb0-95c8-d2ca5def572d/Spark+Execution+Hierarchy.png?format=2500w)\n    - Job - Stage - Task\n\n- RDD 영속화(persistent)\n    - executor는 task 처리를 끝낼 때, 처리 과정에서 생성된 partition 인스턴스를 영속화하는 경우가 있음\n    - Case#1 : 셔플이 발생하는 transformation을 실행하기 직전의 RDD\n    - Case#2 : 사용자에 의해서 명시적으로 영속화가 선언된 RDD\n\n\n- Broadcast variables\n    - Broadcast variables are shared, immutable variables that are cached on every machine in the cluster instead of serialized with every single task\n\n## References\n- 하이 퍼포먼스 스파크\n- 아파치 스파크 입문\n- https://www.advancinganalytics.co.uk/blog/2022/6/8/tips-for-the-databricks-certified-associate-developer-for-apache-spark-30","n":0.047}}},{"i":104,"$":{"0":{"v":"Spark 분산 처리 최적화","n":0.5},"1":{"v":"\n## 1. Partitioning (파티셔닝)\n\n- Spark는 데이터를 파티션별로 나눠 처리함. 적절한 파티션 크기와 수를 선택함으로써, 효율적인 데이터 처리와 리소스 사용이 가능함.\n- `repartition()` 및 `coalesce()` 함수를 사용하여 데이터 파티션 수를 조절할 수 있음.\n\n## 2. Persisting (데이터 지속성)\n\n- 자주 사용하는 RDD나 DataFrame은 메모리에 캐시하거나 디스크에 저장하여 재계산 없이 빠르게 액세스 할 수 있도록 함.\n- `persist()` 또는 `cache()` 메서드를 사용하여 선택적으로 저장 수준을 지정할 수 있음.\n\n## 3. Broadcasting (브로드캐스팅)\n\n- 큰 DataFrame과 작은 DataFrame을 조인할 때, 작은 DataFrame을 모든 노드에 복사하여 네트워크 트래픽을 최소화 할 수 있음.\n\n## 4. Avoiding Shuffling (셔플링 피하기)\n\n- 셔플링은 클러스터 전체의 데이터 재분배를 의미하며, 비용이 많이 듦. 가능한한 셔플링을 최소화하는 연산을 선택하거나, 셔플링이 필요한 연산을 최적화함.\n\n## 5. Tuning Spark Configurations (Spark 설정 튜닝)\n\n- Spark의 설정 옵션은 작업의 특성에 따라 조정될 수 있음\n- 예를 들면, `spark.executor.memory`나 `spark.driver.memory`와 같은 설정을 통해 자원 할당량을 조절할수 있음.\n\n## 6. Using DataFrames over RDDs\n\n- DataFrame은 Catalyst 옵티마이저를 활용화여 연산을 최적화함.\n- 가능하면 RDD 대신 DataFrame API를 사용하는 것이 좋음.\n\n## 7. Monitoring and Profiling (모니터링 및 프로파일링)\n\n- Spark UI는 실행 중인 작업의 성능과 병목 현상을 모니터링하는데 유용함. 이를 통해 잠재적인 문제를 발견하고 최적화 할 수 있음.","n":0.075}}},{"i":105,"$":{"0":{"v":"Dataframe","n":1},"1":{"v":"\n## Spark SQL\n\n> Spark SQL은 옵티마이저에 의해 제어되며 사용자가 제어권을 포기\n\n## Dataframe 생성\n\n```python\ndf = spark.createDataFrame([\n  ('01', 'aws'),\n  ('02', 'azure'),\n  ('03', 'gcp')\n], ())\ndf.show()\n```\n\n```python\nspark.createDataFrame([(\"summer\", 4.5), (\"winter\", 7.5)], [\"season\", \"wind_speed_ms\"])\n```\n\n### from Python Dictionary\n\n```python\ndata = [\n  {'id': 10, 'name': 'Gildong', 'address': 'Seoul'},\n  ...\n]\ndf = spark.createDataFrame([data])\n```\n\n## 데이터 읽기\n```python\nspark.read.format(\"csv\").option(\"header\", True).load(filePath)\n```\n\n```python\ndf = spark.read.parquet(\"path/to/x.parquet\")\n```\n```python\n+------+-----------------------------+-------------------+\n|itemId|attributes                   |supplier           |\n+------+-----------------------------+-------------------+\n|1     |[blue, winter, cozy]         |Sports Company Inc.|\n|2     |[red, summer, fresh, cooling]|YetiX              |\n|3     |[green, summer, travel]      |Sports Company Inc.|\n+------+-----------------------------+-------------------+\n\nitemsDfSchema = StructType([\n  StructField(\"itemId\", IntegerType()),\n  StructField(\"attributes\", ArrayType(StringType())),\n  StructField(\"supplier\", StringType())\n])\nitemsDf = spark.read.schema(itemsDfSchema).parquet(filePath)\n```\n\n```python\n# return the number of columns in the CSV file stored at location filePath. From the CSV file, only lines should be read that do not start with a # character\nlen(spark.read.csv(filePath, comment='#').columns)\n# comment – sets a single character used for skipping lines beginning with this character. By default (None), it is disabled.\n```\n\n- https://www.marks4sure.com/Databricks-Certified-Associate-Developer-for-Apache-Spark-3-0-exam.html\n\n\n## 데이터 저장\n```python\n# 하나의 파일로 저장\ndf.coalease(1).write.format().mode('overwrite').option('header', 'true').save(path)\n```\n\n```python\n# division으로 partitioning 후 저장\ndf.write.partitionBy(\"division\").parquet(filePath)\n```\n\n```python\n# 데이터 추가를 위해 저장 모드 지정\ndef writeAppend(input: DataFrame):\n  input.write.mode(\"append\").save(\"output/\")\n```\n\n```python\n# 우편번호로 파티션해서 저장\ndef writeOutByZip(input: DataFrame):\n  input.write.partitionBy(\"zipcode\").format(\"json\").save(\"output/\")\n```\n\n## 컬럼 조작\nWhat|How\n---|---|---\n선택    |   df.select('id', 'body')\n제거\t|   df.drop('body') <br> df.drop('productId', 'value')\n필터링\t|   df.filter(col(\"col_1\") &\\| col(\"col_2)).limit(10)\n추가\t|   df.withColumn('col', lit(1))\n이름 변경\t|   df.withColumnRenamed('ownerUserId', 'owner')\n정렬\t|   df.orderBy() <br> df.sort()\n조건\t|   when((f.col('a').cast(IntegerType()) > 0), 1).otherwise(0)\n이름 변경\t|   col_name = [...] <br> df.toDF(*col_name)\n\n## 데이터 처리\nWhat|How\n----|---\n필터\t|df.filter\n정렬\t|df.orderBy()\n집약\t|df.agg(avg(col(\"kcal\")) as \"\") <br> df.groupBy(...).agg(max(~).alias(~))\n결합\t|df.join(df2, on=[...], how=\"inner/left\") <br> df.join(df2, df['id'] == postId, 'outer')\n집계\t| df.groupBy().agg… <br> df.distinct()\n\n\n## SQL 활용\n```python\ndf.registerTempTable(\"dessert_table\")\ndf.createOrReplaceTempView(\"dessert_table\")\nnumOver300KcalDf = spark.sql(\"SELECT cound(*) AS num_of_over_30Kcal FROM dessert_table WHERE kcal >= 260\")\n```\n\n## Take\n\n```python\ntransactionDf.filter(col(\"storeId\")==25).take(5)\n```\n\n## Distinct\n특정 컬럼의 unique values\n\n```python\ntransactionDf.select(\"storeId\").distinct()\n```\n\n## Partition 변경\n\n- `partitionBy`: 셔플링을 통해 새로운 RDD를 생성\n- `coalesce`: 파티션 개수를 줄이거나(shuffle=false) 늘림(shuffle=true)\n  - 셔플링을 피하기 위해 같은 worker 안에서 partition들을 병합\n  - partition의 개수가 줄어들면서 (~병합되면서) partition 크기의 편향이 발생할 수 있음\n  \n  ![](https://images.squarespace-cdn.com/content/v1/5bce4071ab1a620db382773e/d14468bd-009b-4f2f-9d84-7c752862b1b8/Before+Repartition.png?format=1000w)\n- `repartition`: coalesce(n, shuffle=true)와 동일\n  ![](https://images.squarespace-cdn.com/content/v1/5bce4071ab1a620db382773e/4f68dd4e-fe65-4106-96c4-1060e9ab8569/After+Repartition.png?format=1000w)\n\n## 컬럼 split\n\n```python\nfrom pyspark.sql.functions import split\n\n(df.withColumn(\n    \"firstName\", split(df[\"name\"], \" \").getItem(0)\n  ).withColumn(\n    \"lastName\", split(df[\"name\"], \" \").getItem(1)\n  )\n)\n```\n\n## 컬럼 이름 변경\n\n```\ndf.withColumnRenamed(`old column name`, `new colun name`)\n```\n\n## 결측 값 다루기\n\n```python\ndf.na.drop() # df.na.drop(\"any\")\ndf.na.drop(\"all\")\n# df.na.drop(col(\"col_name\"))\n# df.na.fill(map(col(\"count\"), 0)))\n\ndf.dropna(thresh=4) # ㅁhave missing data in a least 3 columns\n```\n\n## 첫번째 줄\n\n```cmd\n>>> df.first()\nRow(age=2, name='Alice')\n>>> df.first().name\nAlice\n```\n\n## 조건에 따른 boolean \n\n```python\ntransactionDf.select((col(\"storeId\").between(20, 30)) & (col(\"productId\")==2))\n```\n- https://www.marks4sure.com/databricks-certified-associate-developer-for-apache-spark-3-0-databricks-certified-associate-developer-for-apache-spark-3-0-exam-questions.html\n\n## 시용자 정의 함수\n\n```python\nspark.udf.register(\"ACCESS_PERFORMANCE\", assessPerformance)\nspark.sql(\"SELECT customerSatisfactgion, ASSESS_PERFORMANCE(customerSatisfaction) AS result FROM stores\")\n```\n\n```python\nassessPerformanceUDF = udf(assessPerformance, IntegerType())\nstoredDF.withColumn(\"result\", assessPerformanceUDF(col(\"customerSatisfaction\")))\n```\n\n## UNIX epoch format에서 월(month) 추출\n\n```python\nstoredDF.withColumn(\"openTimestamp\", col(\"openDate\").cast(\"Timestamp\")) \\\n        .withColun(\"month\", month(col(\"openTimestamp\")))\n```\n\n## Join\n\n> 스파크의 기본 join 구현은 `shuffled hash join`\n  - 양쪽 RDD에서 같은 해시값의 키들이 같은 파티션에 모이도록 데이터를 이동(shuffle)\n\n`df1.join(df2, joinExpression, joinType)`\n\n```python\n# inner join\ndf.join(newdf, df.id == newdf.id, \"inner\")\ndf1.join(df2, df1.col(\"id\")==df2.col(\"id\"), \"inner\")\n\n# outer join\nstoresDF.join(employeesDF, \"storeID\", \"outer\")\n\n# multiple keys\nstoresDF.join(employeesDF, [\"storeId\", \"employeeId\"])\n```\n> Join은 일상적으로 쓰이는 스파크의 연산 중 가장 비싼 축에 속한다. Join을 수행하기 전에 필터링을 적용하여 데이터의 크기를 최대한 줄여 놓으면 더 빠르게 join할 수 있다.\n\n\n## Broadcast join\n\n- Join하려는 두 테이블 중 하나가 다른 쪽보다 작은 경우 작은 쪽의 RDD를 큰 쪽의 worker node에 복사 --> join 시 발생하는 shuffle이 일어나지 않도록 하여 join 속도를 올림\n  ```python\n  transactionDf.join(broadcast(itemsDf), \"transactionId\", \"left_semi\")\n  ```\n- [[dev.data+ai.spark.config]]의 `spark.sql.autoBroadcastJoinThreshold` 설정으로 테이블을 자동으로 브로드캐스팅할수도 있음\n\n## Explode\n\n```\n+------+-----------------------------+-------------------+\n|itemId|attributes                   |supplier           |\n+------+-----------------------------+-------------------+\n|1     |[blue, winter, cozy]         |Sports Company Inc.|\n|2     |[red, summer, fresh, cooling]|YetiX              |\n|3     |[green, summer, travel]      |Sports Company Inc.|\n+------+-----------------------------+-------------------+\n```\n\n```python\narticlesDf.select(explode(col('attributes))) \\\n          .groupBy('col').count() \\\n          .sort('count', ascending=False) \\\n          .select('col')\n```\n\n```\n+-------+\n| col   |\n+-------+\n| summer|\n| winter|\n| blue  |\n| cozy  |\n| travel|\n| fresh |\n| red   |\n| cooling|\n| green |\n+-------+\n```\n\n## 그룹에서 순위\n\n```scala\n// 데이터\nval peopleDF = Seq(\n  (\"Ali\", 0, Seq(100)),\n  (\"Barbara\", 1, Seq(300, 250, 100)),\n  (\"Cesar\", 1, Seq(350, 100)),\n  (\"Dongmei\", 1, Seq(400, 100)),\n  (\"Eli\", 2, Seq(250)),\n  (\"Florita\", 2, Seq(500, 300, 100)),\n  (\"Gatimu\", 3, Seq(300, 100))\n).toDF(\"name\", \"department\", \"score\")\n\n// 기대 결과\n|department| name |highest|\n| 0| Ali| 100|\n| 1|Dongmei| 400|\n| 2|Florita| 500|\n| 3| Gatimu| 300|\n\n// 솔루션\nimport org.apache.spark.sql.expressions.Window\nval windowSpec = Window.partitionBy(\"department\").orderBy(col(\"score\").desc)\n\npeopleDF\n.withColumn(\"score\", explode(col(\"score\")))\n.select(col(\"department\"),col(\"name\"),dense_rank().over(windowSpec).alias(\"rank\"), max(col(\"score\")).over(windowSpec).alias(\"highest\"))\n.where(col(\"rank\") === 1)\n.drop(\"rank\")\n.orderBy(\"department\")\n.show()\n```","n":0.043}}},{"i":106,"$":{"0":{"v":"Spark Config","n":0.707},"1":{"v":"\n## 파티션 수\n\n`spark.sql.shuffle.partitions`: 셔플이 끝난 후의 RDD 파티션 수 (default: 200)\n\n`spark.sql.autoBroadcastJoinThreshold`: broatcast() 연산을 사용하지 않고도 브로드캐스팅을 설정 (default: 10MB)\n\n\n## References\n- 아파치 스파크 입문, p188","n":0.204}}},{"i":107,"$":{"0":{"v":"Cluster","n":1},"1":{"v":"\n## 스파크 런타임 컴포넌트\n\n### 클라이언트 프로세스\n- 드라이버 프로그램을 시작\n    > Spark 프로그램을 실행하는 방법\n    1. spark-submit 명령으로 프로그램을 제출\n    2. Spark shell에서 프로그램을 실행\n    3. 별도의 애플리케이션에서 SparkContext 객체를 초기화 및 설정\n\n### 드라이버의 역할\n\n- 스파크 애플리케이션의 실행을 관장하고 모니터링 (스파크 애플리케이션을 감싸는 일종의 래퍼 역할)\n    - 클러스터 매니저에 메모리 및 CPU 리소스를 요청\n    - 애플리케이션 로직을 stage와 task로 분할\n    - 여러 executor에 task를 전달\n    - task 실행 결과를 수집\n\n- 드라이버 프로그램 배포 모드\n    - 클라이언트 모드\n        - 드라이버를 클라이언트의 JVM 프로세스에서 실행\n        \n            ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FOZH7c%2FbtqYhydgOoQ%2FGZkzj48MpqcrdutMdnDAL0%2Fimg.png)\n    - 클러스터 모드\n        - 드라이버 프로세스를 클러스터 내부에서 별도의 JVM 프로세스로 실행하며, 드라이버 프로세스의 리소스(주로 JVM 힙 메모리)를 클러스터가 관리\n        > When Spark runs in Cluster Mode, the Spark Driver runs in a worker node inside the cluster. In <b>Cluster Mode</b>, the cluster manager launches the driver process on a worker node inside the cluster, in addition to the executor processes. This means that the cluster manager is responsible for maintaining all Spark worker nodes. Therefore, the cluster manager places the driver on a worker node and the executors on separate worker nodes.\n        \n        ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbIPFml%2FbtqX32tKwYS%2FU6KUycWZRuw1pKkkRgRMa0%2Fimg.png)\n    - 로컬 모드\n        - spark-submit 명령을 실행한 후 클라이언트 상에서 프로세스를 구동하고, 해당 프로세스 안에서 executor를 구동하여 애플리케이션을 실행하는 동작 모드\n            - `spark-submit --master local` # 1개의 executor와 1개의 core로 실행\n            - `spark-submit --master local[N]` # 1개의 executor와 N개의 core로 실행\n            - `spark-submit --master local[*]` # 1개의 executor와 로컬 머신이 갖고 있는 만큼의 core로 실행\n        \n        ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcU9nst%2FbtqX1pW1SsP%2FxRPXLbjhYuTXG6aqGiomb0%2Fimg.png)\n\n\n### Executor의 역할\n- Executor는 드라이버가 요청한 task들을 받아서 실행하고, 그 결과를 드라이버로 반환하는 JVM 프로세스\n- 각 executor는 드라이버가 요청한 tasks들을 여러 task slot에서 병렬로 실행\n    - 일반적으로 task slop 개수는 CPU 코어 개수의 두세 배 정도로 설정\n\n### SparkContext의 생성\n- 드라이버는 SparkContext 인스턴스를 생성하고 시작\n    - SparkContex는 RDD를 생성하거나 데이터를 로드하는 등 다양한 작업을 수행하는 여러 유용한 메서드를 제공하며, 스파크의 런타임 인스턴스에 접근할 수 있는 기본 인터페이스\n\n\n## 주요 Configuration\n\n### Application configuration\n\nconfiguration|description\n---|---\nspark.driver.cores  |   드라이버가 사용할 코어 수. 클러스터 모드에서만 유효하며 기본값은 1\nspark.driver.maxResultSize  |   collect() 결과로 생성된 결과 값의 최대 크기. 이 값 초과 시 전체 job은 실패로 종료됨. 기본값은 1g\nspark.executor.memory   |   executor 하나의 메모리 크기. 기본값 1g\n\n### Executor configuration\n\nconfiguration   | description\n---|---\nspark.executor.cores    |   executor에 할당할 코어 수 지정\n\n## Configuration 등록 방법\n\n1. Spark 스크립트로 SparkSession 생성 시 지정\n2. spark shell이나 spark-submit에서 명령행 매개변수로 지정\n3. spark-defaults.conf로 지정\n\n## References\n- 스파크를 다루는 기술 (Spark in Action)\n- https://m.blog.naver.com/occidere/221609076332","n":0.052}}},{"i":108,"$":{"0":{"v":"References","n":1},"1":{"v":"\nhttps://github.com/ml874/Cracking-the-Data-Science-Interview","n":1}}},{"i":109,"$":{"0":{"v":"Pandas","n":1}}},{"i":110,"$":{"0":{"v":"Visualization","n":1},"1":{"v":"\n## Histogram\n\n```python\ntrain_data[\"count\"].hist()\n```","n":0.707}}},{"i":111,"$":{"0":{"v":"Pandas로 SQL 쿼리","n":0.577},"1":{"v":"\n## SQL Query\n\n```python\nuser_id = \"user001\"\nquery = f\"SELECT * FROM user_info WHERE id='{user001}'\"\npd.read_sql(query, cnxn)\n```\n\n## Stored Procedure\n\n[[dev.cs.database.stored-procedure]]를 호출하려면 `read_sql_query`를 사용한다.\n\n```python\nuser_id = \"user001\"\nquery = \"EXEC dbo.sp_getUserInfo @id='{user_id}'\"\npd.read_sql_query(query, cnxn)\n```\n","n":0.2}}},{"i":112,"$":{"0":{"v":"결측치 확인 및 처리","n":0.5},"1":{"v":"\n## 결측치 확인\n\n```python\ndf.info()\ndf.isnull().sum()\n```\n\n## 결측치 제거\n\n`DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)`\n- axis=1 : 컬럼을 기준으로 동작\n- how='any' : 결측치가 하나라도 포함된 행 삭제\n- how='all' : 모든 데이터가 결측치인 행 삭제\n- thresh=숫자 : 숫자 이상의 데이터를 가진 행은 삭제하지 않음\n- subset=['col_name1', 'col_name2', ...] : subset으로 대상 컬럼 지정\n\n```python\ndf_dropped = df.dropna(how='all', subset=['col1', 'col2', 'col3'])\n\n# 제거된 행의 개수 확인\nprint(len(df) - len(df_dropped))\n```\n\n## 결측치 대체 (TBD)","n":0.13}}},{"i":113,"$":{"0":{"v":"Memo (TBD)","n":0.707},"1":{"v":"\n> 정리 필요\n\n```python\n   \n1. Imputation\n\nthreshold = 0.7\n\n# Dropping columns with missing value rate higher than threshold\ndata = data[data.columns[data.isnull().mean() < threshold]\n\n# Dropping rows with missing value rate higher than threshold\ndata = data.loc[data.isnull().mean(axis=1) < threshold]\n\n# Filling all missing values with 0\ndata = data.fillna(0)\n\n# Filling missing values with medians of the columns\ndata = data.fillna(data.median())\n\n# Max fill function for categorical columns\ndata['column_name'].fillna(data['column_name'].value_counts().idxmax(), inplace=True)\n\n2. Hangling Outliers\n\n# Dropping the outlier rows with standard deviation\nfactor = 3\nupper_lim = data['column'].mean() + data['column'].std() * factor\nupper_lim = data['column'].mean() + data['column'].std() * factor\n\ndata = data[(data['column'] < upper_lim) & (data['column'] > lower_lim) ]       \n\n# Dropping the outlier rows with Percentiles\nupper_lim = data['column'].quantile(.95)\nlower_lim = data['column'].quantile(.05)\n\ndata = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]\n\n3. Binning\n\n# Numerical Binning Example\n\nValue\t   Bin\n0-30\t-> Low\n31-70\t-> Mid\n71-100\t-> High     \n\n# Categorical Binning Example\n\nValue\t   Bin\nSpain\t-> Europe\nItaly\t-> Europe\nChile\t-> South America\nBrazil\t-> South America      \n\n# Numerical Binning Example\n\ndata['bin'] = pd.cut(data['value'], bins=[0,30,70,100], labels=[\"Low\", \"Mid\", \"High\"])\n\n    value   bin\n 0      2   Low\n 1     45   Mid\n 2      7   Low\n 3     85  High\n 4     28   Low  \n\n# Categorical Binning Example\n\n      Country\n 0      Spain\n 1      Chile\n 2  Australia\n 3      Italy\n 4     Brazil   \n\nconditions = [\n\tdata['Country'].str.contains('Spain'),\n\tdata['Country'].str.contains('Italy'),\n\tdata['Country'].str.contains('Chile'),\n\tdata['Country'].str.contains('Brazil')  \n]\n\nchoices = ['Europe', 'Europe', 'South America', 'South America']  \n\ndata['Continent'] = np.select(conditions, choices, default='Other') \n\n4. Log Transform\n\n...\n\n5. One-hot encoding\n\nencoded_columns = pd.get_dummies(data['column'])\ndata = data.join(encoded_columns).drop('column', axis=1)\n\n6. Group Operations\n\ndata.groupby('id').agg(lambda x: x.value_counts().index[0])\n\n# Pivot table Pandas Example\n\ndata.pivot_table(index='column_to_group', columns='column_to_encode', values='aggregation_column', aggfunc=np.sum, fill_value = 0)\n\ngrouped = data.groupby('column_to_group')\n\nsums = grouped[sum_cols].sum().add_suffix('_sum')\navgs = grouped[mean_cols].mean().add_suffix('_avg')\n\nnew_df = pd.concat([sums, avgs], axis=1)  \n\n7. Feature Split\n\ndata.name.str.split(\" \").map(lambda x: x[0])\ndata.name.str.split(\" \").map(lambda x: x[-1]) \n\ndata.title.head()\n\ndata.title.str.split(\"(\", n=1, expand=True)[1].str.split(\")\"), n=1, expand=True)[0]\n\n8. Scaling\n\n# Normalization\n\ndata = pd.DataFrame({'value': [2, 45, -23, 85, 28, 2, 35, -12})      \n\ndata['normalized'] = (data['value'] - data['value'].min()) /\n\t(data['value'].max() - data['value'].min())\n\n# Standardization\n\ndata['standardized'] = (data['value'] - data['value'].mean()) / \n\tdata['value'].std()\n\n9. Extracting Date\n\nfrom datetime import date\n\ndata = pd.DataFrame({'date':['01-01-2017','04-12-2008','23-06-1988','25-08-1999','20-02-1993',]})\n\n# Transform string to date\ndata['date'] = pd.to_datetime(data.date, format=\"%d-%m-%Y\")\n\n# Extracting Year\ndata['month'] = data['date'].dt.year\n\n# Extracting Month\ndata['month'] = data['date'].dt.month\n\n# Extracting passed years since the date \ndata['passed_years'] = date.today().year - data['date'].dt.year\n\n# Extracting passed months since the date\ndata['passed_months'] = (date.today().year - data['date'].dt.year) * 12 + date.today().month - data['date'].dt.month\n\n# Extracting the weekday name of the date\ndata['day_name'] = data['date'].dt.day_name() \n\n=======\n\n# Drop columns that have too many missing value \nnull = train.isnull().sum()\ntrain1 = train\ntest1 = test\ntrain1.drop(columns=['aaa', 'bbb', 'ccc'], axis=1, inplace=True)\ntest1.drop(columns=['aaa', 'bbb', 'ccc'], axis=1, inplace=True)\n\n# y, X 설정\ny = train1['SalePrice']\nX = train1.drop('SalePrice', axis=1, inplace=True)\n\n# boolean field 설정\ntrain1['Fence'].unique()\nfence_map = {'MnPrv':1, 'GdWo':1, 'GdPrv':1, 'MnWw':1}\ntrain1['Fence'] = train1['Fence'].map(fence_map)\ntest1['Fence'] = test1['Fence'].map(fence_map)\n\n# null 처리\ntrain1['Fence'] = train1['Fence'].fillna(0)     \ntest1['Fence'] = test1['Fence'].fillna(0)\n\ntrain1['GarageCond'].value_counts()\ntrain1['GarageCond'] = train1['GarageCond'].fillna('TA')\ntest1['GarageCond'] = test1['GarageCond'].fillna('TA')\n\n# 숫자를 문자열로 변환\ntrain1['MoSold'] = train1['MoSold'].apply(str)\n\n# 컬럼의 값의 개수\ntest['MSZoning'].value_counts()\n\n# 컬럼 두 개를 하나로 합치기(Merge)\ntrain1['Condition'] = train1.apply(lambda x: x['Condition1'] if (pd.isnull(x['Condition2'])) else str(x['Condition1'] + '-' + str(x['Condition2]), axis=1)\ntrain1.drop(['Condition1', 'Condition2']), axis=1, inplace=True) \n     \n===== \n\ndef date_preprocessing(dataframe):\n    df = dataframe.copy()\n    df['일자'] = pd.to_datetime(df['일자'])\n    df['년도'] = df['일자'].dt.year\n    df['월'] = df['일자'].dt.month\n    df['일'] = df['일자'].dt.day\n    df['주'] = df['일자'].dt.weekday\n    return df   \n```","n":0.048}}},{"i":114,"$":{"0":{"v":"Memo","n":1},"1":{"v":"\n```python\nimport warnings\nwarnings.filterwarnings('ignore') \n```\n\n## pd.\n```python\npd.read_csv('...', index_col=0)\npd.isnull(df['col_name'])\npd.DataFrame({'Id': test1['Id'], 'SalePrice': preds_test})\nnew_df = pd.DataFrame(data=df, columns=df.columns) \npd.to_datetime(df['dt'])\npd.set_option(\"display.max_columns\", 101) \npd.to_numeric(varible_split.str.get(2), errors='coerce') \npd.concat([df1, df2], axis=1) \npd.date_range(start='05-01-2021', end='05-12-2021') \n```\n\n## DataFrame 전체\n```python\ndf.copy()\ndf.shape \ndf.info()\ndf.head()\ndf.describe(include='all').T\ndf.dtypes\ndf.dtypes.value_counts() \ndf.isnull().sum()\ndf.set_index('id') or df['id'] = df.index \ndf.corr() \ndf.filter(['col_1', 'col_2'], axis=1)  \ndf.drop(['col_1', 'col_2'], axis=1, inplace=True)\ndata.drop(data.loc[(data['SalePrice'] < 200000) & (data['GrLivArea'] > 4000)].index) \ndf.to_csv('submission.csv', index=False)\nnumeric_ = df.select_dtypes(exclude=['object']).drop(['MSSubClass'], axis=1).copy()\ncat_train = X.select_dtypes(include=['object']).copy()\ncat_train['MSSubClass'] = X['MSSubClass'] \ndf.isnull().sum().sort_values(ascending=False)\ndf.columns = ['col_1', 'col_2'] \ndf.select_dtypes('object').apply(pd.Series.nunique, axis=0) \ndf.sort_values(by=['id', 'age'], ascending=False) \ndf.reset_index \n```\n\n## 일부 컬럼\n```python\ndf['col_name'].value_counts()\ndf['col_name'].fillna(0) .fillna('TA')\ndf['col_name'].apply(str)\ndf['col'].map({'Ex': 3, 'Gd': 2, 'Fa': 1})\ndf['col'].replace(mapper, inplace=True) \n\ndf['col_sum'] = df['col_1'] + df['col_2'] \ndf['col'] = df.apply(lambda x: x['col1'] if pd.isnull(x['col2']) else x['col1'] + x['col2'], axis=1)\n\ndf['col'].astype(int).plot.hist()\n\ndf['col'].str.split()  \ndf.loc[:, 'col'] \n```\n\n## 일부 로우\n```python\ndf.loc[df.Sex=='female']\ndf.loc[df.Sex=='female']['Survived']\ndf.iloc[[0, 2], :]\ndf.loc[df.age >= 24]  \ndf[df.iloc[:, 1] != 0].sort_values('col', ascending=False).round(1) \n```\n\n## Iterate\n```python\nfor index, row in df.iterrows():\n\t...\n\nfor index in df.index:\n\tif np.isnan(df.loc[i, 'inflation_annual']):\n\t\t...  \n```\n\n## groupby\n```python\ndf.groupby('col')['salary'].mean()  \n```\n\n## unique categories\n```python\nfor col_name in df1.columns:\n    if df1[col_name].dtypes == 'object':\n        unique_cat = len(df1[col_name].unique())\n        print(f'{col_name}: {unique_cat}')\n```\n\n## missing values\n```python\nmis_val = app_train.isnull().sum()\nmis_val_percent = 100 * app_train.isnull().sum() / len(app_train)\nmis_values = pd.concat([mis_val, mis_val_percent], axis=1)\nmis_values.columns = ['Missing Values', '% of Total Values']\nmis_values = mis_values[mis_values.iloc[:, 1] != 0].sort_values('% of Total Values', ascending=False).round(1)\nmis_values.head(20) \n```\n\n## Number of unique classes in each object column \n```python\ndf.select_dtypes('object').apply(pd.Series.nunique, axis=0)  \n```","n":0.076}}},{"i":115,"$":{"0":{"v":"Datetime","n":1},"1":{"v":"\n## 날짜에서 년/월/일 추출\n- Accessor `dt` 사용\n\n```python\ndf['year'] = dt['date'].dt.year\ndf['month'] = dt['date'].dt.month\ndf['day'] = dt['date'].dt.day\n```\n\n## References\n- https://pandas.pydata.org/pandas-docs/stable/reference/series.html#api-series-dt","n":0.258}}},{"i":116,"$":{"0":{"v":"Pandas Dataframe","n":0.707},"1":{"v":"\n## Dataframe 생성\n\n### 2개의 numpy로 dataframe 생성\n\n```python\ndf = pd.DataFrame({'x': x, 'y': y})\n```\n\n## Indexing\n\n### 특정 컬럼\n```python\nseries <- df.loc[:, 'col_name']\ndataframe <- df[['col1_name', 'col2_name']] \n```\n\n## 데이터 제거\n\n### 컬럼 제거\n\n```python\ndf.drop('col_name', inplace=True, axis=1)\n```\n\n## Python 자료구조로 변환\n\n### 특정 컬럼을 리스트로\n```python\nvalue_list = df.loc[:, 'col_name'].values\n```\n\n\n## One-hot encoding\n\n```python\npd.get_dummies(df, columns=['col1_name', 'col2_name'])\n```","n":0.156}}},{"i":117,"$":{"0":{"v":"Pandas Cheat Sheet","n":0.577},"1":{"v":"\n## Cheat Sheat\n\n<iframe width=\"100%\" height=\"950px\" src=\"/assets/Pandas_Cheat_Sheet.pdf\">\n\n\n## References\n- https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf","n":0.354}}},{"i":118,"$":{"0":{"v":"OpenSearch","n":1},"1":{"v":"\n- `OpenSearch`는 커뮤니티 주도의 검색 및 분석 오픈소스이며, 엘라스틱서치 7.10.2 버전에서 분기됨\n- `OpenSearch`는 Apache Lucene 기반의 분선 검색 엔진이며, `OpenSearch Dashboards`는 데이터 시각화 및 유저인터페이스를 제공\n- 엘라스틱서치 오픈 배포판의 모든 기능을 포함\n\n## OpenSearch 용어\n\n클러스터/도메인|노드로 분산 구성된 시스템\n---|---\n노드|컴퓨팅 자원과 스토리지를 가진 개별 인스턴스를 의미. 리더노드와 데이터 노드가 있음.\n인덱스|도큐먼트를 저장하는 논리적 구분자로서 하나 이상의 샤드로 구성.\n샤드|여러 대 노드를 효율적으로 사용하기 위해 도큐먼트를 샤드라는 단위로 나눠 분산 저장. 성능과 처리량을 높이며, 프라이머리 샤드와 레플리카 샤드가 있음.\n도큐먼트|데이터가 저장되는 기본 단위. JSON 구조로 여러 필드와 값을 가짐.\n\n## 관계형 DB와 OpenSearch 비교\n\n관계형DB|OpenSearch\n---|---\n테이블|인덱스\n레코드|도큐먼트\n컬럼|필드\n스키마|매핑\n\n- 인덱스는 샤드들로 구성됨\n- 다수 노드에 샤드가 분산 저장되며 수평 확장 가능\n \n## References\n- AWS Builders Korea Program 200 : Amazon OpenSearch Service로 실시간 데이터 분석과 시각화 기본기 다지기","n":0.094}}},{"i":119,"$":{"0":{"v":"Machine Learning","n":0.707}}},{"i":120,"$":{"0":{"v":"XGBoost","n":1}}},{"i":121,"$":{"0":{"v":"Bias vs Variance","n":0.577},"1":{"v":"\n## What\n\n### 편향(Bias)\n- 모델의 예측값과 실제 값 사이의 평균적인 차이\n- 높은 편향은 모델이 너무 단순하다는 것을 의미하며, 데이터의 복잡성을 춘분히 잡아내지 못할 때 발생 = **언더피팅**\n\n### 분산(Variance)\n- 모델이 학습 데이터의 작은 변동에 얼마나 민감하게 반응하는지를 나타냄\n- 높은 분산은 모델이 학습 데이터에 과도하게 적합하게 되어 새로운 데이터나 테스트 데이터에 대한 성능이 떨어지게 됨 = **오버피팅**\n\n## How\n\n### 편향이 높고 분산이 낮은 경우\n- 모델이 너무 단순하여 데이터의 복잡성을 포착하지 못하게 됨\n- 더 복잡한 모델을 사용하거나 추가적인 특성(feature)을 포함시크는 것으로 개선\n\n### 편향이 낮고 분산이 높은 경우\n- 모델이 학습 데이터에 너무 잘 적합되어 있어, 새로운 데이터에 대한 성능이 떨어지게 됨\n- 정규화(regularization) 기법을 사용하거나, 데이터를 더 많이 모으는 것으로 개선","n":0.098}}},{"i":122,"$":{"0":{"v":"Machine Learning Flow","n":0.577},"1":{"v":"\n## Flow\n\n```mermaid\nflowchart LR\n    A[문제 정의] --> B[데이터 수집];\n    B --> C[데이터 분석];\n    C --> D[Feature Engineering];\n    D --> E[Modeling];\n    E --> F[Validation];\n    F --> G[Testing];\n```\n\n### Feature Engineering\n- 데이터를 분석한 내용과 도메인 지식을 바탕으로 feature vector(컴퓨터가 알 수 있는 숫자)로 만들어 주는 작업\n\n### Validation\n- Cross Validataion : 데이터셋을 나눠서 일부분은 테스트, 일부분은 학습용 데이터로 사용해서 모델의 정확도를 검증하는 방법","n":0.131}}},{"i":123,"$":{"0":{"v":"LLM","n":1}}},{"i":124,"$":{"0":{"v":"LLM Workflow Engine","n":0.577},"1":{"v":"\n터미널에서 ChatGPT API를 사용하여 대화형 챗봇을 구성할 수 있다.<br>https://llm-workflow-engine.readthedocs.io/en/latest/index.html","n":0.333}}},{"i":125,"$":{"0":{"v":"ETL vs ELT","n":0.577},"1":{"v":"\n## Summary\nETL은 데이터 변환의 복잡성을 처리하기 위해 중앙에서 데이터를 변환하는 반면, ELT는 데이터 웨어하우스의 스케일 및 처리 능력을 활용하여 변환을 수행\n\n## ETL (Extract, Transform, Load)\n\n### What\n- 데이터를 원본 소스에서 추출하고, 변환 프로세스를 거쳐 원하는 방식으로 만든 후, 목표 데이터 웨어하우스에 로드하는 과정\n\n### Why\n- 미리 변환된 데이터를 데이터 웨어하우스에 로드하기 때문에, 일단 데이터가 로드되면 쿼리 성능이 빠름\n- 원본 데이터의 품질이나 일관성에 문제가 있을 경우, 변환 단계에서 해당 문제를 해결할 수 있음\n- 전통적인 관계형 데이터베이스 시스템에 잘 맞음\n\n### When\n- 구조화된 데이터를 가진 전통적인 데이터 웨어하우스 환경에서의 데이터 처리와 통합\n\n## ELT (Extract, Load, Transform)\n\n### What\n- 데이터를 원본 소스에서 추출한 후, 먼저 목표 데이터 웨어하우스에 로드하고, 데이터 웨어하우스 내에서 필요한 변환 작업을 수행하는 과정\n\n### Why\n- 현대의 데이터 웨어하우스 솔루션은 대용량의 데이터 변환 작업을 빠르게 처리할 수 있으므로 ELT가 가능\n- 데이터 로딩 후 변환을 수행하기 때문에, 다양한 형식의 데이터를 빠르게 데이터 웨어하우스에 가져올 수 있음\n- 데이터 웨어하우스에서 직접 변환을 수행하기 때문에, 변환 로직을 쉽게 변경하거나 업데이트할 수 있음\n\n### When\n- 빅데이터 및 클라우드 기반 데이터 웨어하우스 환경 또는 데이터 웨어하우스 내에서 다양한 형식의 데이터를 동적으로 변환 및 집계해야 할 때","n":0.076}}},{"i":126,"$":{"0":{"v":"Deep Learning","n":0.707},"1":{"v":"\n\nk/deep-learning-with-python-notebooks","n":1}}},{"i":127,"$":{"0":{"v":"Autoencoder","n":1},"1":{"v":"\nhttps://github.com/rickiepark/handson-ml2/blob/master/17_autoencoders_and_gans.ipynb","n":1}}},{"i":128,"$":{"0":{"v":"Databricks","n":1},"1":{"v":"\n\n\n* https://community.databricks.com/s/\n\n\n","n":0.707}}},{"i":129,"$":{"0":{"v":"Workspace","n":1},"1":{"v":"\n## Notebook\n\n* Notebook에서 library 설치\n```sh\n%pip install /dbfs/path/to/file.whl\n```\n\n* Notebook에서 shell 스크립트 실행\n```sh\n%sh\n/dbfs/path/to/file.sh\n```\n\n* Notebook에서 Python 스크립트 실행\n```sh\n%run ./path/to/file.py\n```\n\n","n":0.25}}},{"i":130,"$":{"0":{"v":"Databricks SQL Connector","n":0.577},"1":{"v":"\n\nThe Databricks SQL Connector for Python is a Python library that allows you to use Python code to run SQL commands on Databricks clusters and Databricks SQL warehouses.\n\n## Install the library\n\n```cmd\npip install databricks-sql-connector\n```\n\n## Query data\n\n```python\nfrom databricks import sql\nimport os\n\nwith sql.connect(server_hostname = os.getenv(\"DATABRICKS_SERVER_HOSTNAME\"),\n                 http_path       = os.getenv(\"DATABRICKS_HTTP_PATH\"),\n                 access_token    = os.getenv(\"DATABRICKS_TOKEN\")) as connection:\n\n  with connection.cursor() as cursor:\n    cursor.execute(\"SELECT * FROM default.diamonds LIMIT 2\")\n    result = cursor.fetchall()\n\n    for row in result:\n      print(row)\n```\n\n## References\n- https://docs.databricks.com/dev-tools/python-sql-connector.html#get-started","n":0.12}}},{"i":131,"$":{"0":{"v":"Pyodbc","n":1},"1":{"v":"\n### #1. msodbcsql17과 pyodbc를 설치하여 사용\n\n```python\n%sh\ncurl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/16.04/prod.list > /etc/apt/sources.list.d/mssql-release.list\nsudo apt-get update\nsudo ACCEPT_EULA=Y apt-get -q -y install msodbcsql17\nsudo apt-get install msodbcsql17\n```\n\n```python\n%pip install pyodbc\n```\n\n\n```python\nimport pyodbc\n\ntry:\n    conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n    cursor = cnxn.cursor()\n    cursor.execute(\"…\")\nexcept Exception as e:\n    print(e)\n```\n\n### #2. Databricks에서 제공하는 connector 사용 \n\n* TBD","n":0.139}}},{"i":132,"$":{"0":{"v":"Delta Table","n":0.707},"1":{"v":"\n## What\n\nDelta table은 parquet 파일에 기능을 더한 Delta 파일에 또 기능을 추가한 것\n\n### Delta Tables\n    - Delta files\n    - Registered in a metastore\n    - Delta tractional logs\n\n### Delta files\n    - Data versioning\n    - Transaction logs\n    - ACID transactions\n\n### Parquet files\n    - Power to store tablular data\n    - Fast\n    - Columnar storage\n\n\n## SQL examples\n\n```sql\nCREATE OR REPLACE TABLE table_name (\n    id STRING,\n    birthDate DATE,\n    avgRating FLOAT\n)\n\nCREATE TABLE IF NOT EXISTS table_name (\n    id STRING,\n    birthDate DATE,\n    avgRating FLOAT\n)\n\nSELECT DISTINCT * FROM my_table;\n```","n":0.112}}},{"i":133,"$":{"0":{"v":"Delta Live Tables","n":0.577},"1":{"v":"\n## What\n- Automated data pipelines for Delta Lake <-- [[dev.data+ai.databricks.delta_table]] + [[dev.data+ai.spark.structured_streaming]]\n- 데이터 처리 파이프라인을 빌드하기 위한 프레임워크\n- 사용자는 데이터에 대해 수행할 변환을 정의하고, DLT는 작업 오케스트레이션, 클러스터 관리, 모니터링, 데이터 품질 및 오류 처리를 관리\n- 여러 Spark 작업을 사용하여 데이터 파이프라인을 정의하는 대신 DLT는 각 처리 단계에 대해 사용자가 정의하는 대상 스키마를 기반으로 데이터를 변화하는 방법을 관리\n- DLT는 [[dev.data+ai.databricks.delta_live_tables.expectations]]를 사용하여 데이터 품질을 적용할 수도 있음\n  - 예상 데이터 품질을 정의하고 이러한 기대에 실패한 레코드를 처리하는 방법을 지정할 수 있음 ([[dev.data+ai.data_ingestion.constraint]])\n\n## How\n\n### Data Ingestion\n  - Cloud storage에 있는 data: `Auto Loader`를 사용하여 데이터가 업로드될 때마다 Delta Live Table이 ingest\n  ```python\nspark.readStream.format('cloudFiles') # enables the use of Auto Loader\n     .option(\"cloudFiles.format\", \"csv\")\n     .load(\"path/to/customers\")\n  ```\n\n  - `Append-only` 속성을 가지는 다른 delta table: `SQL STREAM()` function을 사용하여 ingest\n\n### Delta live table 생성\n```sql\nCREATE LIVE TABLE customers\nAS SELECT * FROM cloud_files(...)\n```\n \n\n### Streaming data processing\n\n#### Python\n```python\n@dlt.table\ndef streaming_bronze():\n  return (\n    # Since this is a streaming source, this table is incremental.\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .load(\"abfss://path/to/raw/data\")\n  )\n\n@dlt.table\ndef streaming_silver():\n  # Since we read the bronze table as a stream, this silver table is also\n  # updated incrementally.\n  return dlt.read_stream(\"streaming_bronze\").where(...)\n\n@dlt.table\ndef live_gold():\n  # This table will be recomputed completely by reading the whole silver table\n  # when it is updated.\n  return dlt.read(\"streaming_silver\").groupBy(\"user_id\").count()\n```\n\n#### SQL\n```sql\nCREATE OR REFRESH STREAMING LIVE TABLE streaming_bronze\nAS SELECT * FROM cloud_files(\n  \"abfss://path/to/raw/data\", \"json\"\n)\n\nCREATE OR REFRESH STREAMING LIVE TABLE streaming_silver\nAS SELECT * FROM STREAM(LIVE.streaming_bronze) WHERE...\n\nCREATE OR REFRESH LIVE TABLE live_gold\nAS SELECT count(*) FROM LIVE.streaming_silver GROUP BY user_id\n```\n\n### Configure a Structured Streaming job \n\n#### From a Bronze table to a Silver table\n```python\n(spark.table(\"sales\")\n    .withColumn(\"avg_price\", col(\"sales\") / col(\"units\")))\n    .writeStream\n    .option(\"checkpointLocation\", checkpointPath)\n    .outputMode(\"append)\n    .table(\"cleanedSales\")\n```\n\n#### To Execute a single micro-batch to process all of the available data\n```Python\n(spark.table(\"sales\")\n    .withColumn(\"avg_price\", col(\"sales\") / col(\"units\")))\n    .writeStream\n    .option(\"checkpointLocation\", checkpointPath)\n    .output(\"complete\")\n    .trigger(once=True)\n    .table(\"new_sales\")\n```\n\n## Medalion Structures\n\n![](https://www.databricks.com/wp-content/uploads/2022/03/delta-lake-medallion-architecture-2.jpeg)\n- Data source > Bronze\n    - A job that ingests raw data from a streaming source into the Lakehouse\n- Bronze > Silver\n    - A job that enriches data by parsing its timestamps into a human-readable format\n- Silver > Gold\n    - A job that queries aggregated data to publish key insights into a dashboard\n    - A job that develops a feature set for a machine learning application\n    - A job that aggregates cleaned data to create standard summary statistics\n\n- https://www.databricks.com/glossary/medallion-architecture","n":0.053}}},{"i":134,"$":{"0":{"v":"Expectations","n":1},"1":{"v":"\n## What\n[[dev.data+ai.databricks.delta_live_tables]]에서는 -데이터셋의 컨텐츠에 대해- 데이터 품질 제약조건([[dev.data+ai.data_ingestion.constraint]])을 지정하여 데이터 유효성(expectation)을 확보할 수 있음\n\n## How\n**Expectation** | **기대 동작**\n----------------|------------\nexpect_all | 유효성 검사에 실패한 레코드가 데이터 세트에 포함\nexpect_all_or_drop | 유효성 검사에 실패한 레코드를 데이터 셋트에서 삭제\nexpect_all_or_fail | 유효성 검사에 실패한 레코드가 있으면 파이프라인 실행을 중지\n\n### Python\n\n```python\n@dlt.expect(\"valid timestamp\", \"col(“timestamp”) > '2012-01-01'\")\n@dlt.expect_or_drop(\"valid_current_page\", \"current_page_id IS NOT NULL AND current_page_title IS NOT NULL\")\n@dlt.expect_or_fail(\"valid_count\", \"count > 0\")\n```\n\n### SQL\n```SQL\nCONSTRAINT valid_timestamp EXPECT (timestamp > '2012-01-01')\nCONSTRAINT valid_current_page EXPECT (current_page_id IS NOT NULL and current_page_title IS NOT NULL) ON VIOLATION DROP ROW\nCONSTRAINT valid_count EXPECT (count > 0) ON VIOLATION FAIL UPDATE\n```\n\n## Reference\n- https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-expectations.html","n":0.104}}},{"i":135,"$":{"0":{"v":"Change Data Capture with Delta Live Tables","n":0.378},"1":{"v":"\n## What\n- 원본 데이터가 변경되면 Delta Live Table pipeline이 실행되어 table이 자동으로 업데이트되게 하는 것\n\n## How\n- `APPLY CHANGES INTO` query 또는 `apply_changes` function 실행\n\n## Reference\n- https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-cdc.html","n":0.196}}},{"i":136,"$":{"0":{"v":"Delta Lake","n":0.707},"1":{"v":"\n## What\n- an open source ACID table storage layer over cloud object stores\n- parquet 데이터 파일을 확장<br>\n    ㄴ How? transaction log를 추가하여 <br>\n    ㄴ So? [[dev.cs.database.ACID]]을 제공하고, 확장 가능한 메타데이터를 처리\n- Apache Spark API 와 완벽하게 호환되며 [[dev.data+ai.spark.structured_streaming]]과의 긴밀하게 통합됨\n<br>\n    ㄴ So? 일괄 처리 및 스트리밍 작업 모두에 단일 데이터 복사본을 쉽게 사용하고, 대규모 증분 처리를 제공할 수 있음\n- Databricks의 [[dev.data+ai.databricks.data_lakehouse]] 플랫폼에서는 데이터 및 테이블을 저장하기 위한 기반을 제공하는 최적화된 `스토리지 계층`으로 사용됨\n\n> Delta Lake is an open source ACID table storage layer over cloud object stores initially\ndeveloped at Databricks. Delta Lake uses a transaction log that is\ncompacted into Apache Parquet format to provide ACID properties,\ntime travel, and significantly faster metadata operations for large\ntabular datasets (e.g., the ability to quickly search billions of table\npartitions for those relevant to a query). It also leverages this design to provide high-level features such as automatic data layout\noptimization, upserts, caching, and audit logs.\n\n## WHY\n- [[dev.data+ai.data_lake]]는 Schema Enforcement, Data Quality, ACID Transaction 등을 지원하지 않음\n- 이러한 Data Lakes의 결점을 Delta Lake로 극복할 수 있음\n\n## Z-ordering\n- Z-ordering is a technique to colocate related information in the same set of files. This co-locality is automatically used by Delta Lake on Databricks data-skipping algorithms. This behavior dramatically reduces the amount of data that Delta Lake on Databricks needs to read. \n\n```sql\nOPTIMIZE events\nWHERE date >= current_timestamp() - INTERVAL 1 day\nZORDER BY (eventType)\n```\n\n## Reference\n- https://learn.microsoft.com/en-us/azure/databricks/delta/\n- https://www.databricks.com/wp-content/uploads/2020/08/p975-armbrust.pdf\n- https://www.linkedin.com/pulse/delta-lake-tables-umair-yelurkar","n":0.066}}},{"i":137,"$":{"0":{"v":"Databricks Connect","n":0.707},"1":{"v":"\n\n- Databricks Connect의 구문 분석 및 계획 작업은 로컬 컴퓨터에서 실행되는 반면, 작업은 원격 컴퓨팅 리소스에서 실행됨 --> 런타임 오류를 디버그하기 힘듦\n\n\n- 로컬 컴퓨터의 파일을 (원격 클러스터에서) 처리하고 싶은 경우\n    1. Pandas로 로컬 컴퓨터의 파일을 읽는다.\n    2. Pandas dataframe을 Spark dataframe으로 변환한다.\n    3. ...\n    4. 결과를 로컬 컴퓨터에 저장하려면 Spark dataframe을 Pandas dataframe으로 변환 후 저장한다.\n```python\nimport pandas as pd\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\ndf = spark.createDataFrame(pd.read_csv('./data/sample.csv'))\n\ndf.toPandas().to_parquet('./data/result.parquet')\n```","n":0.123}}},{"i":138,"$":{"0":{"v":"Databricks SQL","n":0.707},"1":{"v":"\n## Databricks SQL\n\n- 데이터 레이크에서 빠른 임시 SQL을 실행할 수 있음\n- 쿼리 결과에 대한 다양한 시각화를 지원\n\n## SQL 엔드포인트\n\n- Databricks SQL 내의 데이터 개체에서 SQL 명령을 실행할 수 있는 계산 리소스\n    - Azure Databricks 컴퓨팅 리소스의 한 유형\n\n## Overwriting a table vs Deleting and Recreating a table\n- Overwriting a table is eficient because no files need to be deleted.\n- Overwriting a table maintains the old version of the table for Time Travel.\n- Overwriting a table is an atomic operation and will not leave the table in an unfinished state.\n- Overwriting a table allows for concurrent queries to be completed while in progress.\n\n## SQL Reference\n\n### 데이터베이스 생성\n```sql\nCREATE DATABASE IF NOT EXISTS customer360 LOCATION '/customer/customer360';\n```\n\n### 테이블 생성\n\n```sql\nCREATE TABLE my_table (id STRING, value STRING);\n\nCREATE OR REPLACE TABLE table_name (\n    id STRING,\n    birthDate DATE,\n    avgRating FLOAT\n)\n```\n\n`CREATE OR REPLACE TABLE`\n- 테이블이 없으면 새로 생성\n- 테이블이 있으면 테이블을 리셋\n> Delta Lake Table을 dropping하고 re-creating하는 것보다 REPLACE 명령을 사용할 것\n\n`CREATE TABLE IF NOT EXISTS`\n- 테이블이 있으면 테이블이 생성되지 않고 명령이 무시됨\n\n\n```sql\nCREATE TABLE customersPerCountry AS\nSELECT country,\n       COUNT(*) AS customers\nFROM customerLocations\nGROUP BY country;\n```\n\n```sql\n-- Createa a CSV table from an external directory\nCREATE TABLE student USING CSV LOCATION '/mnt/csv_files';\n```\n\n### 사용자에게 권한 부여\n```sql\nGRANT ALL PRIVILEAGES ON TABLE sales TO new.engineer@company.com;\nGRANT SELECT ON TABLE sales TO new.engineer@company.com;\n```\n\n### 중복 제거\n```sql\nSELECT DISTINCT * FROM my_table;\n```\n\n### Upsert\nTo insert or update. If the item does not exist, add or insert it; if the item already exists, then update it with new information.\n\n- `MERGE SQL` operation : upsert data from a source table, view, or DataFrame into a target Delta table (supported only for Delta Lake tables)\n    - 만약 id가 있으면 -> to update rows\n    - id가 없으면 -> to insert new rows\n\n```sql\nMERGE INTO people10m\nUSING people10mupdates\nON people10m.id = People10mupdates.id\nWHEN MATCHED THEN\n  UPDATE SET\n    id = people10mupdates.id,\n    firstName = people10mupdates.firstName,\n    ssn = people10mupdates.ssn\nWHEN NOT MATCHED\n  THEN INSERT (\n    id,\n    firstName,\n    ssn\n  )\n  VALUES (\n    people10mupdates.id,\n    people10mupdates.firstName,\n    people10mupdates.ssn\n  )\n```\n\n### Window functions\n\n```sql\nSELECT\n    name\n  , dept\n  , salary\n  , RANK() OVER (PARTITION BY dept ORDER BY salary) AS rank\n  , DENSE_RANK() OVER (PARTITION BY dept ORDER BY salary) AS dense_rank\n  , DENSE_RANK() OVER (PARTITION BY dept ORDER BY salary\n                       ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank2\n  , MIN(salary) OVER (PARTITION BY dept ORDER BY salary) AS min\n  , LAG(salary) OVER (PARTITION BY dept ORDER BY salary) AS lag\n  , LEAD(salary, 1, 0) OVER (PARTITION BY dept ORDER BY salary) AS lead\nFROM employees\n;\n```\n![](/assets/images/sql_result_window_functions.png)\n\n### TBD\n```sql\n-- unnest\n-- card_id STRING, items ARRAY<item_id:STRING> --> card_id STRING, item_id STRING\nSELECT cart_id, explode(items) AS item_id FROM raw_table;\n\n-- extract\n-- transaction_id STRING, payload ARRAY<customer_id:STRING, date:TIMESTAMP, store_id:STRING>\n-- --> transacction_id: STRING, date TIMESTAMP\nSELECT transaction_id, payload.date FROM raw_table;\n```\n\n## Reference\n- [Upsert into a Delta Lake table using merge](https://docs.databricks.com/delta/merge.html)\n- [CREATE TABLE [USING]](https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html)\n- [MERGE INTO](https://docs.databricks.com/sql/language-manual/delta-merge-into.html)\n- [Window functions](https://docs.databricks.com/sql/language-manual/sql-ref-window-functions.html)","n":0.048}}},{"i":139,"$":{"0":{"v":"Data Lakehouse","n":0.707},"1":{"v":"\n## What\n\n- In short, a Data Lakehouse is an architecture that enables efficient and secure Artificial Intelligence (AI) and Business Intelligence (BI) directly on vast amounts of data stored in Data Lakes.\n- The Data Lakehouse enables storing all your data once in a [[dev.data+ai.data_lake]] and doing AI and BI on that data directly. \n\n> We define a [Lakehouse](https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf?utm_source=microsoft&utm_medium=web&utm_campaign=7013f000000Tx4QAAS) as a data management system based on lowcost and directly-accessible storage that also provides traditional analytical DBMS management and performance features such as ACID transactions, data versioning, auditing, indexing, caching, and query optimization. Lakehouses thus combine the key benefits of data lakes and data warehouses: low-cost storage in an open format accessible by a variety of systems from the former, and powerful management and optimization features from the latter. - Michael Armbrust, Ali Ghodsi, Reynold Xin, Matei Zaharia\n\n![](https://www.databricks.com/wp-content/uploads/2020/01/data-lakehouse-new.png)\n\n- Lakehouses는 데이터 레이크의 저비용 및 유연성과 데이터 웨어하우스의 안정성 및 성능을 결합\n- Lakehouse 아키텍처는 다음과 같은 몇 가지 주요 기능을 제공\n\n    - 개방형 형식의 안정적이고 확장 가능하며 저렴한 스토리지\n    - [[dev.cs.database.ACID]]을 사용한 ETL 및 스트림 처리\n    - 쿼리 시 관리 용이성과 성능을 보장하기 위한 메타데이터, 버전 관리, 캐싱 및 인덱싱\n    - 데이터 과학 및 기계 학습을 위한 선언적 DataFrame API와 함께 BI 및 보고용 SQL API\n\n    ![](https://techcommunity.microsoft.com/t5/image/serverpage/image-id/243936iECABC2CB42BE68E3/image-size/medium?v=v2&px=400)\n\n\n- Lakehouse instead adds traditional data warehousing capabilities to existing data lakes, including ACID transactions, fine-grained data security, low-cost updates and deletes, first-class SQL support, optimized performance for SQL queries, and BI style reporting. By building on top of a data lake, the Lakehouse stores and manages all existing data in a data lake, including all varieties of data, such as text, audio and video, in addition to structured data in tables. Lakehouse also natively supports data science and machine learning use cases by providing direct access to data using open APIs and supporting various ML and Python/R libraries, such as PyTorch, Tensorflow or [[dev.data+ai.ml.xgboost]], unlike data warehouses. Thus, Lakehouse provides a single system to manage all of an enterprise’s data while supporting the range of analytics from BI and AI.\n\n### Lakehouse의 3가지 핵심 원칙과 구성 요소\n![](https://techcommunity.microsoft.com/t5/image/serverpage/image-id/244539i02EE86FA40EC52EF/image-dimensions/671x329?v=v2)\n1. 오픈 소스 형식의 선별된 레이어와 함께 모든 데이터를 저장하는 [[dev.data+ai.data_lake]]\n    - 데이터 레이크는 모든 유형, 크기 및 속도의 데이터를 수용할 수 있어야 합니다. 레이크에서 선별된 데이터의 형식은 개방형이어야 하고 클라우드 네이티브 보안 서비스와 통합되어야 하며 ACID 트랜잭션을 지원해야 합니다.\n2. 개방형 표준을 기반으로 구축된 기본 컴퓨팅 계층\n    - 데이터 레이크 큐레이팅(ETL 및 스트림 처리), 데이터 과학 및 기계 학습, 데이터 레이크에 대한 SQL 분석을 포함한 모든 핵심 레이크하우스 사용 사례를 지원하는 기본 컴퓨팅 계층이 있어야 합니다.\n3. 추가 및/또는 새로운 사용 사례를 위한 손쉬운 통합\n    - 단일 서비스가 모든 것을 할 수는 없습니다. 핵심 Lakehouse 사용 사례의 일부가 아닌 새로운 또는 추가 사용 사례가 항상 있을 것입니다. 이러한 신규 또는 추가 사용 사례에는 전문 서비스나 도구가 필요한 경우가 많습니다. 이것이 바로 선별된 데이터 레이크, 기본 컴퓨팅 계층, 기타 서비스 및 도구 간의 손쉬운 통합이 핵심 요구 사항인 이유입니다.\n\n\n## Data Lakehouse의 장점\n- A data lakehouse enables both batch and streaming analytrics.\n- A data lakehouse stores unstructured data and is ACID-compliant.\n\n\n## Databricks Lakehouse의 구성 요소\n\n### [[dev.data+ai.databricks.delta_table]]\n\n### Unity Catalog\n\n## Incremental Multi-Hop in the Lakehouse\n\n![](/assets/images/databricks-multi-hop.png)\n\n파이프라인의 각 Bronze, Silver, Gold 데이터 단계별로 프로세싱되는 과정을 거치면서 가장 최신의 데이터를 준실시간성(near-real time)으로 처리하여 분석가에게 제공됨\n\n- Bronze 테이블은 다양한 소스(JSON files, RDBMS data, IoT data등)에서 수집한 원본 데이터를 저장\n- Silver 테이블은 우리 데이터에 좀더 정제된 view를 제공. 다양한 bronze 테이블과 조인하거나 불필요한 정보의 제거, 업데이트 등을 수행\n- Gold 테이블은 주로 리포트나 대시보드에서 사용되는 비지니스 수준의 aggregation을 수행한 뷰를 제공 (예: 일간 사용자수나 상품별 매출 등의 뷰)\n\n## Lakehouse Architecture 도입 사례\n- [[dev.cloud.aws.lakehouse]]\n\n## Reference\n- https://www.databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html\n- https://www.databricks.com/blog/2021/08/30/frequently-asked-questions-about-the-data-lakehouse.html\n- https://techcommunity.microsoft.com/t5/analytics-on-azure-blog/simplify-your-lakehouse-architecture-with-azure-databricks-delta/ba-p/2027272","n":0.041}}},{"i":140,"$":{"0":{"v":"Data+AI Summit 2022 키노트 주요 어나운스먼트 요약","n":0.378},"1":{"v":"\n## 1. Delta Lake 2.0\n\n- Storage layer 역할\n- [[dev.data+ai.databricks.delta_lake]] 오픈소스화\n- github에서 2.0.0 확인 가능\n- 데이터브릭스 플랫폼에서만 제공하던 기능을 오픈\n- 성능과 관리 편의성을 획기적으로 개선\n\n## 2. Spark\n- 이론적인 실제적인 공로를 인정받음\n- Spark Connect\n- Spark를 모든 곳에서 쓸 수 있게\n- OOM?\n- 씬 클라이언트에서도 Spark 사용 가능\n \n## 3. Project Lightspeed\n- 차세대 Spark [[dev.data+ai.spark.structured_streaming]] \n- 더 빠르고 심플한 스트림 프로세싱\n \n## 4. [[dev.data+ai.databricks.delta_live_tables]]\n- ETL Framework 서비스\n- Enzyme 옵티마이저 -> 증분 ETL 작업\n \n## 5. Databricks SQL Serverless\n \n## 6. Unity Catalog\n- 모든 데이터 및 AI asset을 통합 관리하기 위한 기능","n":0.107}}},{"i":141,"$":{"0":{"v":"Cluster","n":1},"1":{"v":"\n## Create a cluster\n\nThere are two types of clusters:\n- All-Purpose clusters can be shared by multiple users. There are typically used to run notebooks. All-Purpose clusters remain active until you terminate them.\n- Job clusters run a job. You create a job cluster when you create a job. Such clusters are terminated automatically after the job is completed.\n    - ex) An automated workflow needs to be run every 30 minutes.\n\n## Cluster 상태\n\n### Cluster의 status가 pending\n- cluster 생성 \n- package 생성\n- 에러 발생\n    - Spark job의 input parameter 오류\n\n### 라이브러리 설치 시 \n\nlibrary를 uninstall하면\n- cluster를 한 번 껐다 켜야 함\n- 노트북에서 magic command로 uninstall\n    - `%pip uninstall ...`\n","n":0.097}}},{"i":142,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n## DBFS\n\n```\ndatabricks fs cp a.file dbfs:/path/to/a.file\ndbfs cp a.file dbfs:/FileStore/a.file\n```\n\n## Workspace\n```\ndatabricks workspace ls /Users/someone@example.com/example -l\ndababricks workspace import a.file /Users/someone@example.com/path/to/a.file -l PYTHON -f JUPYTER\n```\n\n\n## Reference\n- https://docs.databricks.com/dev-tools/cli/workspace-cli.html","n":0.204}}},{"i":143,"$":{"0":{"v":"Architecture overview","n":0.707},"1":{"v":"\n\n\n## High-level architecture\n\n![](https://docs.databricks.com/_images/databricks-architecture.png)\n\n## Control plane and data plane\n\n- The control plane includes the backend services that Databricks manages in its own AWS account. Notebook commands and many other workspace configurations are stored in the control plane and encrypted at rest.\n\n- The data plane is where your data is processed.\n    - For most Databricks computation, the compute resources are in your AWS account in what is called the Classic data plane. This is the type of data plane Databricks uses for notebooks, jobs, and for Classic Databricks SQL warehouses.\n\n\n## Reference\n- https://docs.databricks.com/getting-started/overview.html","n":0.105}}},{"i":144,"$":{"0":{"v":"Data Warehouse","n":0.707},"1":{"v":"\n## 데이터 웨어하우스\n> Data warehouses are proprietary systems that are built to store and manage only structured or semi-structured (primarily JSON format) data for SQL-based analytics and business intelligence. \n\n- 조직 내 서로 다른 다양한 소스의 정보를 저장하고 분석할 수 있게 하는 시스템\n- 대규모 데이터 저장소에 걸쳐 발생하는 복잡한 쿼리에 적합<br>\n    - 예) 데이터로부터 숨겨인 인사이트를 발굴하기 위해 여러 데이터베이스에서 정보를 마이닝하는 것\n    - ⭐️ 소규모의 다량의 원자성 트랜잭션에는 적합하지 않음\n- 여러 다양한 데이터 저장소를 순환하는 복잡한 쿼리를 사용하므로 많은 리소스를 필요<br>\n    - 확장성이 엔터프라이즈급 데이터베이스에는 미치지 못함\n\n## 데이터베이스와의 비교\n|     | **데이터베이스**    |\t**데이터 웨어하우스**\n  -- |-------------|----------------\n주 사용 목적\t| 데이터 기록\t|데이터 분석\n처리 방법\t|OLTP\t|OLAP\n동시 사용자 수\t|한 번에 수천 명의 사용자를 처리\t|상대적으로 작은 규모의 사용자만 처리\n사용 사례\t|소규모의 원자성 트랜잭션에 유용\t|높은 수준의 복잡한 분석\n다운타임\t|항시 사용 가능해야 함\t|일부 예정된 다운타임 허용\n최적화\t|CRUD 작업 기준\t|대규모 데이터 저장소에 걸쳐 발생하는 소규모의 복잡한 쿼리 기준\n데이터 유형\t|실시간 상세 데이터\t|요약형 기록 데이터\n\n### OLTP와 OLAP 비교\n[[dev.cs.database.oltp-vs-olap]]\n\n## 데이터 마트\n- 특정 정보 유형이나 마케팅, 영업, 재무 또는 인사 등 조직 내 특정 사용자 집합을 위한 정보를 저장하는 것을 목적으로 하는 데이터베이스\n- 데이터 마트는 자체 엔터티가 될 수도 있고, 대규모 데이터 웨어하우스에 속한 소규모 파티션이 될 수도 있음\n\n## Summary\n- 신속한 데이터 이용을 위해 OLTP 솔루션이 필요하면 데이터베이스를 사용\n- 현재 데이터뿐만 아니라 과거 정보도 집계할 수 있는 OLAP 솔루션이 필요하면 데이터 웨어하우스 시스템 도입\n\n## Reference\n- https://www.databricks.com/blog/2021/08/30/frequently-asked-questions-about-the-data-lakehouse.html","n":0.069}}},{"i":145,"$":{"0":{"v":"Data Mining","n":0.707},"1":{"v":"\n# What\n- Analyzing historical data to find patterns","n":0.354}}},{"i":146,"$":{"0":{"v":"Data Lake","n":0.707},"1":{"v":"\n## What\n\n> “A data lake is a storage repository that holds a vast amount of raw data in its native format, including structured, semi-structured, and unstructured data.\"\n\n## 특징\n\n- 중앙 집중식 데이터 스토리지의 한 방법으로서 구조화된 데이터와 구조화되지 않은 데이터가 함께 저장될 수 있음\n- 현재 및 과거 정보 모두에 대해 일종의 `하치장` 같은 역할\n- 구조화된 데이터베이스보다 일반적으로 유연성과 적응성이 높음\n- 추후에 개발자와 분석가가 이러한 대량의 정보를 처리하고 사용하고자 할 때 그에 따른 불편을 감수해야 함","n":0.116}}},{"i":147,"$":{"0":{"v":"Data ingestion","n":0.707}}},{"i":148,"$":{"0":{"v":"Constraint","n":1},"1":{"v":"\n## 유효 기대값을 벗어나는 데이터 인입 처리\n\n> How to handle records that violates expectations?\n- <b>Track</b> number of bad records\n- <b>Drop</b> bad records\n- <b>Abort</b> processing for a single bad record\n\n\n- Databricks의 `Delta Live Tables`에서는 아래와 같이 [[dev.data+ai.databricks.delta_live_tables.expectations]] clause를 정의하면, 기대값을 벗어나는 데이터는 target dataset에 추가됨과 동시에 event log에 invalid로 기록이 됨\n  ```sql\nCONSTRAINT valid_timestamp EXPECT (timestamp > '2022-01-01')\n  ```","n":0.132}}},{"i":149,"$":{"0":{"v":"Colab","n":1},"1":{"v":"\n구글 코랩 소개\n- https://post.naver.com/viewer/postView.nhn?volumeNo=26447765&vType=VERTICAL","n":0.5}}},{"i":150,"$":{"0":{"v":"ML-DL Book Landscape","n":0.577},"1":{"v":"\n\n![](https://tensorflowkorea.files.wordpress.com/2021/10/book_roadmap.jpg)\n\nhttps://tensorflow.blog/book-roadmap/","n":1}}},{"i":151,"$":{"0":{"v":"Computer Science","n":0.707}}},{"i":152,"$":{"0":{"v":"Database","n":1}}},{"i":153,"$":{"0":{"v":"Vacuum","n":1},"1":{"v":"\n## What\n- 데이터베이스의 쓰레기 데이터를 정리하여 쾌적하게 청소하라는 명령\n- `디스크 조각 모음`이라고 생각하면 됨\n\n\n## Reference\n- https://blog.gaerae.com/2015/09/postgresql-vacuum-fsm.html","n":0.25}}},{"i":154,"$":{"0":{"v":"구체화된 뷰(Structured View)","n":0.577},"1":{"v":"\n## 구체화된 뷰란 무엇인가요?\n\n구체화된 뷰는 더 빠른 데이터 검색을 위해 여러 기존 테이블의 데이터를 결합하여 생성되는 중복 데이터 테이블\n- 예를 들어, 고객 테이블과 제품 테이블이 있는 애플리케이션에서 특정 고객이 구매한 제품의 상세 정보를 얻으려면 두 테이블을 상호 참조해야 함\n- 이렇게 만드는 대신에 고객 이름 및 관련 제품 세부 정보를 단일 임시 테이블에 저장하는 구체화된 뷰를 생성할 수 있음\n- 구체화된 뷰에 인덱스 구조를 구축하여 데이터 읽기 성능을 향상시킬 수 있음\n\n## 구체화된 뷰의 이점은 무엇인가요?\n구체화된 뷰는 관련 데이터에 빠르고 효율적으로 액세스할 수 있는 방법으로, 데이터 집약적인 애플리케이션의 쿼리 최적화를 지원\n\n### 속도\n읽기 쿼리는 다양한 테이블과 데이터 행을 스캔하여 필요한 정보를 수집함. 구체화된 뷰를 사용하면 매번 새 정보를 계산할 필요 없이 새 뷰에서 직접 데이터를 쿼리할 수 있음. 쿼리가 복잡할수록 구체화된 뷰를 사용하면 더 많은 시간을 절약할 수 있음.\n\n### 데이터 스토리지 단순성\n구체화된 뷰를 사용하면 복잡한 쿼리 로직을 단일 테이블에 통합할 수 있음. 이를 통해 개발자는 데이터 변환 및 코드 유지관리가 더 쉬워짐. 또한 복잡한 쿼리를 더 쉽게 관리할 수 있음. 데이터 서브셋을 사용하여 뷰에서 복제애야 하는 데이터의 양을 줄일 수도 있음. \n\n### 일관성\n구체화된 뷰는 특정 순간에 캡처된 데이터를 일관되게 보여줌. 구체화된 뷰에서 읽기 일관성을 구성하고 동시성 제어가 필수적인 다중 사용자 환경에서도 데이터에 액세스할 수 있도록 할 수 있음.\n\n또한 구체화된 뷰는 소스 데이터가 변경되거나 삭제되는 경우에도 데이터 액세스를 제공함. 시간이 지남에 따라 구체화된 뷰를 사용하여 시간 기반 데이터 스냅샷에 대해 보고할 수 있음. 원본 테이블과의 격리 수준을 통해 데이터 전반에서 일관성을 높일 수 있음.\n\n### 향상된 액세스 제어\n구체화된 뷰를 사용하여 특정 데이터에 액세스할 수 있는 사용자를 제아할 수 있음. 원본 테이블에 대한 액세스 권한을 부여하지 않고도 사용자에 대한 정보를 필터링할 수 있음. 이 접근 방식은 누가 어떤 데이터에 액세스할 수 있고 얼마나 많은 데이터를 상호 작용할 수 있는지를 제어하려는 경우에 유용함.\n\n## 구체화된 뷰의 사용 사례로는 무엇이 있나요?\n\n### 필터링된 데이터 배포\n최근 데이터를 여러 위치에 배포하는 경우\n- 구체화된 뷰를 사용하여 여러 사이트에 데이터를 복제하고 배포\n- 데이터에 액세스해야 하는 사람들은 지리적으로 가장 가까운 복제 데이터 저장소와 상호 작용함\n- 이 시스템은 동시성을 허용하고 네트워크 부하를 줄임\n- 읽기 전용 데이터베이스를 사용하는 효과적인 접근 방식\n\n### 시계열 데이터 분석\n시간 경과에 따른 데이터를 분석하는 경우\n- 구체화된 뷰는 타임스탬프가 지정된 데이터 세트 스냅샷을 제공하기 때문에 시간 경과에 따른 정보 변화를 모델링할 수 있음\n- 월별 또는 주별 요약과 같이 미리 계산된 데이터 집계를 저장할 수 있음\n- 이러한 용도는 비즈니스 인텔리전스 및 보고 플랫폼에 유용\n\n### 원격 데이터 상호 작용\n원격 서버의 데이터를 사용하는 경우\n- 분산 데이터베이스 시스템에서는 구체화된 뷰를 사용하여 원격 서버의 데이터와 관련된 쿼리를 최적화할 수 있음\n- 원격 소스에서 반복적으로 데이터를 가져오는 대신 로컬 구체화된 뷰에서 데이터를 가져와 저장할 수 있음\n- 따라서 네트워크 통신의 필요성이 줄어들고 성능이 향상됨\n- 예를 들어 외부 데이터베이스나 API를 통해 데이터를 받는 경우 구체화된 뷰가 데이터를 통합하여 처리하는 데 도움이 됨\n\n### 주기적 배치 처리\n주기적으로 배치 처리를 하는 경우\n- 구체화된 뷰는 주기적인 배치 처리가 필요한 상황에 유용\n- 예를 들어, 금융 기관은 구체화된 뷰를 사용하여 당일 잔액과 이자 계산을 저장할 수 있음\n- 또는 포트폴리오 성과 요약을 저장하여 각 영업일이 끝날 때 새로 고칠 수도 있음\n\n## 구체화된 뷰는 어떻게 동작하나요?\n구체화된 뷰는 특정 쿼리의 결과를 미리 계산하고 데이터베이스의 물리적 테이블로 저장하는 방식으로 작동함. 데이터베이스는 정기적으로 사전 계산을 수행하거나 사용자가 특정 이벤트를 통해 사전 계산을 트리거할 수 있음.\n\n### 구체화된 뷰 생성\n- 구체화된 뷰를 생성하기 위해 하나 이상의 소스 테이블에서 원하는 데이터를 검색하는 쿼리를 정의 (이 쿼리에는 필요에 따라 필터링, 집계, 조인 및 기타 작업이 포함될 수 있음)\n- 데이터베이스는 처음에 소스 데이터에 대해 정의된 질의를 실행하여 구체화된 뷰를 채움 (쿼리 결과는 데이터베이스의 물리적 테이블로 저장되며 이 테이블은 구체화된 뷰를 나타냄)\n\n### 구체화된 뷰 업데이트\n- 구체화된 뷰의 데이터는 소스 테이블에 있는 기본 데이터의 변경 사항을 반영하도록 주기적으로 업데이트해야 함 (데이터 새로 고침 빈도는 사용 사례 및 요구 사항에 따라 다름)\n\n## 뷰와 구체화된 뷰의 차이점은 무엇이가요?\n### 관계형 데이터베이스에서의 *뷰*\n- 여러 기본 테이블의 데이터를 변환하고 결합하여 만든 임시 테이블\n- 데이터 자체를 저장하지 않는 가상 테이블\n- 사용자가 뷰를 쿼리할 때마다 데이터베이스 엔진은 원본 테이블에 대해 기본 쿼리를 실행하여 결과를 동적으로 계산함\n- 뷰의 데이터는 액세스할 때마다 원본 테이블에서 직접 전달되므로 항상 최신 상태\n### 구체화된 뷰\n- 특정 쿼리의 결과를 데이터베이스의 물리적 테이블로 저장\n- 구체화된 뷰의 데이터는 미리 계산되어 저장되므로 뷰에 액세스할 때마다 쿼리를 다시 계산할 필요 없이 이미 나온 결과를 사용할 수 있음\n- 그러나 구체화된 뷰의 데이터가 항상 최신 상태인 것은 아님\n- 따라서 데이터 최신성과 쿼리 성능 간의 균형을 맞추려면 업데이트 빈도를 구성해야 함\n\n## References\nhttps://aws.amazon.com/ko/what-is/materialized-view/","n":0.038}}},{"i":155,"$":{"0":{"v":"Stored Procedure","n":0.707},"1":{"v":"\n## What\n\n- 저장 프로시저\n- 일련의 쿼리를 마치 하나의 함수처럼 실행하기 위한 쿼리의 집합\n- 데이터베이스에 대한 일련의 작업을 정리한 절차를 DBFS에 저장한 것\n- `Persistent Storage Module`이라고도 불림\n\n## How\n\n- 생성 (~ 커피 자판기)\n    - `CREATE PROCEDURE ~`\n- 호출 (~ 커피 뽑기)\n    - `CALL ~`\n    - `EXEC ~`\n\n## Example\n\n```sql\nCREATE PROCEDURE dbo.sp_getCustomerInfo @CustomerID int\nAS\nBEGIN\n\tSELECT * FROM dbo.customer WHERE CustomerID = @CustomerID\nEND\n```","n":0.132}}},{"i":156,"$":{"0":{"v":"Storage","n":1},"1":{"v":"\n## Block, File and Object\n![](https://tech.gluesys.com/assets/blockfileobject.png)\n\n### Block Storage\n- 데이터를 고정된 사이즈의 블록으로 나누어 각각 고유한 식별자와 함게 저장하는 방식\n\n### File Storage\n- 데이터는 계층적 파일 디렉터리 내의 폴더에 저장됨\n\n### Object Storage\n- Object라고 불리는 개별 유닛에 데이터를 저장하는 스토리지 포맷\n- 각 유닛에는 고유의 식별자 혹은 키가 있어서 분산된 시스템 내 어디에 저장되어 있든지 상관없이 데이터를 찾을 수 있음\n\n## Summary\n![](https://cdn.imweb.me/upload/S202001291023ba77fb258/e6131cf5b2abd.png)\n\n## Reference\n- [블록, 파일, 오브젝트 스토리지 쉽게 이해하기](https://www.dknyou.com/blog/?q=YToxOntzOjEyOiJrZXl3b3JkX3R5cGUiO3M6MzoiYWxsIjt9&bmode=view&idx=10474168&t=board)\n- [오브젝트 스토리지란](https://tech.gluesys.com/blog/2021/04/20/storage_9_intro.html)","n":0.124}}},{"i":157,"$":{"0":{"v":"OLTP vs OLAP","n":0.577},"1":{"v":"\n## OLTP (Online Transactional Processing; 온라인 트랜잭션 처리)\n- **트랜잭션 중심**의 데이터 처리 시스템을 일컫는 용어<br>\n    ㄴ 데이터베이스의 데이터를 수시로 갱신하는 프로세싱<br>\n    ㄴ 호스트 컴퓨터가 데이터베이스를 액세스하고, 바로 처리 결과를 돌려주는 형태\n- 빠르고 효율적인 쿼리와 정확한 최신 정보를 필요로 하는 경우에 적합\n- 행 기반\n- 기존 DBMS\n- Amazon RDS\n\n## OLAP (Online Analytical Processing; 온라인 분석 처리)\n- 성능 및 일상적인 사용보다는 데이터 분석 및 의사 결정에 초점을 맞춘 데이터 처리 시스템을 일컫는 용어<br>\n    ㄴ 대용량 데이터 집합을 대상\n- 보통은 비즈니스 인텔리전스(BI) 솔루션과 연동\n- 컬럼 기반\n- Amazon Redshift\n\n## Summary\n\n| **구분**        | **OLTP**                                | **OLAP**         |\n|---------------|-----------------------------------------|------------------|\n| **기초**        | 많은 수의 온라인 단기거래 관리                       | 데이터 분석에 사용       |\n| **데이터베이스 타입** | 기존 DBMS 사용                              | 데이터 웨어하우스(DW) 사용 |\n| **데이터 수정**    | 모든 insert, update, delete transaction 관리 | 데이터 읽기에 사용       |\n| **응답시간**      | 밀리초 단위                                  | 처리가 조금 느림        |\n| **정규화**       | 정규화 O                                   | 정규화 X            |\n| **데이터 특성**    | 트랜잭션 중심                                 | 주제 중심            |\n\n\n## Reference\n- https://simroot.tistory.com/24","n":0.08}}},{"i":158,"$":{"0":{"v":"데이터베이스 정규화","n":0.707},"1":{"v":"\n\n## Reference\n- https://wkdtjsgur100.github.io/database-normalization/","n":0.577}}},{"i":159,"$":{"0":{"v":"Hot Block","n":0.707},"1":{"v":"\n## What & Why\n- 데이터베이스 시스템에서 자주 접근되는 데이터 블록\n    - 데이터베이스의 블록은 데이터 저장의 기본 단위로서, 특정 크기의 데이터 집합을 의미\n- 특정 데이타 블록에 대한 요청 또는 액세스가 과도하게 높아져서 시스템의 성능에 영향을 줌\n- 문제 현상\n    1. 성능 저하: 많은 사용자나 프로세스가 동일한 데이터 블록에 동시에 액세스하려고 할 때, 이 블록에 대한 액세스 경쟁이 발생하게 되어 성능이 저하될 수 있음\n    2. 대기 시간 증가: 데이터 블록에 대한 요청이 폭증하면, 다른 사용자나 프로세스는 해당 블록에 액세스하기 위해 대기해야 할 수 있음\n\n## When\n한정 판매 이벤트나 주문 시스템 같은 경우에는 많은 사용자가 동시에 동일한 데이터 블록에 액세스하려고 시도할 수 있음\n\n## How\n1. 데이터 분할: 데이터를 여러 블록이나 테이블로 분할하여 액세스를 분산시킴\n2. 캐싱: 자주 접근되는 데이터를 메모리에 캐싱하여 빠른 응답 시간을 제공할 수 있음\n3. 로드 밸런싱: 요청을 여러 서버나 리소스에 분산시켜 hot block 문제를 완화할 수 있음","n":0.087}}},{"i":160,"$":{"0":{"v":"Change data capture (CDC)","n":0.5},"1":{"v":"\n## What\n\nIn databases, change data capture (CDC) is a set of software design patterns used to determine and track the data that has changed so that action can be taken using the changed data.\n\nCDC is an approach to data integration that is based on the identification, capture and delivery of the changes made to enterprise data sources.\n\n## Reference\n- https://en.wikipedia.org/wiki/Change_data_capture","n":0.13}}},{"i":161,"$":{"0":{"v":"ACID 트랜잭션","n":0.707},"1":{"v":"\n## Transaction\n  - 데이터베이스에서 데이터에 대한 하나의 논리적 실행 단계<br>\n    ㄴ 예) 은행에서의 계좌 이체 : 계좌 이체의 구현은 내부적으로 여러 단계로 이루어질 수 있지만 전체적으로는 '송신자 계좌의 금액 감소'와 '수신자 계좌의 금액 증가'가 한 동작으로 이루어져야 함\n\n## ACID\n  - `Atomicity` (원자성)<br>\n    : transaction과 관련된 작업들이 부분적으로 실행되다가 중단되지 않는 것을 보장하는 능력\n  - `Consistency` (일관성)<br>\n    : transaction이 실행을 성공적으로 완료하면 언제나 일관성 있는 데이터베이스 상태로 유지하는 것\n  - `Isolation` (독립성)<br>\n    : transaction을 수행 시 다른 transaction의 연산 작업이 끼어들지 않도록 보장하는 것\n  - `Durability` (지속성)<br>\n    : 성공적으로 수행된 transaction은 영원히 반영되어야 함\n  \ncf)\n  - ACID stands for atomicity, consistency, isolation, and durability.\n  - `Atomicity` means that all transactions either succeed or fail completely.\n  - `Consistency` guarantees relate to how a given state of the data is observed by simultaneous operations.\n  - `Isolation` refers to how simultaneous operations potentially conflict with one another.\n  - `Durability` means that committed changes are permanent.\n\n## Reference\n- https://ko.wikipedia.org/wiki/ACID\n- https://docs.databricks.com/lakehouse/acid.html","n":0.081}}},{"i":162,"$":{"0":{"v":"Cloud","n":1}}},{"i":163,"$":{"0":{"v":"Solutions Architect 인터뷰 준비 - Troubleshooting","n":0.408},"1":{"v":"\n# 1. Infrastructure\n\n<details>\n<summary>서버가 갑자기 응답하지 않을 때 첫 번째로 확인해야 할 것은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 서버가 응답하지 않을 때 첫 번째로 확인해야 할 것은 서버의 전원 상태와 네트워크 연결 상태입니다. 그 후, 서버의 로그나 시스템 모니터링 도구를 통해 추가적인 문제점을 파악해야 합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>서버의 CPU 사용률이 계속 100%에 가까울 때 어떤 절차를 밟아야 하나요?</summary>\n<div markdown=\"1\">\n\n- 서버의 CPU 사용률이 높을 때, 우선 `top` 또는 `htop`과 같은 명령어를 사용하여 어떤 프로세스가 CPU를 많이 사용하는지 확인해야 합니다. 과도한 CPU 사용의 원인을 찾아 해당 프로세스를 최적화하거나 필요에 따라 재시작해야 할 수도 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>서버에 저장 공간이 부족하다는 경고가 나타났을 때 어떤 조치를 취해야 하나요?</summary>\n<div markdown=\"1\">\n\n- 저장 공간이 부족할 때, `df`와 `du` 명령어를 사용하여 어느 파티션이나 디렉토리가 공간을 많이 차지하고 있는지 확인합니다. 불필요한 파일이나 로그, 캐시를 정리하여 공간을 확보하거나, 필요하다면 추가 스토리지를 확장 또는 추가하는 작업을 계획해야 합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>서버 부팅 시 'Filesystem check failed' 오류가 발생하면 어떻게 대처해야 하나요?</summary>\n<div markdown=\"1\">\n\n- 이 오류는 파일 시스템에 문제가 있음을 나타냅니다. fsck 명령어를 사용하여 해당 파일 시스템의 무결성을 검사하고 문제점을 해결해야 합니다. 필요하다면 백업을 먼저 수행하고 작업을 진행하는 것이 좋습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>서버에서 RAM 사용량이 계속 높다고 경고가 나올 때 어떤 절차를 밟아야 하나요?</summary>\n<div markdown=\"1\">\n\n- `free` 명령어나 `vmstat`을 사용하여 실제 메모리 사용량을 확인합니다. 그 다음, `top` 명령어로 어떤 프로세스가 메모리를 많이 차지하는지 확인하고, 해당 프로세스를 최적화하거나, 설정을 조정하거나 재시작해야 할 수도 있습니다. 지속적으로 높은 RAM 사용량이 발생한다면, 추가 RAM을 확장하는 것을 고려해야 합니다.\n\n</div>\n</details>\n\n<br />\n\n# 2. Networking\n\n<details>\n<summary>어떤 클라이언트에서 서버로의 연결이 실패할 때, 첫 번째로 확인해야 할 것은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 연결 실패 시 가장 먼저 확인해야 할 것은 클라이언트와 서버 사이의 네트워크 연결 상태입니다. ping 또는 traceroute 명령어를 사용하여 네트워크 경로와 연결 가능성을 확인해 보아야 합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Network TCP handshaking에서 트러블이 생겼을 때 어떻게 해결해야 하는지?</summary>\n<div markdown=\"1\">\n\n- TCP handshaking 문제 발생 시 `tcpdump`나 `wireshark`와 같은 패킷 캡쳐 도구를 사용하여 패킷을 분석해야 합니다. SYN, SYN-ACK, ACK 패킷의 교환 상태를 확인하고, 필요하다면 방화벽 설정이나 네트워크 장비 설정을 점검해야 합니다.\n\n</div>\n</details>\n\n<br />\n<details>\n<summary>VPN 연결이 자주 끊길 때 어떤 조치를 취해야 하나요?</summary>\n<div markdown=\"1\">\n\n- VPN 연결 끊김 현상 발생 시, VPN 로그를 확인하여 오류 메시지나 원인을 파악해야 합니다. 그 후, VPN 서버와 클라이언트의 설정을 점검하고, 네트워크의 안정성, 중간 네트워크 장비의 상태 및 설정도 확인해야 합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>DHCP 서버에서 IP 주소를 할당받지 못할 때 어떤 절차를 밟아야 하나요?</summary>\n<div markdown=\"1\">\n\n- DHCP IP 할당 문제 발생 시, 클라이언트의 네트워크 설정 및 케이블 연결 상태를 체크해야 합니다. 그 다음, DHCP 서버의 설정과 상태, IP 주소 풀의 사용량 등을 확인하여 문제의 원인을 찾아 해결해야 합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>네트워크 대역폭이 갑자기 줄어들었을 때, 어떤 절차를 밟아야 하나요?</summary>\n<div markdown=\"1\">\n\n- 네트워크 대역폭 감소 시, 우선 네트워크 트래픽을 모니터링하는 도구를 사용하여 현재 네트워크 사용량과 트래픽의 출처를 확인해야 합니다. 문제의 원인이 되는 트래픽이나 장비를 찾아내어 적절한 조치를 취해야 합니다. 필요하다면 네트워크 장비의 설정을 수정하거나 업데이트를 고려해야 합니다.\n\n</div>\n</details>\n\n<br />\n\n# 3. AppDev\n\n<details>\n<summary>애플리케이션 로딩 시간이 갑자기 길어졌을 때 첫 번째로 확인해야 할 것은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 로딩 시간이 길어진 원인을 파악하기 위해, 애플리케이션 로그를 확인하고 프로파일러를 사용하여 병목 구간을 파악해야 합니다. 데이터베이스 쿼리의 효율성, 외부 API 호출, 리소스 로딩 등을 점검해야 합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>애플리케이션에서 갑자기 500 Internal Server Error가 발생한다면 어떤 조치를 취해야 하나요?</summary>\n<div markdown=\"1\">\n\n- 500 오류는 서버 내부에서 발생한 오류를 나타냅니다. 먼저 애플리케이션의 로그를 확인하여 오류의 원인을 파악해야 합니다. 코드의 문제, 데이터베이스 연결 문제, 서버 자원 문제 등 다양한 원인이 있을 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>API 호출 시 응답 시간이 너무 길다면 어떤 절차를 밟아야 하나요?</summary>\n<div markdown=\"1\">\n\n- API 응답 시간이 길 경우, 해당 API의 로그와 코드를 분석하여 병목 구간을 파악해야 합니다. 또한, 네트워크 지연, 서버 자원 부족, 데이터베이스 쿼리 최적화 필요 등의 문제도 고려해야 합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>애플리케이션 배포 후 특정 기능이 제대로 동작하지 않는다면 어떻게 해결해야 하나요?</summary>\n<div markdown=\"1\">\n\n- 배포 후 기능 장애가 발생한 경우, 배포 이전과 이후의 코드 및 설정 차이를 확인하고, 배포 로그를 점검해야 합니다. 문제의 원인을 파악한 후, 롤백하거나 버그를 수정하여 재배포를 고려해야 합니다.\n\n</div>\n</details>\n\n<br />\n<details>\n<summary>애플리케이션에서 메모리 누수가 의심될 때 어떤 도구나 방법을 사용하여 문제를 진단하고 해결할 수 있나요?</summary>\n<div markdown=\"1\">\n\n- 메모리 누수 문제를 진단하기 위해 프로파일러나 메모리 모니터링 도구를 사용해야 합니다. 해당 도구를 통해 메모리 사용량과 가비지 컬렉션 상황을 모니터링하면서 문제의 원인을 찾아야 합니다. 문제가 발견되면 코드를 수정하여 메모리 누수를 해결해야 합니다.\n\n</div>\n</details>\n\n<br />\n\n# 4. Security\n \n<details>\n<summary>웹 애플리케이션에 SQL 인젝션 공격이 의심될 때 첫 번째로 어떤 조치를 취해야 하나요?</summary>\n<div markdown=\"1\">\n\n- SQL 인젝션 공격이 의심될 경우, 해당 공격의 원인이 될 수 있는 입력값 및 쿼리 로그를 즉시 확인해야 합니다. 잠재적인 피해를 줄이기 위해 해당 기능이나 서비스를 일시적으로 중단하고, 파라미터화된 쿼리나 ORM을 사용하여 코드를 보완해야 합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>DDoS 공격이 발생했다는 알림을 받았을 때, 어떤 조치를 취해야 하나요?</summary>\n<div markdown=\"1\">\n\n-  DDoS 공격이 발생할 경우, 트래픽 모니터링 도구를 사용하여 공격의 패턴 및 원천을 파악해야 합니다. 공격을 미리 방지하거나 완화하기 위해 DDoS 방어 솔루션, 웹 방화벽, 트래픽 제한 기능을 활용해야 합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>내부 직원이 기업의 중요 데이터에 무단으로 접근했다는 의심을 받았을 때, 어떤 절차를 밟아야 하나요?</summary>\n<div markdown=\"1\">\n\n- 내부 위협의 의심이 생긴 경우, 접근 로그와 사용자 활동을 철저히 검토하여 무단 접근 여부를 확인해야 합니다. 필요한 경우 접근 권한을 즉시 수정하고, 해당 직원과의 인터뷰를 통해 상황을 파악해야 합니다. 또한, 보안 인식 교육 및 접근 제어 정책 강화를 고려해야 합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>애플리케이션에서 사용자 패스워드가 평문으로 저장되고 있다는 사실을 알게 되었습니다. 어떻게 이 문제를 해결해야 하나요?</summary>\n<div markdown=\"1\">\n\n- 패스워드는 절대 평문으로 저장되어서는 안됩니다. 즉시 해시 함수와 솔트를 사용하여 패스워드를 암호화해야 합니다. 사용자들에게 패스워드 변경을 권장하고, 보안 인식 교육 및 개발 프로세스에 보안 검토를 포함시켜야 합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>SSL/TLS 인증서가 만료되어 웹사이트가 접근되지 않는다고 알림을 받았습니다. 어떤 조치를 취해야 하나요?</summary>\n<div markdown=\"1\">\n\n- SSL/TLS 인증서 만료 시, 즉시 새로운 인증서를 발급받아야 합니다. 발급받은 인증서를 웹 서버에 적용하고, 서버를 재시작한 후 접속이 정상적으로 이루어지는지 확인해야 합니다. 또한, 인증서 만료일을 모니터링하여 미리 알림을 받을 수 있도록 시스템을 설정하는 것이 좋습니다.\n\n</div>\n</details>\n\n<br />\n\n# 5. Database & Analytics\n\n<details>\n<summary>사용자들이 데이터베이스 조회 속도가 급격히 느려졌다고 합니다. 이 문제를 어떻게 해결하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- 먼저, 데이터베이스의 CPU, 메모리, 디스크 I/O 사용률 등의 성능 지표를 확인합니다. SQL 쿼리의 실행 계획을 분석하여 비효율적인 쿼리가 있는지 확인하고, 인덱싱 등의 최적화 방법을 적용합니다. 필요하다면 데이터베이스 구조 및 설정을 검토하여 튜닝합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>복제(replication) 환경에서 슬레이브 데이터베이스의 데이터가 마스터와 동기화되지 않는 문제가 발생했습니다. 어떻게 접근하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- 복제 로그와 에러 로그를 체크하여 문제의 원인을 파악합니다. 네트워크 문제, 디스크 문제, 설정 오류 등 다양한 원인이 있을 수 있습니다. 문제의 원인을 파악한 후 적절한 조치를 취하며, 필요하다면 복제를 재설정합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>데이터 웨어하우스에서 최근 데이터 추출 작업(ETL)이 실패했습니다. 원인과 해결 방법은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- ETL 실패의 원인은 다양합니다. 로그 파일을 확인하여 실패한 작업의 상세한 에러 메시지를 확인합니다. 데이터 형식의 문제, 소스 시스템의 변경, 네트워크 문제 등이 원인이 될 수 있습니다. 문제의 원인을 정확히 파악한 후, 적절한 수정 작업을 수행하여 ETL을 재실행합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>사용자의 실시간 애널리틱스 요청이 데이터베이스 성능 저하를 일으키고 있습니다. 이 문제를 어떻게 해결하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- 실시간 애널리틱스 요청과 OLTP 작업을 분리하는 것이 좋습니다. Read Replica나 별도의 분석용 데이터베이스를 구성하여, 애널리틱스 쿼리는 해당 데이터베이스에서 실행되게 합니다. 또한, 캐싱 솔루션을 도입하여 자주 요청되는 쿼리 결과를 빠르게 제공할 수 있도록 합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>데이터베이스에 대량의 데이터 삽입 작업 중 시스템이 다운되었습니다. 장애 복구 절차는 어떻게 되나요?</summary>\n<div markdown=\"1\">\n\n- 먼저, 데이터베이스의 로그와 시스템 로그를 확인하여 다운의 원인을 파악합니다. 데이터베이스 백업 및 로그 백업 상태를 확인하고, 최근 백업으로 복원 후 로그를 적용하여 장애 발생 직전 상태로 복구합니다. 복구 후 장애의 원인을 분석하고 재발 방지 대책을 수립합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>데이터베이스에 시간 지연이 발생하거나 failover 상황이 생겼을 때 어떠한 조치를 취해야 하는지 설명하십시오.</summary>\n<div markdown=\"1\">\n\n- 문제의 원인 파악: 첫 번째 단계는 항상 문제의 원인을 파악하는 것입니다. 로그를 확인하여 오류 메시지나 경고를 확인합니다. 시스템 모니터링 도구를 사용하여 리소스 사용량, 네트워크 지연, 디스크 문제 등을 확인합니다.\n\n- Performance Bottlenecks 확인: 성능 문제의 원인이 될 수 있는 네트워크 지연, CPU 및 메모리 과부하, 디스크 I/O 문제 등을 체크합니다. 또한, 데이터베이스 쿼리 성능이 저하된 경우 쿼리 최적화가 필요할 수 있습니다.\n\n- Failover: 주 데이터베이스에 문제가 발생한 경우, 보조 데이터베이스로 자동으로 전환되도록 설정된 경우, 이를 확인하십시오. 자동 failover가 설정되어 있지 않은 경우, 수동으로 보조 데이터베이스로 전환할 수 있습니다.\n\n- Backup 및 복구: 데이터베이스 손상의 경우, 최근의 백업에서 데이터베이스를 복구하는 것을 고려해야 합니다. 이는 데이터 손실을 최소화하며 서비스를 빠르게 복구할 수 있습니다.\n\n- 대용량 트래픽 대응: 일시적인 대용량 트래픽으로 인한 지연의 경우, 로드 밸런서를 사용하여 여러 데이터베이스 인스턴스 간에 트래픽을 분산시킬 수 있습니다.\n\n- 향후 예방: 한 번 문제가 발생하면, 같은 문제가 미래에 다시 발생하지 않도록 예방 조치를 취하는 것이 중요합니다. 이를 위해 모니터링 도구를 사용하여 시스템을 지속적으로 감시하고, 정기적인 백업과 복구 전략을 갖추며, 데이터베이스 성능 최적화를 수행합니다.\n</div>\n</details>\n<br />\n\n# 6. Machine Learning & Artificial Intelligence\n\n<details>\n<summary>학습한 모델이 훈련 데이터에서는 높은 정확도를 보이지만, 실제 운영 환경에서의 성능이 매우 저조하다는 피드백이 있습니다. 이 문제를 어떻게 해결하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- 이 문제는 과적합(overfitting)으로 보입니다. 모델이 훈련 데이터에 과도하게 최적화되어 있어 실제 데이터에서는 제대로 작동하지 않을 수 있습니다. 데이터의 다양성을 높이거나 규제(regularization)를 적용하여 모델을 간소화하고, 교차 검증(cross-validation)을 수행하여 모델의 일반화 성능을 확인하겠습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>딥러닝 모델 학습 중에 NaN(Not a Number) 값이 발생하고 있습니다. 어떤 원인이 있을 수 있나요?</summary>\n<div markdown=\"1\">\n\n- NaN 값은 학습률(learning rate)이 너무 높아 가중치 업데이트 과정에서 발산하는 경우, 초기화 문제, 또는 로스 함수에서 발생할 수 있습니다. 학습률을 조절하거나 다른 최적화 알고리즘을 사용하고, 가중치 초기화 방식을 변경해보겠습니다. 로스 함수도 확인하여 이상한 값이 발생하는지 점검하겠습니다.\n\n</div>\n</details>\n\n<br />\n<details>\n<summary>Natural Language Processing(NLP) 프로젝트에서 텍스트 데이터 전처리 과정 중 일부 문자열이 제대로 처리되지 않았습니다. 어떻게 접근하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- 전처리 과정에서 사용한 파이프라인과 함수들을 차례대로 검토하겠습니다. 특수 문자, 인코딩 문제, 정규식 등 사용한 모든 변환 메커니즘을 체크하면서 문제의 원인을 파악하겠습니다. 문제의 원인을 찾으면, 해당 부분을 수정하여 전처리 과정을 재진행하겠습니다.\n\n</div>\n</details>\n\n<br />\n<details>\n<summary>이미지 인식 모델에서 특정 클래스의 이미지를 잘못 분류하는 문제가 지속적으로 발생하고 있습니다. 어떻게 해결하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- 먼저, 해당 클래스의 훈련 데이터를 검토하여 데이터의 질과 양을 확인하겠습니다. 데이터에 노이즈나 부정확한 라벨이 있을 수 있습니다. 데이터 양이 부족하다면 데이터 증강(augmentation) 기법을 사용하여 데이터를 확장하겠습니다. 또한, 모델 구조나 학습 파라미터를 조정하여 성능을 개선하려고 노력하겠습니다.\n\n</div>\n</details>\n\n<br />\n<details>\n<summary>텍스트 기반의 추천 시스템이 사용자의 선호도와 맞지 않게 추천을 하고 있습니다. 원인과 해결 방법은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 추천 시스템의 문제 원인은 다양합니다. 사용자의 상호작용 데이터가 충분하지 않거나, 현재 모델이 사용자의 선호도를 정확히 반영하지 못하는 방식으로 학습되었을 수 있습니다. 먼저 사용자의 데이터와 상호작용 패턴을 분석하고, 모델의 구조와 학습 데이터를 점검하겠습니다. 필요하다면, 모델을 재학습시키거나 다른 알고리즘을 적용하여 성능을 개선하겠습니다.\n\n</div>\n</details>\n\n<br />\n\n# 7. Virtualization\n\n<details>\n<summary>가상 머신(VM)이 갑자기 응답하지 않습니다. 어떤 단계를 밟아 문제를 해결하려고 하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- 먼저 VM의 상태를 체크하겠습니다. 호스트 시스템의 리소스 사용량, VM에 할당된 CPU, 메모리 리소스 등을 확인합니다. VM의 로그 파일도 검토하여 특별한 오류 메시지가 있는지 확인하겠습니다. 리소스 부족이 원인일 경우, VM에 더 많은 리소스를 할당하거나, 필요없는 VM을 종료하여 리소스를 확보하겠습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>하이퍼바이저에서 VM을 새로 생성했는데 네트워크에 연결되지 않습니다. 어떻게 문제를 해결하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- 먼저 VM의 네트워크 설정과 하이퍼바이저의 네트워크 구성을 검토하겠습니다. VM에 올바른 네트워크 인터페이스 카드(NIC)가 할당되었는지, 가상 스위치 설정이 제대로 되어 있는지 확인하겠습니다. 추가로, 보안 그룹이나 방화벽 설정이 올바른지도 확인하여 필요한 포트가 허용되어 있는지 점검하겠습니다.\n\n</div>\n</details>\n\n<br />\n<details>\n<summary>VM 내부에서 성능 저하가 느껴집니다. 원인이 무엇일까요?</summary>\n<div markdown=\"1\">\n\n- VM 성능 저하의 원인은 다양합니다. VM에 할당된 리소스가 충분하지 않을 수 있습니다. 디스크 I/O, CPU 사용률, 메모리 사용량 등을 모니터링하며, 병목 현상을 찾겠습니다. 또한, 하이퍼바이저 레벨에서 다른 VM과의 리소스 경쟁 상황도 고려해봐야 합니다. 적절한 리소스 조정과 최적화를 통해 성능을 개선하겠습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>VM 스냅샷을 복원하려고 했지만, 복원이 실패하고 있습니다. 어떻게 대응하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- 스냅샷 복원 실패는 스냅샷 파일의 손상, 저장소 문제, 호환성 문제 등 여러 원인이 있을 수 있습니다. 먼저, 스냅샷과 관련된 로그를 확인하여 구체적인 오류 메시지를 파악하겠습니다. 스냅샷 파일의 정합성 체크와 저장소의 상태도 점검하겠습니다. 필요한 경우, 다른 시점의 스냅샷으로 복원을 시도하거나, VM을 새로 생성하여 복원하겠습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>VM을 마이그레이션하려고 하는데, 호환성 문제로 실패합니다. 어떻게 해결하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- 호스트 하이퍼바이저 버전, VM 하드웨어 버전, OS 버전 등 호환성을 체크하겠습니다. 목표 시스템의 요구 사항과 현재 VM의 구성을 비교하며 문제의 원인을 파악하겠습니다. 호환성을 위해 VM의 하드웨어 버전을 업그레이드하거나, 필요한 경우 다운그레이드하여 마이그레이션을 수행하겠습니다.\n\n</div>\n</details>\n\n<br />\n\n# 8. Microservices Architecture\n\n<details>\n<summary>한 마이크로서비스가 다른 서비스에 응답하지 않을 때, 어떤 단계를 밟아 문제를 해결하려고 하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- 먼저 서비스 간의 네트워크 연결상태와 해당 서비스의 상태를 확인하겠습니다. 서비스 디스커버리 메커니즘이 올바르게 작동하는지 검토하며, 로드 밸런서와 네트워크 설정에 문제가 없는지 점검합니다. 로그 및 모니터링 도구를 통해 오류 메시지나 특별한 경고를 확인하겠습니다.\n\n</div>\n</details>\n\n<br />\n<details>\n<summary>마이크로서비스 아키텍처에서 특정 서비스가 지연되면 전체 시스템의 응답성이 떨어집니다. 이 문제를 어떻게 해결하겠습니까?</summary>\n<div markdown=\"1\">\n\n- Circuit Breaker 패턴을 도입하여 지연된 서비스로의 호출이 시스템 전체의 성능에 영향을 미치지 않도록 할 것입니다. 이 패턴은 장애가 있는 서비스를 임시로 차단하여 시스템 전체의 안정성을 유지합니다. 또한, 지연이 발생하는 원인을 파악하여 해당 서비스를 최적화하거나 확장하겠습니다.\n\n</div>\n</details>\n\n<br />\n<details>\n<summary>마이크로서비스에서 데이터 일관성을 유지하는 것이 어렵습니다. 어떻게 이 문제를 해결하겠습니까?</summary>\n<div markdown=\"1\">\n\n- 마이크로서비스 아키텍처에서는 데이터 일관성을 유지하기 위해 이벤트 기반의 아키텍처와 사가 패턴을 사용합니다. 서비스 간의 데이터 변경은 이벤트를 발행하고, 다른 서비스는 해당 이벤트를 구독하여 데이터를 동기화하거나 관련 비즈니스 로직을 실행합니다. 사가 패턴은 여러 서비스에 걸친 트랜잭션을 조율하며, 롤백 메커니즘을 포함하여 데이터의 일관성을 보장합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>특정 마이크로서비스가 자주 실패합니다. 어떻게 안정성을 높일 수 있을까요?</summary>\n<div markdown=\"1\">\n\n- 해당 마이크로서비스의 로그와 모니터링 데이터를 분석하여 실패의 원인을 파악하겠습니다. 리소스 부족, 코드의 버그, 외부 서비스와의 통신 문제 등 다양한 원인이 있을 수 있습니다. 원인을 파악한 후, 적절한 방법으로 문제를 해결하겠습니다. 예를 들면, 자동 스케일링, 재시도 메커니즘 도입, 코드의 최적화 등의 방법을 사용할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>마이크로서비스 아키텍처에서 여러 서비스가 동일한 데이터베이스를 공유하고 있습니다. 이로 인해 문제가 발생하였습니다. 어떻게 개선하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- 마이크로서비스 아키텍처의 핵심 원칙 중 하나는 각 서비스가 자신의 데이터를 소유하고 독립적으로 관리하는 것입니다. 공유 데이터베이스를 사용하면 서비스 간의 강한 결합이 발생할 수 있습니다. 이를 해결하기 위해, 각 마이크로서비스에 대해 독립된 데이터 스토어를 제공하겠습니다. 필요한 경우, 이벤트 기반의 데이터 동기화나 API 호출을 통해 다른 서비스의 데이터에 접근하게 할 것입니다.\n\n</div>\n</details>\n\n<br />\n\n# 9. Containerization\n\n<details>\n<summary>Docker 컨테이너가 예기치 않게 종료되었습니다. 이를 파악하고 해결하기 위해 어떤 절차를 밟을 것인가요?</summary>\n<div markdown=\"1\">\n\n- 먼저, `docker logs [container_name_or_id]` 명령을 사용하여 해당 컨테이너의 로그를 확인합니다. 이를 통해 컨테이너가 종료된 원인을 파악할 수 있습니다. 또한, `docker inspect [container_name_or_id]`를 사용하여 컨테이너의 구성을 확인하며 문제의 원인을 찾을 수 있습니다. 필요한 경우, 컨테이너의 리소스 할당량을 조절하거나 의존성 문제를 해결하여 안정적으로 실행될 수 있도록 조치합니다.\n\n</div>\n</details>\n\n<br />\n<details>\n<summary>Docker 컨테이너간의 네트워크 통신에 문제가 발생하였습니다. 어떻게 문제를 해결하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- `docker network inspect [network_name]` 명령을 사용하여 해당 네트워크의 구성과 컨테이너 간의 연결 상태를 확인합니다. 네트워크 설정이나 컨테이너의 네트워크 설정에 문제가 없는지 점검하고, 필요한 경우 네트워크 설정을 조절합니다. 또한, 컨테이너 내부의 네트워크 설정 및 서비스 구성도 확인하여 통신 문제를 해결합니다.\n\n</div>\n</details>\n\n<br />\n<details>\n<summary>Kubernetes 클러스터에서 특정 파드가 계속해서 재시작됩니다. 이 문제를 어떻게 진단하고 해결하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- `kubectl describe pod [pod_name]` 및 `kubectl logs [pod_name]` 명령을 사용하여 파드의 상태와 로그를 확인합니다. 이를 통해 파드의 재시작 원인을 파악할 수 있습니다. 또한, 파드의 리소스 제한, 라이브니스 및 레디니스 프로브 설정, 의존성 문제 등을 점검하여 문제를 해결합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Docker 이미지를 빌드할 때 `Dockerfile`에서 발생하는 오류가 있습니다. 이를 어떻게 해결하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- 먼저, 오류 메시지를 자세히 확인하여 `Dockerfile`에서 발생하는 문제점을 파악합니다. 문제가 발생하는 특정 명령어나 설정을 확인하고, 문법 오류나 경로 문제, 의존성 문제 등을 점검합니다. 필요한 경우, 해당 부분을 수정하거나 보완하여 이미지 빌드 오류를 해결합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>컨테이너화된 애플리케이션에서 메모리 부족 문제가 발생하였습니다. 어떻게 해결하시겠습니까?</summary>\n<div markdown=\"1\">\n\n- 컨테이너의 리소스 제한 설정을 확인하여 메모리 할당량이 적절한지 검토합니다. 필요한 경우, 컨테이너에 할당된 메모리를 증가시킵니다. 또한, 애플리케이션의 메모리 사용 패턴을 모니터링 도구를 통해 확인하고, 애플리케이션 코드 내의 메모리 누수나 비효율적인 리소스 사용이 있는지 검토하여 문제를 해결합니다.\n\n</div>\n</details>","n":0.021}}},{"i":164,"$":{"0":{"v":"Solutions Architect 인터뷰 준비 - 기본 개념","n":0.378},"1":{"v":"\n# 1. Infrastructure\n\n<details>\n<summary>인프라스트럭처를 구성할 때 IaaS, PaaS, SaaS의 차이점은 무엇이며 어떤 것을 선택해야 하는지 설명해주세요.</summary>\n<div markdown=\"1\">\n\n- IaaS(Infrastructure as a Service): 가상화된 컴퓨팅 리소스를 제공합니다. 예: AWS EC2, Azure Virtual Machines.\n- PaaS(Platform as a Service): 개발, 실행, 관리를 위한 플랫폼을 제공합니다. 예: AWS Elastic Beanstalk, Azure App Service.\n- SaaS(Software as a Service): 클라우드 기반의 응용 프로그램을 제공합니다. 예: Google Workspace, Salesforce.\n  \n  선택은 특정 작업의 요구 사항, 관리의 용이성, 비용 효율성에 따라 결정됩니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>온프레미스와 클라우드 인프라스트럭처의 주요 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 온프레미스는 조직의 물리적인 위치에 설치된 인프라를 나타내며, 자체 서버와 데이터 센터에서 관리됩니다. 반면 클라우드 인프라는 서드파티 제공자가 호스팅하며, 인터넷을 통해 액세스됩니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>높은 가용성을 위한 인프라 설계 전략은 무엇이 있나요?</summary>\n<div markdown=\"1\">\n\n- 여러 데이터 센터나 가용 영역 간의 리소스 분산, 클러스터링, 로드 밸런싱, 자동 장애 복구, 백업 및 재해 복구 전략 등이 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Terraform과 CloudFormation의 주요 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- Terraform은 다양한 클라우드 제공자에 걸쳐 사용할 수 있는 Infrastructure as Code 도구입니다. 반면, CloudFormation은 AWS 전용입니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>컨테이너와 VM의 주요 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- VM은 각각의 OS를 포함하며, 컨테이너는 호스트 OS의 일부를 공유하며 동작합니다. 따라서 컨테이너는 VM에 비해 가볍고 빠르게 시작됩니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>인프라를 코드로 관리하는 이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 자동화, 일관성, 재사용 가능성, 문서화, 버전 관리 및 협업의 향상 등이 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>성능 모니터링 도구를 사용하여 인프라의 어떤 측면을 모니터링 할 수 있나요?</summary>\n<div markdown=\"1\">\n\n- CPU 사용률, 메모리 사용, 디스크 I/O, 네트워크 대역폭 및 지연 등을 모니터링할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>인프라 보안의 핵심 요소는 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 방화벽, 암호화, 접근 제어, 감사 및 로깅, 보안 패치 및 업데이트 관리 등이 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>하이브리드 클라우드 인프라스트럭처의 장점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 유연성, 비용 효율성, 보안 및 규정 준수, 최적의 리소스 사용 등이 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>인프라를 자동화하기 위해 어떤 도구나 기술을 사용할 수 있나요?</summary>\n<div markdown=\"1\">\n\n- Terraform, Ansible, Chef, Puppet, Jenkins 등의 도구 및 기술을 사용하여 인프라를 자동화할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>RAID는 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- RAID(Redundant Array of Independent Disks)는 여러 개의 디스크를 하나의 논리적 단위로 묶어 데이터의 신뢰성과 성능을 향상시키는 기술입니다. RAID는 여러 가지 레벨(RAID 0, RAID 1, RAID 5 등)로 구성될 수 있으며, 각 레벨은 데이터 분배 및 복제 방식에 따라 다른 성능과 내구성 특성을 가집니다.\n\n</div>\n</details>\n\n<br />\n\n# 2. Networking\n\n<details>\n<summary>OSI 모델은 무엇이며, 각 계층을 설명해주세요.</summary>\n<div markdown=\"1\">\n\n- OSI(Open Systems Interconnection) 모델은 네트워킹의 표준 모델로, 7개의 계층으로 구분됩니다. 물리, 데이터 링크, 네트워크, 전송, 세션, 표현, 응용 계층입니다. 각 계층은 특정한 네트워크 기능을 담당합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>TCP와 UDP의 주요 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- TCP는 연결 지향적이로 신뢰성 있는 통신을 제공합니다. 데이터 전송의 정확성을 확인합니다. 반면, UDP는 연결 없이 데이터를 전송하며, 신뢰성 보다는 속도를 중요시합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>NAT(Network Address Translation)의 역할은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- NAT는 사설 IP 주소를 공인 IP 주소로 변환하는 과정을 말합니다. 이를 통해 여러 장치가 하나의 공인 IP 주소를 사용하여 인터넷에 접속할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>VPN이란 무엇이며, 어떻게 작동하나요?</summary>\n<div markdown=\"1\">\n\n- VPN(Virtual Private Network)은 퍼블릭 네트워크를 통해 가상의 사설 네크워크 연결을 생성하는 기술입니다. 암호화를 사용해 데이터의 보안을 보장합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>CIDR 표기법이란 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- CIDR(Classless Inter-Domain Routing) 표기법은 IP 주소와 함께 서브넷 마스크의 길이를 나타내는 방법입니다. 예: `192.168.1.0/24`\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Load Balancer의 역할과 종류에 대해 설명해주세요.</summary>\n<div markdown=\"1\">\n\n- 로드 밸런서는 네트워크 트래픽을 여러 서버 간에 분산시키는 역할을 합니다. 주요 종류로는 L44(TCP/UDP 기반) 및 L7(HTTP/HTTPS) 로드 밸런서가 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>IPv4와 IPv6의 주요 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- IPv4는 32비트 주소 체계를 사용하며, IPv6는 128비트 주소 체계를 사용합니다. IPv6는 주소 공간이 훨씬 크므로 더 많은 장치에 IP 주소를 할당할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n<details>\n<summary>네트워크에서의 라우팅이란 무엇이며, 정적 라우팅과 동적 라우팅의 차이는 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 라우팅은 데이터 패킷이 소스에서 목적지까지 어떤 경로를 통해 이동할지 결정하는 과정입니다. 정적 라우팅은 수동으로 경로를 설정하는 반면, 동적 라우팅은 라우팅 프로토콜을 사용하여 경로를 자동으로 결정합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>DNS(Domain Name System)의 역할은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- DNS는 도메인 이름을 IP 주소로 변환하거나 IP 주소를 도메인 이름으로 변환하는 시스템입니다. 이를 통해 사용자는 IP 주소가 아닌 친숙한 도메인 이름을 사용하여 웹사이트에 접속할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>네트워크에서 패킷 흐름을 모니터링하려면 어떤 도구를 사용하나요?</summary>\n<div markdown=\"1\">\n\n- 패킷 캡처 및 분석 도구인 Wireshark, tcpdump 등을 사용하여 네트워크의 패킷 흐름을 모니터링하고 분석할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n# 3. AppDev\n\n<details>\n<summary>MVC 패턴이란 무엇이며, 왜 사용하나요?</summary>\n<div markdown=\"1\">\n\n- MVC는 Model, View, Controller의 약자로, 애플리케이션의 구조를 세 부분으로 나눈 설계 패턴입니다. 이 패턴을 사용하면 애플리케이션의 관심사를 분리하여 유지보수와 확장성을 향상시킬 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>RESTful API란 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- RESTful API는 웹의 표준을 기반으로 데이터를 주고받은 설계 패러다임입니다. 이를 통해 자원에 URL을 부여하고 CRUD 연산을 HTTP 메서드(GET, POST, PUT, DELETE 등)로 수행합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>모바일 앱과 웹 앱의 주요 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 모바일 앱은 특정 OS에 최적화되어 설치되며, 웹 앱은 브라우저를 통해 접근합니다. 모바일 앱은 일반적으로 웹 앱보다 더 빠르고 기능이 풍부하나, 업데이터나 배포가 복잡할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>앱의 성능을 향상시키기 위한 방법은 무엇이 있을까요?</summary>\n<div markdown=\"1\">\n\n- 코드 최적화, 캐싱, 데이터베이스 질의 최적화, CDN 사용, 압축 알고리즘 사용 등이 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Dependency Injection이란 무엇이며, 왜 사용하나요?</summary>\n<div markdown=\"1\">\n\n- Dependency Injection은 객체의 의존성을 외부에서 주입받는 패턴입니다. 이를 사용하면 코드의 유연성과 테스트 용이성을 향상시킬 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Progressive Web Apps(PWA)의 장점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- PWA는 웹 앱을 모바일 앱처럼 사용할 수 있게 만듭니다. 오프라인 작동, 홈 화면 추가, 푸시 알림 등의 기능을 제공하며, 별도의 설치 없이 접근 가능합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Single Page Application(SPA)이란 무엇이며, 어떤 장단점이 있나요?</summary>\n<div markdown=\"1\">\n\n- SPA는 하나의 웹 페이지에서 모든 사용자 인터페이스를 제공하는 애플리케이션입니다. 페이지 리로드 없이 데이터를 동적으로 업데이트합니다. 장점으로는 빠른 사용자 경험이 있고, 단점으로는 초기 로딩 시간이 길 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>앱 개발에서 메모리 누수(memory leak)의 원인과 해결 방법은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 메모리 누수는 더 이상 필요 없는 객체에 대한 참조가 제거되지 않아 메모리 해제가 안 되는 현상입니다. 정기적인 코드 리뷰, 프로파일러 사용, 참조 제거 등으로 해결할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>앱의 반응성을 향상시키기 위한 방법은 무엇이 있을까요?</summary>\n<div markdown=\"1\">\n\n- 비동기 프로그래밍, 캐싱, 데이터 로딩 최적화, UI 업데이트 최적화 등을 통해 반응성을 향상시킬 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>앱 개발에서 Unit Testing의 중요성은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- Unit Testing은 개별 코드 단위의 기능을 테스트합니다. 이를 통해 코드의 품질을 보장하고, 변경 시 발생할 수 있는 문제점을 빠르게 파악하며, 리팩토링과 기능 추가를 안전하게 수행할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>3 tier 웹 애플리케이션의 정의와 구성방식은 어떤 것이 있을까요?</summary>\n<div markdown=\"1\">\n\n- 3 tier 웹 애플리케이션은 프리젠테이션(Presentation), 로직(Business Logic), 데이터(Data)의 세 가지 계층으로 구성된 애플리케이션 아키텍처를 의미합니다.\n    - 프리젠테이션 계층: 사용자 인터페이스(UI)와 관련된 부분입니다.\n    - 로직 계층: 비즈니스 로직, 데이터 처리 및 API 서비스 등이 포함됩니다.\n    - 데이터 계층: 데이터베이스와의 상호작용 및 데이터 저장과 관련된 부분입니다.\n</div>\n</details>\n\n<br />\n\n\n# 4. Security\n\n<details>\n<summary>암호화와 해시의 주요 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 암호화는 데이터를 원래의 형태로 복호화(복원)할 수 있는 방식으로 변환하는 것입니다. 해시는 데이터를 고정된 길이의 값으로 변환하며, 이 과정은 일방향적이므로 복원이 불가능합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>DDoS 공격이란 무엇이며 어떻게 대비해야 하나요?</summary>\n<div markdown=\"1\">\n\n- DDoS(Distributed Denial Of Service)는 여러 곳에서 대량의 트래픽을 발생시켜 서비스를 중단시키는 공격입니다. 대비방안으로 트래픽 모니터링, 웹 방화벽, CDN 사용 등이 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>OWASP Top 10이란 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- OWASP Top 10은 웹 애플리케이션 보안에 대한 취약점 목록 중 가장 위험한 10가지를 나열한 것입니다. 이를 통해 개발자와 기업들이 보안 위협에 대비할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>인증(Authentication)과 인가(Authorization)의 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 인증은 사용자의 신원을 확인하는 과정입니다. 인가는 해당 사용자에게 주어진 권한을 검증하는 과정입니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Man-in-the-Middle 공격이란 무엇이며, 어떻게 방어할 수 있나요?</summary>\n<div markdown=\"1\">\n\n- Man-in-the-Middle 공격은 통신하는 두 당사자 사이에 중간에서 데이터를 가로채는 공격입니다. SSL/TLS 인증서를 사용하여 통신을 암호화하는 것으로 방어할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>SQL Injection이란 무엇이며 어떻게 예방할 수 있나요?</summary>\n<div markdown=\"1\">\n\n- SQL Injection은 악의적인 SQL 코드가 데이터베이스에 직접 입력되어 실행되게 하는 공격입니다. 입력 값을 검증하거나, Prepared Statements를 사용하여 예방할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Zero-Day 공격이란 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- Zero-Day 공격은 아직 발견되지 않은 취약점을 대상으로 하는 공격입니다. 이는 보안 패치가 나오기 전까지 알려지지 않습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>2-Factor Authentication(2FA)의 장점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 2FA는 두 가지 인증 방법을 결합하여 보안을 강화합니다. 이를 통해 단순한 패스워드 유출만으로는 시스템에 접근하기 어려워집니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>HTTPS의 기능과 중요성에 대해 설명해주세요.</summary>\n<div markdown=\"1\">\n\n- HTTPS는 HTTP에 보안 계층을 추가한 것입니다. SSL/TLS를 사용하여 데이터를 암호화합니다. 이를 통해 중간자 공격과 데이터 탈취를 방지합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>보안 인증서(Certificates)와 키(Key)의 역할은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 보안 인증서는 기관의 신원을 증명하고 공개 키를 포함하며, 키는 암호화와 복호화 과정에서 사용됩니다. 개인 키는 비밀로 유지되며, 공개 키는 인증서와 함께 공개됩니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>웹방화벽이 일반 방화벽과 어떻게 다른가요?</summary>\n<div markdown=\"1\">\n\n- 웹방화벽(WAF, Web Application Firewall)은 주로 HTTP 트래픽을 모니터링하며 웹 애플리케이션의 보안을 강화하기 위해 설계되었습니다. 이는 SQL 인젝션, 크로스 사이트 스크립팅(XSS)과 같은 고유의 웹 위협을 탐지하고 차단합니다. 반면 일반 방화벽은 네트워크 트래픽을 모니터링하여 악성 트래픽을 차단하며, 주로 포트와 IP 주소를 기반으로 작동합니다.\n\n</div>\n</details>\n\n<br />\n\n# 5. Database & Analytics\n\n<details>\n<summary>RDBMS와 NoSQL 데이터베이스의 주요 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- RDBMS는 정규화된 구조와 고정된 스키마를 가지며, ACID 트랜잭션을 지원합니다. 반면, NoSQL은 스키마가 유연하고, 대규모 분산 데이터를 처리하는 데 적합합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>데이터 웨어하우스와 데이터 레이크의 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 데이터 웨어하우스는 구조화된 데이터를 위한 중앙 집중식 저장소입니다. 데이터 레이크는 구조화 및 비구조화 데이터 모두를 원시 형식으로 저장하는 대규모 저장소입니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>OLAP와 OLTP의 주요 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- OLTP는 온라인 트랜잭션 처리를 위한 시스템으로, 실시간 데이터 입력, 수정, 검색을 지원합니다. OLAP는 온라인 분석 처리를 위한 시스템으로, 복잡한 질의와 데이터 분석에 최적화되어 있습니다. (cf. [[dev.cs.database.oltp-vs-olap]])\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>데이터 정규화의 주요 목적은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 데이터 중복성을 최소화하고, 데이터의 무결성을 향상시키며, 데이터베이스의 구조적 효율성을 증가시키는 것입니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Sharding이란 무엇이며, 어떤 장점이 있나요?</summary>\n<div markdown=\"1\">\n\n- Sharding은 데이터를 여러 데이터베이스나 서버에 분할 저장하는 전략입니다. 이를 통해 읽기/쓰기 성능을 향상시키고, 확장성을 증가시킬 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>캐싱의 목적과 사용 시 주의할 점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 캐싱은 자주 접근되는 데이터나 연산 결과를 빠르게 재사용하기 위한 기법입니다. 단, 캐싱된 데이터의 만료와 일관성 관리에 주의해야 합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>비정규화(De-normalization)의 목적과 장단점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 비정규화는 읽기 성능을 향상시키기 위해 데이터의 중복성을 허용하는 것입니다. 장점은 쿼리 성능 향상이며, 단점은 데이터 관리의 복잡성 증가입니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>ETL 과정이란 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- ETL은 '추출(Extract)', '변환(Tranform)', '로드(Load)'의 약자로, 데이터 웨어하우스에 데이터를 가져오고, 가공하고, 저장하는 과정을 의미합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>데이터 마이닝이란 무엇이며, 어떤 응용 분야에서 사용되나요?</summary>\n<div markdown=\"1\">\n\n- 데이터 마이닝은 대규모 데이터에서 의미 있는 패턴이나 관계를 발견하기 위한 기법입니다. 시장 분석, 고객 세그멘테이션, 추천 시스템 등에서 사용됩니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>인덱싱이란 무엇이며, 어떻게 데이터베이스에 성능을 향상시키나요?</summary>\n<div markdown=\"1\">\n\n- 인덱싱은 데이터베이스에서 데이터를 빠르게 조회하기 위한 자료구조입니다. 인덱스를 사용하면 특정 데이터를 빠르게 찾아 접근할 수 있어 쿼리 성능이 향상됩니다.\n\n</div>\n</details>\n\n</br>\n\n<details>\n<summary>Hadoop은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- Hadoop은 대규모 데이터 세트를 처리하기 위한 오픈소스 프레임워크입니다. 분산 저장 시스템인 HDFS(Hadoop Distributed File System)와 맵리듀스(MapReduce)라는 프로그래밍 모델을 기반으로 하며, 대용량 데이터를 여러 개의 머신에 분산하여 저장하고 처리하는 데 효과적입니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>열 지향(Column-oriented) 데이터베이스와 행 지향(Row-oriented) 데이터베이스의 주요 차이점은 무엇이며, 어떤 상황에서 열 지향 방식이 유리한가요?</summary>\n<div markdown=\"1\">\n\n열 지향 데이터베이스는 데이터를 열 기반으로 저장하며, 행 지향 데이터베이스는 데이터를 행 기반으로 저장합니다.\n\n- 열 지향 데이터베이스의 장점:\n    - 데이터 압축: 같은 데이터 타입의 값들이 연속적으로 저장되기 때문에 압축 효율이 더 좋습니다.\n    - 집계 쿼리 성능: 특정 열(들)만을 대상으로 하는 집계나 연산이 필요할 때, 행 전체를 읽는 것보다 훨씬 빠르게 동작합니다.\n- 행 지향 데이터베이스의 장점:\n    - 트랜잭션 처리 성능: 단일 레코드의 CRUD 작업이 빠르며, 특히 레코드 추가나 수정에 유리합니다.\n\n열 지향 방식은 대용량 데이터에서 특정 열에 대한 집계나 분석 작업이 주로 필요한 OLAP(Online Analytical Processing) 같은 분석 쿼리에 유리하며, 행 지향 방식은 OLTP(Online Transaction Processing) 시스템에서 더 흔히 사용됩니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Columnar storage의 주요 장점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 데이터 압축: 동일한 데이터 타입의 값들이 같은 열에 연속적으로 저장되므로 더 효율적인 압축이 가능합니다.\n- 쿼리 성능 향상: 쿼리는 필요한 열만 읽을 수 있으므로 I/O 작업이 줄어들고, 이는 집계 및 분석 쿼리의 성능을 크게 향상시킵니다.\n- 스캔 속도: 특정 열의 데이터만 읽어들이기 때문에 전체 행을 읽는 것보다 스캔 속도가 빠릅니다.\n- 비용 효율성: 더 나은 압축률과 I/O 최적화로 인해 저장 공간 및 쿼리 비용이 절약됩니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>데이터 웨어하우스에서 Star Schema와 Snowflake Schema의 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- Star Schema와 Snowflake Schema는 두 가지 주요 데이터 웨어하우스 스키마 설계 방식입니다.\n- Star Schema:\n    - 중심에 큰 \"fact table\"이 위치하고, 이 테이블에 직접 연결된 차원 테이블(dimension tables)로 구성됩니다.\n    - 차원 테이블은 정규화되지 않은 형태입니다.\n    - 쿼리 성능이 뛰어나며, 직관적이어서 이해하기 쉽습니다.\n- Snowflake Schema:\n    - Star Schema의 확장 형태로, 차원 테이블이 추가적으로 분해되어 정규화된 형태로 저장됩니다.\n    - 데이터 중복성이 줄어들고, 저장 공간을 더 효율적으로 사용할 수 있습니다.\n    - 그러나 복잡한 조인으로 인해 쿼리 성능에 영향을 줄 수 있습니다.\n- 따라서 Star Schema는 간단하고 빠른 성능을 원할 때, Snowflake Schema는 저장 공간의 효율성과 데이터의 일관성을 원할 때 선택될 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>ETL과 ELT 프로세스의 주요 차이점을 설명하고, 각 프로세스의 장점 및 적절한 사용 시나리오에 대해 설명해 주십시오.</summary>\n<div markdown=\"1\">\n\n- ETL(Extract, Transform, Load):\n    - 정의: ETL은 데이터를 원본 소스에서 추출하고, 변환 프로세스를 거쳐 원하는 형식으로 만든 후, 목표 데이터 웨어하우스에 로드하는 과정을 의미합니다.\n    - 장점:\n        - 미리 변환된 데이터를 데이터 웨어하우스에 로드하기 때문에, 일단 데이터가 로드되면 쿼리 성능이 빠릅니다.\n        - 원본 데이터의 품질이나 일관성에 문제가 있을 경우, 변환 단계에서 해당 문제를 해결할 수 있습니다.\n        - 전통적인 관계형 데이터베이스 시스템에 잘 맞습니다.\n    - 적절한 사용 시나리오: 구조화된 데이터를 가진 전통적인 데이터 웨어하우스 환경에서의 데이터 처리와 통합.\n- ELT(Extract, Load, Transform):\n    - 정의: ELT는 데이터를 원본 소스에서 추출한 후, 먼저 목표 데이터 웨어하우스에 로드하고, 데이터 웨어하우스 내에서 필요한 변환 작업을 수행하는 과정을 의미합니다.\n    - 장점:\n        - 현대의 데이터 웨어하우스 솔루션은 대용량의 데이터 변환 작업을 빠르게 처리할 수 있으므로 ELT가 가능합니다.\n        - 데이터 로딩 후 변환을 수행하기 때문에, 다양한 형식의 데이터를 빠르게 데이터 웨어하우스에 가져올 수 있습니다.\n        - 데이터 웨어하우스에서 직접 변환을 수행하기 때문에, 변환 로직을 쉽게 변경하거나 업데이트할 수 있습니다.\n    - 적절한 사용 시나리오: 빅데이터 및 클라우드 기반 데이터 웨어하우스 환경, 데이터 웨어하우스 내에서 다양한 형식의 데이터를 동적으로 변환 및 집계해야 할 때.\n- 요약하면, ETL은 데이터 변환의 복잡성을 처리하기 위해 중앙에서 데이터를 변환하는 반면, ELT는 데이터 웨어하우스의 스케일 및 처리 능력을 활용하여 변환을 수행합니다.\n</div>\n</details>\n\n<br />\n\n# 6. Machine Learning & Artificial Intelligence\n\n<details>\n<summary>머신러닝과 딥러닝의 주요 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 머신러닝은 알고리즘을 사용하여 데이터에서 패턴을 학습하는 방법론입니다. 딥러닝은 머신러닝의 하위 집합으로, 심층 신경망(Deep Neural Networks)을 활용하여 학습하는 방식을 의미합니다.\n\n</div>\n</details>\n\n<br />\n\n\n<details>\n<summary>과적합(Overfitting)이란 무엇이며, 어떻게 방지할 수 있나요?</summary>\n<div markdown=\"1\">\n\n- 과적합은 모델이 훈련 데이터에 너무 잘 맞아서 실제 데이터에서는 일반화되지 않는 현상을 의미합니다. 교차 검증, 드롭아웃, 정규화 등의 방법으로 과적합을 방지할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n\n<details>\n<summary>회귀와 분류의 주요 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 회귀는 연속적인 값을 예측하는 반면, 분류는 주어진 카테고리 중 하나로 데이터를 구분하는 작업입니다.\n\n</div>\n</details>\n\n<br />\n\n\n<details>\n<summary>손실 함수(Loss Function)의 목적은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 손실 함수는 모델의 예측값과 실제값 사이의 차이를 측정하는 함수입니다. 이 차이를 최소화하도록 모델을 학습시킵니다.\n\n</div>\n</details>\n\n<br />\n\n\n<details>\n<summary>랜덤 포레스트(Random Forest) 알고리즘의 주요 특징은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 랜덤 포레스트는 여러 개의 의사결정트리를 조합하여 작동하는 앙상블 기법입니다. 이는 과적합을 방지하고 예측의 정확도를 향상시킵니다.\n\n</div>\n</details>\n\n<br />\n\n\n<details>\n<summary>Gradient Descent란 무엇이며, 어떻게 작동하나요?</summary>\n<div markdown=\"1\">\n\n- Gradient Descent는 손실 함수의 기울기를 사용하여 최소값을 찾는 최적화 알고리즘입니다. 기울기의 반대 방향으로 파라미터를 업데이트하여 손실을 점차 줄여나갑니다.\n\n</div>\n</details>\n\n<br />\n\n\n<details>\n<summary>특성 추출(Feature Extraction)과 특성 선택(Feature Selection)의 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 특성 선택은 중요한 특성을 선택하고 불필요한 특성을 제거하는 과정입니다. 특성 추출은 원래의 특성을 사용하여 새로운 특성을 생성하는 과정입니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>정밀도(Precision)과 재현율(Recall)의 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 정밀도는 모델이 True라고 예측한 것 중 실제로 True인 것의 비율입니다. 재현율은 실제 True 중에서 모델이 True라고 예측한 것의 비율입니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Transfer Learning의 주요 장점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- Transfer Learning은 사전에 학습된 모델의 지식을 새로운 작업에 적용하는 방식입니다. 이를 통해 적은 양의 데이터로도 빠르고 효과적인 학습이 가능합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>심층 신경망에서 활성화 함수(Activation Function)의 역할은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 활성화 함수는 신경망의 각 뉴런의 출력값을 결정합니다. 이는 비선형성을 추가하여 모델의 표현력을 높여줍니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>특성 공학(Feature Engineering)의 중요성은 무엇이며, 효과적인 특성 공학을 위한 주요 전략은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 특성 공학은 머신러닝에서 데이터를 모델에 입력하기 전에 데이터의 원시 특성을 변환하거나 새로운 특성을 생성하는 과정입니다. 올바른 특성을 선택하고 구성하는 것은 모델의 성능에 결정적인 영향을 미칠 수 있습니다.\n\n- 특성 공학의 중요성:\n    1. 성능 향상: 적절한 특성을 사용하면 모델의 학습 능력과 예측 성능이 향상될 수 있습니다.\n    2. 학습 시간 단축: 불필요한 특성을 제거하거나 데이터의 차원을 줄이면 모델의 학습 시간을 줄일 수 있습니다.\n    3. 이해력 향상: 중요한 특성을 파악하면 문제 도메인에 대한 통찰력을 얻을 수 있으며, 모델의 예측을 더 잘 이해할 수 있습니다.\n- 효과적인 특성 공학을 위한 주요 전략:\n    1. 도메인 지식 활용: 문제 도메인에 대한 지식을 활용하여 데이터에 숨겨진 패턴이나 관계를 반영하는 특성을 생성할 수 있습니다.\n    2. 다양한 변환 시도: 로그 변환, 폴리노미얼 특성, 상호 작용 특성 등 다양한 수학적 변환을 시도해보는 것이 좋습니다.\n    3. 특성 스케일링: 모든 특성이 동일한 스케일에 있도록 조정합니다. 예를 들면, 정규화(Normalization) 또는 표준화(Standardization)를 사용할 수 있습니다.\n    4. 특성 선택: 중요한 특성만을 선택하여 차원의 저주를 피하고 과적합을 방지할 수 있습니다.\n    5. 자동 특성 생성: 딥러닝과 같은 방법을 사용하여 데이터로부터 자동으로 특성을 추출할 수 있습니다.\n- 결론적으로, 특성 공학은 모델의 성능을 최적화하는 데 중요한 역할을 합니다. 적절한 특성을 선택하고 구성하는 방법은 해당 문제와 사용되는 데이터에 따라 달라집니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>편향(Bias)과 분산(Variance)의 차이점은 무엇이며, 이들이 모델 성능에 미치는 영향에 대해 설명해 주십시오.</summary>\n<div markdown=\"1\">\n\n- 편향과 분산은 머신 러닝에서 모델의 성능과 오버피팅(overfitting) 혹은 언더피팅(underfitting) 문제를 이해하는 데 중요한 개념입니다.\n\n- 편향 (Bias):\n    - 편향은 모델의 예측값과 실제 값 사이의 평균적인 차이를 나타냅니다.\n    - 높은 편향은 모델이 너무 단순하다는 것을 의미하며, 데이터의 복잡성을 충분히 잡아내지 못할 때 발생합니다. 이를 언더피팅이라고 합니다.\n- 분산 (Variance):\n    - 분산은 모델이 학습 데이터의 작은 변동에 얼마나 민감하게 반응하는지를 나타냅니다.\n    - 높은 분산은 모델이 학습 데이터에 과도하게 적합하게 되어 새로운 데이터나 테스트 데이터에 대한 성능이 떨어지는 경우를 의미합니다. 이를 오버피팅이라고 합니다.\n- 모델 성능에 미치는 영향:\n    - 편향이 높고 분산이 낮은 경우: 모델이 너무 단순하여 데이터의 복잡성을 포착하지 못하는 상황을 나타냅니다. 이 경우, 더 복잡한 모델을 사용하거나, 추가적인 특성(feature)을 포함시키는 것으로 개선할 수 있습니다.\n    - 편향이 낮고 분산이 높은 경우: 모델이 학습 데이터에 너무 잘 적합되어 있어, 새로운 데이터에 대한 성능이 떨어지는 상황을 나타냅니다. 정규화(regularization) 기법을 사용하거나, 데이터를 더 많이 모으는 것으로 개선할 수 있습니다.\n- 보통 모델의 복잡도가 증가하면 분산은 증가하고 편향은 감소하는 경향이 있습니다. 따라서, 머신 러닝에서는 이 두 가지 문제 사이의 균형을 잘 찾는 것이 중요합니다. 이를 **편향-분산 트레이드오프(Bias-Variance Tradeoff)**라고 합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>정규화(Regularization)란 무엇이며, 이를 사용하는 이유는 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 정규화(Regularization)는 머신 러닝 모델의 학습 과정에 추가되는 페널티 항목으로, 모델의 가중치가 너무 큰 값을 갖지 않도록 제한하는 기법을 의미합니다. 이는 모델의 과적합(overfitting)을 방지하는데 도움을 줍니다.\n\n- 정규화의 주요 목적과 이유는 다음과 같습니다:\n\n    1. 과적합 방지: 모델이 학습 데이터에 너무 정확하게 적합되어 새로운 데이터에 대한 일반화 성능이 떨어지는 상황, 즉 과적합을 방지하기 위해 사용됩니다.\n\n    2. 모델 복잡도 제한: 정규화는 모델의 가중치에 대한 제약을 추가함으로써 모델의 복잡도를 제한합니다. 이로 인해 모델이 학습 데이터의 노이즈나 불필요한 패턴에 적합하는 것을 방지할 수 있습니다.\n\n    3. 특성 선택: 일부 정규화 기법은 불필요한 특성의 가중치를 0으로 만들어 모델에서 해당 특성을 사용하지 않게 할 수 있습니다. 이를 통해 더 간단하고 해석하기 쉬운 모델을 얻을 수 있습니다.\n\n- 대표적인 정규화 기법에는 L1 정규화 (Lasso)와 L2 정규화 (Ridge)가 있습니다. L1은 특성의 수를 줄이는 효과가 있어 특성 선택에 유용하며, L2는 가중치의 제곱항을 페널티로 사용하여 모든 특성에 일관된 제약을 가합니다.\n\n</div>\n</details>\n\n<br />\n\n\n<details>\n<summary>어텐션(Attention) 메커니즘의 역할은 무엇이며, 자연어 처리에 있어서의 그 중요성에 대해 설명해 주십시오.</summary>\n<div markdown=\"1\">\n\n- 어텐션 메커니즘은 딥러닝 모델, 특히 순환 신경망(RNN)과 트랜스포머(Transformer) 아키텍처에서 중요한 역할을 하는 기술입니다. 기본적으로 어텐션 메커니즘은 입력 데이터의 다양한 부분에 가중치를 부여함으로써 모델이 중요한 정보에 집중할 수 있게 도와줍니다.\n\n- 어텐션의 핵심 역할:\n    1. 가중치 부여: 입력 시퀀스의 각 요소에 대한 가중치를 계산하여, 모델이 어떤 부분에 더 집중해야 하는지 결정합니다.\n    2. 중요한 정보 선택: 모든 입력 정보가 동등하게 중요하지 않기 때문에, 어텐션은 중요한 정보에 대해 높은 가중치를 부여합니다.\n\n- 자연어 처리에서의 중요성:\n\n    1. 시퀀스-투-시퀀스 모델링 향상: 예를 들어, 번역에서 소스 문장의 특정 단어나 구에 대한 타겟 문장의 단어 간의 관계를 명확하게 학습할 수 있습니다.\n    2. 긴 의존성 학습: 전통적인 RNN은 긴 시퀀스에서의 의존성을 잘 학습하는 데 어려움이 있습니다. 어텐션 메커니즘을 통해 이러한 문제를 완화할 수 있습니다.\n    3. 인터프리터빌리티 향상: 어텐션 가중치를 시각화하면 모델이 특정 출력을 생성하기 위해 어떤 입력 부분에 집중하는지 이해하기 쉽습니다.\n\n- 결론적으로, 어텐션 메커니즘은 딥러닝 모델이 입력 데이터의 중요한 부분에 집중하도록 도와주며, 특히 자연어 처리 태스크에서 성능 향상과 모델의 해석 가능성을 높이는 데 중요한 역할을 합니다.\n\n</div>\n</details>\n\n<br />\n\n\n\n# 7. Virtualization\n\n<details>\n<summary>가상화(Virtualization)란 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 가상화는 물리적 자원을 여러 개의 가상 환경으로 분리하는 기술입니다. 이를 통해 하나의 물리적 하드웨어 위에서 여러 운영체제나 애플리케이션을 동시에 실행할 수 있게 됩니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>하이퍼바이저(Hypervisor)란 무엇이며, 그 종류는 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 하이퍼바이저는 가상화 환경에서 여러 가상 머신(VM)을 실행하고 관리하는 소프트웨어 또는 하드웨어입니다. 주로 Type 1(베어 메탈) 하이퍼바이저와 Type 2(호스트 기반) 하이퍼바이저로 분류됩니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>가상화의 주요 이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 자원 최적화, 높은 유연성, 개발 및 테스팅 편의성, 재해 복구 용이성, 서버의 운용 및 유지 비용 절감 등이 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>오버커밋(Overcommit)이란 무엇이며, 왜 주의해야 하나요?</summary>\n<div markdown=\"1\">\n\n- 오버커밋은 물리적으로 사용 가능한 자원보다 더 많은 가상 자원을 할당하는 것을 의미힙니다. 과도한 오버커밋은 성능 저하 및 가상 머신 간의 자원 경쟁을 초래할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>컨테이너와 VM(Virtual Machine)의 주요 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- VM은 전체 운영체제를 포함하므로 무거운 반면, 컨테이너는 OS 커널을 공유하므로 경량화되어 있습니다. 따라서 컨테이너는 VM보다 빠르게 시작되고, 더 적은 자원을 사용합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>가상 머신의 스냅샷(Snapshot) 기능의 장점과 단점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 스냅샷은 특정 시점의 가상 머신 상태를 캡처합니다. 이를 통해 실패나 문제 발생 시 빠르게 원래 상태로 복원할 수 있습니다. 단, 너무 많은 스냅샷이 누적되면 성능 저하와 스토리지 사용량 증가의 문제가 발생할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>서버 가상화와 네트워크 가상화의 주요 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 서버 가상화는 하드웨어 서버 자원을 가상 서버로 분리하는 것이며, 네트워크 가상화는 물리적 네트워크 자원을 여러 가상 네트워크로 분리하는 것입니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>가상화에서 \"파티셔닝(Partitioning)\"과 \"아이솔레이션(Isolation)\"의 의미는 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 파티셔닝은 하나의 물리적 자원을 여러 개의 독립된 부분(파티션)으로 나누는 것을 의미하며, 아이솔레이션은 각 파티션끼리의 상호 작용을 제한하여 독립성을 보장하는 것을 의미합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>가상 스위치(Virtual Switch)란 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 가상 스위치는 가상화 환경 내에서 가상 머신 간의 통신을 관리하는 네트워킹 컴포넌트입니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Paravirtualization이란 무엇이며, 이 방식의 장점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- Paravirtualization은 전체 하드웨어를 시뮬레이션하는 대신, 운영체제가 직접 하이퍼바이저와 상호 작용하도록 수정된 가상화 방식입니다. 이 방식은 성능 향상 및 오버헤드 감소의 장점이 있습니다.\n\n</div>\n</details>\n\n<br />\n\n# 8. Microservices Architecture\n<br />\n\n<details>\n<summary>마이크로서비스 아키텍처란 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- [[dev.architecture.microservices]] 아키텍처는 하나의 큰 애플리케이션을 작고, 독립적인 서비스로 분리하는 설계 패러다임입니다. 각 서비스는 독립적으로 배포 및 확장이 가능하며, 특정 비즈니스 기능을 수행합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>모놀리식 아키텍처와 마이크로서비스의 주요 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 모놀리식 아키텍처는 애플리케이션의 모든 구성 요소가 하나의 코드 베이스와 실행 단위로 통합된 구조입니다. 반면, 마이크로서비스는 애플리케이션을 여러 독립적인 서비스로 분리합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>마이크로서비스 아키텍처의 주요 이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 확장성, 빠른 배포와 반복, 서비스 독립성, 기술 스택 유연성, 장애 격리 등이 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>마이크로서비스에서는 서비스 간의 통신을 어떻게 처리하나요?</summary>\n<div markdown=\"1\">\n\n- 주로 HTTP/REST, gRPC, 메시징 큐 등의 방식을 사용하여 서비스 간 통신을 처리합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>서비스 디스커버리(Service Discovery)란 무엇이며, 왜 중요한가요?</summary>\n<div markdown=\"1\">\n\n- 서비스 디스커버리는 마이크로서비스 환경에서 서비스의 위치와 메타데이터를 동적으로 찾고 관리하는 과정입니다. 서비스의 확장성과 동적 환경 대응을 위해 필요합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>마이크로서비스에서는 데이터 관리에 어떤 접근 방식을 취하나요?</summary>\n<div markdown=\"1\">\n\n- 마이크로서비스에서는 각 서비스가 자신의 데이터베이스를 갖는 '데이터베이스 퍼 서비스' 패턴을 추천합니다. 이를 통해 서비스의 독립성과 확장성을 보장합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>API Gateway의 역할은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- API Gateway는 클라이언트와 마이크로서비스 간의 중간 계층으로, 요청/응답의 라우팅, 인증, 인가, 로드 밸런싱, 캐싱, 요율 제한 및 모니터링 등의 기능을 제공합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Circuit Breaker 패턴의 목적은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- Circuit Breaker는 연속된 실패 후 일정 시간 동안 서비스 호출을 중단하여 시스템의 과부하를 방지하고, 복구 시간을 확보하는 패턴입니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>마이크로서비스에서의 로깅와 모니터링의 중요성에 대해 설명하십시오.</summary>\n<div markdown=\"1\">\n\n- 분산된 여러 서비스에서 발생하는 문제를 신속하게 진단하고 해결하기 위해 통합된 로깅과 모니터링이 필수적입니다. 이는 서비스의 건강 상태를 파악하고, 장애 원인을 추적하는데 중요한 역할을 합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>마이크로서비스 아키텍처의 주요 단점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 데이터 일관성 유지의 어려움, 서비스 간의 복잡한 통신, 분산된 시스템의 복잡성, 트랜잭션 관리의 어려움 등이 있습니다.\n\n</div>\n</details>\n\n<br />\n\n# 9. Containerization\n\n<details>\n<summary>컨테이너(Container)란 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 컨테이너는 애플리케이션과 그에 필요한 모든 종속성 및 설정을 하나의 패키지로 캡슐화한 것입니다. 이를 통해 애플리케이션의 일관성 있는 실행 환경을 보장합니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Docker과 Kubernetes의 주요 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- Docker는 컨테이너화된 애플리케이션을 만들고 실행하기 위한 플랫폼입니다. Kubernetes는 여러 Docker 호스트에서 컨테이너 클러스터를 자동화, 스케일링 및 관리하는 오케스트레이션 시스템입니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>컨테이너 이미지와 컨테이너 인스턴스의 차이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 컨테이너 이미지는 애플리케이션과 그 종속성을 포함한 스냅샷이며, 컨테이너 인스턴스는 이 이미지를 기반으로 실행되는 실제 활성화된 환경입니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Docker Compose의 주요 역할은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- Docker Compose는 멀티-컨테이너 Docker 애플리케이션의 정의와 실행을 단순화하기 위한 도구입니다. `docker-compose.yml` 파일을 통해 서비스, 네트워크, 볼륨 등을 정의할 수 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>컨테이너화의 주요 이점은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 일관된 환경, 빠른 배포, 확장성, 격리성, 리소스 효율성 등이 컨테이너화의 주요 이점으로 꼽힙니다.\n\n</div>\n</details>\n\n<br />\n<details>\n<summary>컨테이너 런타임은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 컨테이너 런타임은 컨테이너의 실행과 관리를 담당하는 소프트웨어입니다. 예를 들면, Docker Daemon, containerd, runc 등이 있습니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>Kubernetes의 Pod란 무엇이며, 이를 사용하는 주된 이유는 무엇입니까?</summary>\n<div markdown=\"1\">\n\n- Pod는 Kubernetes에서 가장 작은 배포 단위로, 하나 이상의 컨테이너로 구성됩니다. Pod는 동일한 네트워크 네임스페이스 아래에서 실행되는 컨테이너의 그룹으로, 서로 밀접하게 연관된 컨테이너들을 함께 실행하기 위해 사용됩니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>컨테이너의 스토리지 볼륨을 관리하는 데 있어서 주요 고려사항은 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 데이터의 영속성, 볼륨의 생명 주기, I/O 성능, 백업 및 복원, 볼륨의 공유 여부 등이 주요 고려사항으로 꼽힙니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>컨테이너 네트워킹의 주요 도전과제는 무엇인가요?</summary>\n<div markdown=\"1\">\n\n- 서비스 발견, 로드 밸런싱, 네트워크 격리, 인그레스 및 이그레스 트래픽 관리, 네트워크 보안 등이 컨테이너 네트워킹의 주요 도전과제로 꼽힙니다.\n\n</div>\n</details>\n\n<br />\n\n<details>\n<summary>`Dockerfile`이란 무엇이며, 어떻게 사용하는가요?</summary>\n<div markdown=\"1\">\n\n- `Dockerfile`은 Docker 이미지를 빌드하기 위한 스크립트 파일입니다. 여기에는 이미지를 생성하기 위한 명령어와 설정이 포함되어 있으며, `docker build` 명령어로 Docker 이미지를 빌드할 때 사용됩니다.\n\n</div>\n</details>","n":0.016}}},{"i":165,"$":{"0":{"v":"Azure","n":1}}},{"i":166,"$":{"0":{"v":"Azure Message Services","n":0.577},"1":{"v":"\nAzure의 메시지 서비스를 비교해보고 목적에 맞는 서비스를 이용해보자.\n\n## Azure Event Grid\n- 이벤트-드리븐, 반응형 프로그래밍을 가능하게 하는 이벤트 백플레인 \n- 게시-구독(publish-subscribe) 모델을 사용 \n- 게시자는 이벤트를 내보내지만 이벤트가 처리되는 방식에 대한 기대치는 없음 \n- 구독자는 어떤 이벤트를 처리할지 결정 \n\n## Azure Event Hubs \n- 빅 데이터 스트리밍 플랫폼 및 이벤트 수집 서비스 \n- 초당 수백만 개의 이벤트를 수신하고 처리 가능 \n- 실시간 처리를 위한 신속한 데이터 검색과 저장된 원시 데이터의 반복 재생이 가능 \n\n## Azure Service Bus \n- 메시지 큐와 게시-구독 토픽이 있는 메시지 브로커\n- 트랜잭션, 순서 지정, 중복 검색 및 즉각적인 일관성이 필요한 엔터프라이즈 애플리케이션을 대상으로 함 \n- 손실되거나 중복될 수 없는 높은 가치의 메시지를 처리할 때 사용 \n- 소비 주체에서 메시지를 받을 준비가 될때가지 메시지를 브로커(큐)에 저장\n\n## Summary\n\n서비스 |용도 |Type |사용 시기\n----|-----|------|-----\nEvent Grid |반응형 프로그래밍 |이벤트 배포(불연속) |상태 변경에 대응 \nEvent Hubs | 빅 데이터 파이프라인  | 이벤트 스트리밍(시리즈) | 원격 분석 및 분산 데이터 스트리밍 \nService Bus | 높은 가치의 엔터프라이즈 메시지 | 메시지  | 주문 처리 및 금융 거래 \n\n## References\n- https://learn.microsoft.com/ko-kr/azure/event-grid/compare-messaging-services","n":0.078}}},{"i":167,"$":{"0":{"v":"Azure Event Grid","n":0.577},"1":{"v":"\n## Azure Event Grid\n\n- 서버리스 이벤트 브로커\n- 이벤트 기반 아키텍처를 사용하여 솔루션을 연결할 수 있음\n    - 이벤트를 사용하여 다른 애플리케이션이나 서비스와 같은 시스템 상태 변경 내용을 전달\n    - 필터를 사용하여 다른 엔드포인트에 대한 특정 이벤트를 라우팅하고, 여러 엔드포인트로 멀티캐스트하며, 이벤트가 안정적으로 배달될 수 있도록 할 수 있음\n- 이벤트 처리기\n    - Azure 함수\n    - Event Hubs\n    - Service Bus 큐 및 항목\n- 용어\n    - 이벤트: 발생한 내용\n    - 이벤트 원본 : 이벤트가 발생한 곳\n    - 토픽 : 게시자가 이번테를 보낸 엔드포인트\n    - 이벤트 구독 : \n    - 이벤트 처리기 : 이벤트에 반응하는 앱 또는 서비스","n":0.103}}},{"i":168,"$":{"0":{"v":"DevOps","n":1},"1":{"v":"\n## PAT\n\n- Personal Access Token\n- Azure DevOps에 인증을 위한 대체 암호\n- 보안 자격 증명이 포함됨\n    - 사용자, 사용자가 접근 가능한 조직, 접근 범위\n\n\n## Reference\n- https://learn.microsoft.com/ko-kr/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?toc=%2Fazure%2Fdevops%2Forganizations%2Ftoc.json&bc=%2Fazure%2Fdevops%2Forganizations%2Fbreadcrumb%2Ftoc.json&view=azure-devops&tabs=Windows\n","n":0.2}}},{"i":169,"$":{"0":{"v":"AWS","n":1}}},{"i":170,"$":{"0":{"v":"Certification","n":1}}},{"i":171,"$":{"0":{"v":"클라우드 AWS 자격증으로 시작하기","n":0.5},"1":{"v":"\n![](https://contents.kyobobook.co.kr/sih/fit-in/458x0/pdt/9788931558111.jpg)\n\n# 01. 클라우드 컴퓨팅과 AWS\n\n## 01. 클라우드 컴퓨팅 개요\n\n### 1. 클라우드 컴퓨팅\n- 클라우드 컴퓨팅은 인터넷을 사용해서 자원(서버, 네트워크, 스토리지)을 사용할 수 있는 서비스\n\n### 2. On-demand 서비스와 SLA\n- 주문형(On-demand) 서비스: 클라우드 컴퓨팅 사용자가 요청한 만큼 서비스를 제공하고 비용을 청구하는 모델\n- SLA(Service Level Agreement)\n    - on-demand 서비스를 사용할 때 클라우드 컴퓨팅 서비스 사용자와 서비스 제공자(예: AWS) 간의 서비스 수준에 대한 협약서\n    - SLA를 통해 사용자는 사용한 서비스만큼 비용을 지불\n\n### 3. 가상화(Virtualization)\n- 여러 개의 물리적 자원을 하나로 통합해서 관리하거나 하나의 물리적 자원을 여러 개로 분할하여 사용하는 기술\n- 종류\n    - 호스트 가상화\n        - Host OS[^1] 위에 guest OS[^2]가 실행되는 방식\n        - 예: VM Workstation, VMServer, Virtual Box\n        - 단점: Host OS 위에 다시 guest OS를 설치해서 사용하기 때문에 오버헤드가 큼\n    - 하이퍼바이저 가상화\n        - Host OS가 없고 하드웨어에 하이퍼바이저라는 가상화 소프트웨어를 설치하여 사용하는 방식\n        - 예: Xen, Microsoft Hyper-V, Citrix, KVM\n        - 장점: Host OS가 없기 때문에 오버헤드가 적음\n    - 컨테이너 가상화\n        - Host OS 위에 컨테이너 관리 소프트웨어를 설치하여 논리적인 컨테이너를 나누어서 사용\n        - 예: Docker\n        - 장점: 오버헤드가 적고 속도가 빠름\n\n## 02. 클라우드 컴퓨팅 모델\n\n### 1. On-Premise\n- 서버, 데이터베이스, 네트워크 장비 등을 모두 구매해서 구축하고 운영하는 서비스\n- 일반적으로 IDC(Internet Data Center)에 서버를 설치하고 전용 네트워크를 통해서 운영하는 시스템 형태\n### 2. 클라우드 컴퓨팅 종류\n- Private Cloud: 기업 내부에서 기업 내부 조직원들을 위한 서비스를 제공\n- Public Cloud: 인터넷을 사용해서 제공하는 클라우드 컴퓨팅 형태로 AWS, Azure 등이 있음\n- Hybrid Cloud: Private 및 Public Cloud를 모두 제공하는 형태\n### 3. 클라우드 컴퓨팅 모델\n- IaaS(Infrastructure as a Service)\n    - 인프라를 서비스 형태로 제공하는 것으로 서버, 스토리지, 네트워크 관련 각종 물리적 장비를 서비스 형태로 제공\n    - AWS에서는 EC2, S3, VPC 등의 서비스\n- PaaS(Platform as a Service)\n    - 인프라에 설치되는 운영체제, 미들웨어, 데이터베이스 관리 시스템 등의 소프트웨어를 제공\n    - AWS에서는 리눅스 및 윈도우 운영체제, Oracle, MySQL 등의 DBMS를 제공하는 것\n- SaaS(Software as a Service)\n    - 응용 프로그램을 서비스 형태로 제공하는 것\n    - 구글 Office 365, 드롭박스 등의 응용 프로그램\n\n## 03. AWS(Amazon Web Service)\n\n### 1. AWS(Amazon Web Service)\nAWS는 컴퓨팅, 네트워킹, 스토리지, 분석 플랫폼 등 다양한 서비스를 제공\n\n#### 컴퓨팅 서비스\n\n#### 네트워킹 서비스\n\n#### 스토리지 서비스\n\n#### 분석 플랫폼\n\n#### 기타\n\n### 2. AWS 장점\n\n### 3. AWS 공동 책임 모델 및 규정 준수 프로그램\nAWS 공동책임 모델이란 AWS와 클라우드 서비스 사용자(고객) 간에 책임을 정의한 모델\n\n#### AWS 책임\n- AWS 클라우드에서 제공되는 모든 서비스에 대해서 인프라를 보호할 책임\n- 여기서 인프라란 하드웨어, 소프트웨어, 네트워킹 및 시설을 의미\n\n#### 고객 책임\n- AWS 내에 저장 및 관리하는 고객 데이터 관리(암호화), 자산 분류, 적절한 허가를 위한 IAM 도구, AWS EC2 인스턴스에 설치한 모든 애플리케이션 등은 고객 책임\n\n### 4. AWS 사용 방법\n#### AWS 인터페이스 방법\n- AWS Management Console\n- CLI(Command Line Interface)\n- SDK(Software Development Kit)\n\n### 5. 리전과 가용 영역\n- 리전(Region): 전 세계에 있는 AWS의 데이터 센터의 물리적 위치\n- 가용 영역(AZ; Availability Zone): 논리적 데이터 센터의 그룹\n\n### 6. 요금 정책\n\n## References\n- [클라우드 AWS 자격증으로 시작하기](https://www.millie.co.kr/v3/bookDetail/179572928?referrer=searchRecent)\n\n[^1]: Host OS: 하드웨어 위에 설치된 운영체제\n[^2]: Guest OS: 호스트 가상화 혹은 하이퍼바이저 위에 설치된 운영체제","n":0.047}}},{"i":172,"$":{"0":{"v":"AWS Certified Solutions Architect - Associate (SAA-C03) 오답 노트","n":0.333},"1":{"v":"\n`AWS Certified Solutions Architect - Associate (SAA-C03) Exam Practice Questions` 오답 노트\n\n## Questions\n\n<hr>\n\n### Question #3\n\nA company uses AWS Organizations to manage multiple AWS accounts for different departments. The management account has an Amazon S3 bucket that contains project reports. The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations.\n\nWhich solution meets these requirements with the LEAST amount of operational overhead?\n\nA. Add the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.\n\nB. Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.\n\nC. Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly.\n\nD. Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #9\n\nA company is running an SMB file server in its data center. The file server stores large files that are accessed frequently for the first few days after the files are created. After 7 days the files are rarely accessed.\nThe total data size is increasing and is close to the company's total storage capacity. A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues.\n\nWhich solution will meet these requirements?\n\nA. Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.\n\nB. Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.\n\nC. Create an Amazon FSx for Windows File Server file system to extend the company's storage space.\n\nD. Install a utility on each user's computer to access Amazon S3. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #12\n\nA global company hosts its web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The web application has static data and dynamic data. The company stores its static data in an Amazon S3 bucket. The company wants to improve performance and reduce latency for the static data and dynamic data. The company is using its own domain name registered with Amazon Route 53.\n\nWhat should a solutions architect do to meet these requirements?\n\nA. Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins. Configure Route 53 to route traffic to the CloudFront distribution.\n\nB. Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint Configure Route 53 to route traffic to the CloudFront distribution.\n\nC. Create an Amazon CloudFront distribution that has the S3 bucket as an origin. Create an AWS Global Accelerator standard accelerator that has the ALB and the CloudFront distribution as endpoints. Create a custom domain name that points to the accelerator DNS name. Use the custom domain name as an endpoint for the web application.\n\nD. Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint. Create two domain names. Point one domain name to the CloudFront DNS name for dynamic content. Point the other domain name to the accelerator DNS name for static content. Use the domain names as endpoints for the web application.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #15\n\nA company recently migrated to AWS and wants to implement a solution to protect the traffic that flows in and out of the production VPC. The company had an inspection server in its on-premises data center. The inspection server performed specific operations such as traffic flow inspection and traffic filtering. The company wants to have the same functionalities in the AWS Cloud.\n\nWhich solution will meet these requirements?\n\nA. Use Amazon GuardDuty for traffic inspection and traffic filtering in the production VPC.\n\nB. Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering.\n\nC. Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for the production VPC.\n\nD. Use AWS Firewall Manager to create the required rules for traffic inspection and traffic filtering for the production VPC.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #16\n\nA company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations. The rest of the company should have only limited access.\n\nWhich solution will meet these requirements?\n\nA. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.\n\nB. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.\n\nC. Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.\n\nD. Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #19\n\nA company has a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC. The application servers and database servers are deployed in private subnets in the same VPC. The company has deployed a third-party virtual firewall appliance from AWS Marketplace in an inspection VPC. The appliance is configured with an IP interface that can accept IP packets.\nA solutions architect needs to integrate the web application with the appliance to inspect all traffic to the application before the traffic reaches the web server.\nWhich solution will meet these requirements with the LEAST operational overhead?\n\nA. Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.\n\nB. Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.\n\nC. Deploy a transit gateway in the inspection VPConfigure route tables to route the incoming packets through the transit gateway.\n\nD. Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance.\n\n<details>\n<summary>Show Answer</summary>\n\n> D\n\n</details>\n\n<br><hr>\n\n### Question #34\n\nA company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the company must track configuration changes on its AWS resources and record a history of API calls made to these resources.\n\nWhat should a solutions architect do to meet these requirements?\n\nA. Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.\n\nB. Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.\n\nC. Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.\n\nD. Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #37\n\nA company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The company needs to create a strategy to access and administer the instances remotely and securely. The company needs to implement a repeatable process that works with native AWS services and follows the AWS Well-Architected Framework.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n\nA. Use the EC2 serial console to directly access the terminal interface of each instance for administration.\n\nB. Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager Session Manager to establish a remote SSH session.\n\nC. Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion host in a public subnet to provide a tunnel for administration of each instance.\n\nD. Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises machines to connect directly to the instances by using SSH keys across the VPN tunnel.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n</details>\n\n<br><hr>\n\n### Question #50\n\nA company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The workload is powered by third-party software. The company needs to patch the third-party software on all EC2 instances as quickly as possible to remediate a critical security vulnerability.\n\nWhat should a solutions architect do to meet these requirements?\n\nA. Create an AWS Lambda function to apply the patch to all EC2 instances.\n\nB. Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.\n\nC. Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.\n\nD. Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances.\n\n<details>\n<summary>Show Answer</summary>\n\n> D\n\n</details>\n\n<br><hr>\n\n### Question #56\n\nA company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway in the ca-central-1 Region as a public interface for its backend microservice APIs. Third-party services consume the APIs securely. The company wants to design its API Gateway URL with the company's domain name and corresponding certificate so that the third-party services can use HTTPS.\n\nWhich solution will meet these requirements?\n\nA. Create stage variables in API Gateway with Name=\"Endpoint-URL\" and Value=\"Company Domain Name\" to overwrite the default URL. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM).\n\nB. Create Route 53 DNS records with the company's domain name. Point the alias record to the Regional API Gateway stage endpoint. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region.\n\nC. Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.\n\nD. Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region. Attach the certificate to the API Gateway APIs. Create Route 53 DNS records with the company's domain name. Point an A record to the company's domain name.\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #64\n\nA company has more than 5 TB of file data on Windows file servers that run on premises. Users and applications interact with the data each day.\nThe company is moving its Windows workloads to AWS. As the company continues this process, the company requires access to AWS and on-premises file storage with minimum latency. The company needs a solution that minimizes operational overhead and requires no significant changes to the existing file access patterns. The company uses an AWS Site-to-Site VPN connection for connectivity to AWS.\n\nWhat should a solutions architect do to meet these requirements?\n\nA. Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data to FSx for Windows File Server. Reconfigure the workloads to use FSx for Windows File Server on AWS.\n\nB. Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to the S3 File Gateway. Reconfigure the on-premises workloads and the cloud workloads to use the S3 File Gateway.\n\nC. Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to Amazon S3. Reconfigure the workloads to use either Amazon S3 directly or the S3 File Gateway. depending on each workload's location.\n\nD. Deploy and configure Amazon FSx for Windows File Server on AWS. Deploy and configure an Amazon FSx File Gateway on premises. Move the on-premises file data to the FSx File Gateway. Configure the cloud workloads to use FSx for Windows File Server on AWS. Configure the on-premises workloads to use the FSx File Gateway.\n\n<details>\n<summary>Show Answer</summary>\n\n> D\n\n</details>\n\n<br><hr>\n\n\n\n### Question #66\n\nA company has an application that generates a large number of files, each approximately 5 MB in size. The files are stored in Amazon S3. Company policy requires the files to be stored for 4 years before they can be deleted. Immediate accessibility is always required as the files contain critical business data that is not easy to reproduce. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days.\n\nWhich storage solution is MOST cost-effective?\n\nA. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Glacier 30 days from object creation. Delete the files 4 years after object creation.\n\nB. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days from object creation. Delete the files 4 years after object creation.\n\nC. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Delete the files 4 years after object creation.\n\nD. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Move the files to S3 Glacier 4 years after object creation.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #68\n\nA solutions architect is designing a new hybrid architecture to extend a company's on-premises infrastructure to AWS. The company requires a highly available connection with consistent low latency to an AWS Region. The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails.\n\nWhat should the solutions architect do to meet these requirements?\n\nA. Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the primary Direct Connect connection fails.\n\nB. Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN tunnel for private connectivity and as a backup if the primary VPN connection fails.\n\nC. Provision an AWS Direct Connect connection to a Region. Provision a second Direct Connect connection to the same Region as a backup if the primary Direct Connect connection fails.\n\nD. Provision an AWS Direct Connect connection to a Region. Use the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #71\n\nA company runs a shopping application that uses Amazon DynamoDB to store customer information. In case of data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour.\n\nWhat should the solutions architect recommend to meet these requirements?\n\nA. Configure DynamoDB global tables. For RPO recovery, point the application to a different AWS Region.\n\nB. Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.\n\nC. Export the DynamoDB data to Amazon S3 Glacier on a daily basis. For RPO recovery, import the data from S3 Glacier to DynamoDB.\n\nD. Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the DynamoDB table by using the EBS snapshot.\n\n<details>\n<summary>Show Answer</summary>\n\n> B \n\n</details>\n\n<br><hr>\n\n### Question #82\n\nA company hosts its web applications in the AWS Cloud. The company configures Elastic Load Balancers to use certificates that are imported into AWS Certificate Manager (ACM). The company's security team must be notified 30 days before the expiration of each certificate.\n\nWhat should a solutions architect recommend to meet this requirement?\n\nA. Add a rule in ACM to publish a custom message to an Amazon Simple Notification Service (Amazon SNS) topic every day, beginning 30 days before any certificate will expire.\n\nB. Create an AWS Config rule that checks for certificates that will expire within 30 days. Configure Amazon EventBridge (Amazon CloudWatch Events) to invoke a custom alert by way of Amazon Simple Notification Service (Amazon SNS) when AWS Config reports a noncompliant resource.\n\nC. Use AWS Trusted Advisor to check for certificates that will expire within 30 days. Create an Amazon CloudWatch alarm that is based on Trusted Advisor metrics for check status changes. Configure the alarm to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS).\n\nD. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect any certificates that will expire within 30 days. Configure the rule to invoke an AWS Lambda function. Configure the Lambda function to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS).\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #83\n\nA company's dynamic website is hosted using on-premises servers in the United States. The company is launching its product in Europe, and it wants to optimize site loading times for new European users. The site's backend must remain in the United States. The product is being launched in a few days, and an immediate solution is needed.\n\nWhat should the solutions architect recommend?\n\nA. Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.\n\nB. Move the website to Amazon S3. Use Cross-Region Replication between Regions.\n\nC. Use Amazon CloudFront with a custom origin pointing to the on-premises servers.\n\nD. Use an Amazon Route 53 geoproximity routing policy pointing to on-premises servers.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #84\n\nA company wants to reduce the cost of its existing three-tier web architecture. The web, application, and database servers are running on Amazon EC2 instances for the development, test, and production environments. The EC2 instances average 30% CPU utilization during peak hours and 10% CPU utilization during non-peak hours.\n\nThe production EC2 instances run 24 hours a day. The development and test EC2 instances run for at least 8 hours each day. The company plans to implement automation to stop the development and test EC2 instances when they are not in use.\n\nWhich EC2 instance purchasing solution will meet the company's requirements MOST cost-effectively?\n\nA. Use Spot Instances for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.\n\nB. Use Reserved Instances for the production EC2 instances. Use On-Demand Instances for the development and test EC2 instances.\n\nC. Use Spot blocks for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.\n\nD. Use On-Demand Instances for the production EC2 instances. Use Spot blocks for the development and test EC2 instances.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #88\n\nA survey company has gathered data for several years from areas in the United States. The company hosts the data in an Amazon S3 bucket that is 3 TB in size and growing. The company has started to share the data with a European marketing firm that has S3 buckets. The company wants to ensure that its data transfer costs remain as low as possible.\n\nWhich solution will meet these requirements?\n\nA. Configure the Requester Pays feature on the company's S3 bucket.\n\nB. Configure S3 Cross-Region Replication from the company's S3 bucket to one of the marketing firm's S3 buckets.\n\nC. Configure cross-account access for the marketing firm so that the marketing firm has access to the company's S3 bucket.\n\nD. Configure the company's S3 bucket to use S3 Intelligent-Tiering. Sync the S3 bucket to one of the marketing firm's S3 buckets.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #93\n\nA company runs an on-premises application that is powered by a MySQL database. The company is migrating the application to AWS to increase the application's elasticity and availability.\n\nThe current architecture shows heavy read activity on the database during times of normal operation. Every 4 hours, the company's development team pulls a full export of the production database to populate a database in the staging environment. During this period, users experience unacceptable application latency. The development team is unable to use the staging environment until the procedure completes.\n\nA solutions architect must recommend replacement architecture that alleviates the application latency issue. The replacement architecture also must give the development team the ability to continue using the staging environment without delay.\n\nWhich solution meets these requirements?\n\nA. Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility.\n\nB. Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to create the staging database on-demand.\n\nC. Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Use the standby instance for the staging database.\n\nD. Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #101\n\nA solutions architect is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates.\n\nWhat should the solutions architect do to enable Internet access for the private subnets?\n\nA. Create three NAT gateways, one for each public subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT gateway in its AZ.\n\nB. Create three NAT instances, one for each private subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT instance in its AZ.\n\nC. Create a second internet gateway on one of the private subnets. Update the route table for the private subnets that forward non-VPC traffic to the private internet gateway.\n\nD. Create an egress-only internet gateway on one of the public subnets. Update the route table for the private subnets that forward non-VPC traffic to the egress-only Internet gateway.\n\n<details>\n<summary>Show Answer</summary>\n\n> A \n\n</details>\n\n<br><hr>\n\n### Question #113\n\nA company uses 50 TB of data for reporting. The company wants to move this data from on premises to AWS. A custom application in the company’s data center runs a weekly data transformation job. The company plans to pause the application until the data transfer is complete and needs to begin the transfer process as soon as possible.\n\nThe data center does not have any available network bandwidth for additional workloads. A solutions architect must transfer the data and must configure the transformation job to continue to run in the AWS Cloud.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n\nA. Use AWS DataSync to move the data. Create a custom transformation job by using AWS Glue.\n\nB. Order an AWS Snowcone device to move the data. Deploy the transformation application to the device.\n\nC. Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue.\n\nD. Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data to the device. Create a new EC2 instance on AWS to run the transformation application.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #116\n\nA company uses a popular content management system (CMS) for its corporate website. However, the required patching and maintenance are burdensome. The company is redesigning its website and wants a new solution. The website will be updated four times a year and does not need to have any dynamic content available. The solution must provide high scalability and enhanced security.\n\nWhich combination of changes will meet these requirements with the LEAST operational overhead? (Choose two.)\n\nA. Configure Amazon CloudFront in front of the website to use HTTPS functionality.\n\nB. Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality.\n\nC. Create and deploy an AWS Lambda function to manage and serve the website content.\n\nD. Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.\n\nE. Create the new website. Deploy the website by using an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer.\n\n<details>\n<summary>Show Answer</summary>\n\n> A, D\n\n</details>\n\n<br><hr>\n\n\n\n### Question #119\n\nA global company is using Amazon API Gateway to design REST APIs for its loyalty club users in the us-east-1 Region and the ap-southeast-2 Region. A solutions architect must design a solution to protect these API Gateway managed REST APIs across multiple accounts from SQL injection and cross-site scripting attacks.\n\nWhich solution will meet these requirements with the LEAST amount of administrative effort?\n\nA. Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage.\n\nB. Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules.\n\nC. Set up AWS Shield in bath Regions. Associate Regional web ACLs with an API stage.\n\nD. Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #120\n\nA company has implemented a self-managed DNS solution on three Amazon EC2 instances behind a Network Load Balancer (NLB) in the us-west-2 Region. Most of the company's users are located in the United States and Europe. The company wants to improve the performance and availability of the solution. The company launches and configures three EC2 instances in the eu-west-1 Region and adds the EC2 instances as targets for a new NLB.\n\nWhich solution can the company use to route traffic to all the EC2 instances?\n\nA. Create an Amazon Route 53 geolocation routing policy to route requests to one of the two NLBs. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution’s origin.\n\nB. Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in us-west-2 and eu-west-1. Add the two NLBs as endpoints for the endpoint groups.\n\nC. Attach Elastic IP addresses to the six EC2 instances. Create an Amazon Route 53 geolocation routing policy to route requests to one of the six EC2 instances. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution's origin.\n\nD. Replace the two NLBs with two Application Load Balancers (ALBs). Create an Amazon Route 53 latency routing policy to route requests to one of the two ALBs. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution’s origin.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #125\n\nA company runs its two-tier ecommerce website on AWS. The web tier consists of a load balancer that sends traffic to Amazon EC2 instances. The database tier uses an Amazon RDS DB instance. The EC2 instances and the RDS DB instance should not be exposed to the public internet. The EC2 instances require internet access to complete payment processing of orders through a third-party web service. The application must be highly available.\n\nWhich combination of configuration options will meet these requirements? (Choose two.)\n\nA. Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-AZ DB instance in private subnets.\n\nB. Configure a VPC with two private subnets and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the private subnets.\n\nC. Use an Auto Scaling group to launch the EC2 instances in public subnets across two Availability Zones. Deploy an RDS Multi-AZ DB instance in private subnets.\n\nD. Configure a VPC with one public subnet, one private subnet, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnet.\n\nE. Configure a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnets.\n\n<details>\n<summary>Show Answer</summary>\n\n> A, D\n\n</details>\n\n<br><hr>\n\n### Question #129\n\nA company is running a multi-tier web application on premises. The web application is containerized and runs on a number of Linux hosts connected to a PostgreSQL database that contains user records. The operational overhead of maintaining the infrastructure and capacity planning is limiting the company's growth. A solutions architect must improve the application's infrastructure.\n\nWhich combination of actions should the solutions architect take to accomplish this? (Choose two.)\n\nA. Migrate the PostgreSQL database to Amazon Aurora.\n\nB. Migrate the web application to be hosted on Amazon EC2 instances.\n\nC. Set up an Amazon CloudFront distribution for the web application content.\n\nD. Set up Amazon ElastiCache between the web application and the PostgreSQL database.\n\nE. Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS).\n\n<details>\n<summary>Show Answer</summary>\n\n> A, E\n\n</details>\n\n<br><hr>\n\n### Question #134\n\nA company wants to move its application to a serverless solution. The serverless solution needs to analyze existing and new data by using SL. The company stores the data in an Amazon S3 bucket. The data requires encryption and must be replicated to a different AWS Region.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n\nA. Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region kays (SSE-KMS). Use Amazon Athena to query the data.\n\nB. Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region keys (SSE-KMS). Use Amazon RDS to query the data.\n\nC. Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon Athena to query the data.\n\nD. Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon RDS to query the data.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #139\n\nA reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon QuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket.\n\nThe reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3 bucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker Pipelines.\n\nWhat should a solutions architect do to meet these requirements with the LEAST operational overhead?\n\nA. Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.\n\nB. Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.\n\nC. Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.\n\nD. Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.\n\n<details>\n<summary>Show Answer</summary>\n\n> D \n\n</details>\n\n<br><hr>\n\n### Question #140\n\nA solutions architect needs to help a company optimize the cost of running an application on AWS. The application will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for compute within the architecture.\n\nThe EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic and unpredictable. Workloads that run on EC2 instances can be interrupted at any time. The application front end will run on Fargate, and Lambda will serve the API layer. The front-end utilization and API layer utilization will be predictable over the course of the next year.\n\nWhich combination of purchasing options will provide the MOST cost-effective solution for hosting this application? (Choose two.)\n\nA. Use Spot Instances for the data ingestion layer\n\nB. Use On-Demand Instances for the data ingestion layer\n\nC. Purchase a 1-year Compute Savings Plan for the front end and API layer.\n\nD. Purchase 1-year All Upfront Reserved instances for the data ingestion layer.\n\nE. Purchase a 1-year EC2 instance Savings Plan for the front end and API layer.\n\n<details>\n<summary>Show Answer</summary>\n\n> A, C\n\n</details>\n\n<br><hr>\n\n### Question #143\n\nA company wants to migrate its existing on-premises monolithic application to AWS. The company wants to keep as much of the front-end code and the backend code as possible. However, the company wants to break the application into smaller applications. A different team will manage each application. The company needs a highly scalable solution that minimizes operational overhead.\n\nWhich solution will meet these requirements?\n\nA. Host the application on AWS Lambda. Integrate the application with Amazon API Gateway.\n\nB. Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API that is integrated with AWS Lambda.\n\nC. Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2 instances in an Auto Scaling group as targets.\n\nD. Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target.\n\n<details>\n<summary>Show Answer</summary>\n\n> D\n\n</details>\n\n<br><hr>\n\n### Question #144\n\nA company recently started using Amazon Aurora as the data store for its global ecommerce application. When large reports are run, developers report that the ecommerce application is performing poorly. After reviewing metrics in Amazon CloudWatch, a solutions architect finds that the ReadIOPS and CPUUtilizalion metrics are spiking when monthly reports run.\n\nWhat is the MOST cost-effective solution?\n\nA. Migrate the monthly reporting to Amazon Redshift.\n\nB. Migrate the monthly reporting to an Aurora Replica.\n\nC. Migrate the Aurora database to a larger instance class.\n\nD. Increase the Provisioned IOPS on the Aurora instance.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #145\n\nA company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. The analytics software is written in PHP and uses a MySQL database. The analytics software, the web server that provides PHP, and the database server are all hosted on the EC2 instance. The application is showing signs of performance degradation during busy times and is presenting 5xx errors. The company needs to make the application scale seamlessly.\n\nWhich solution will meet these requirements MOST cost-effectively?\n\nA. Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load to each EC2 instance.\n\nB. Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances.\n\nC. Migrate the database to an Amazon Aurora MySQL DB instance. Create an AWS Lambda function to stop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization surpasses 75%.\n\nD. Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web application. Apply the AMI to a launch template. Create an Auto Scaling group with the launch template Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group.\n\n<details>\n<summary>Show Answer</summary>\n\n> D\n\n</details>\n\n<br><hr>\n\n### Question #150\n\nA company is migrating an application from on-premises servers to Amazon EC2 instances. As part of the migration design requirements, a solutions architect must implement infrastructure metric alarms. The company does not need to take action if CPU utilization increases to more than 50% for a short burst of time. However, if the CPU utilization increases to more than 50% and read IOPS on the disk are high at the same time, the company needs to act as soon as possible. The solutions architect also must reduce false alarms.\n\nWhat should the solutions architect do to meet these requirements?\n\nA. Create Amazon CloudWatch composite alarms where possible.\n\nB. Create Amazon CloudWatch dashboards to visualize the metrics and react to issues quickly.\n\nC. Create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm.\n\nD. Create single Amazon CloudWatch metric alarms with multiple metric thresholds where possible.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #151\n\nA company wants to migrate its on-premises data center to AWS. According to the company's compliance requirements, the company can use only the ap-northeast-3 Region. Company administrators are not permitted to connect VPCs to the internet.\n\nWhich solutions will meet these requirements? (Choose two.)\n\nA. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3.\n\nB. Use rules in AWS WAF to prevent internet access. Deny access to all AWS Regions except ap-northeast-3 in the AWS account settings.\n\nC. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.\n\nD. Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0. Create an IAM policy for each user to prevent the use of any AWS Region other than ap-northeast-3.\n\nE. Use AWS Config to activate managed rules to detect and alert for internet gateways and to detect and alert for new resources deployed outside of ap-northeast-3.\n\n<details>\n<summary>Show Answer</summary>\n\n> A, C\n\n</details>\n\n<br><hr>\n\n\n\n### Question #157\n\nA company stores data in an Amazon Aurora PostgreSQL DB cluster. The company must store all the data for 5 years and must delete all the data after 5 years. The company also must indefinitely keep audit logs of actions that are performed within the database. Currently, the company has automated backups configured for Aurora.\n\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)\n\nA. Take a manual snapshot of the DB cluster.\n\nB. Create a lifecycle policy for the automated backups.\n\nC. Configure automated backup retention for 5 years.\n\nD. Configure an Amazon CloudWatch Logs export for the DB cluster.\n\nE. Use AWS Backup to take the backups and to keep the backups for 5 years.\n\n<details>\n<summary>Show Answer</summary>\n\n> D, E\n\n</details>\n\n<br><hr>\n\n### Question #158\n\nA solutions architect is optimizing a website for an upcoming musical event. Videos of the performances will be streamed in real time and then will be available on demand. The event is expected to attract a global online audience.\n\nWhich service will improve the performance of both the real-time and on-demand streaming?\n\nA. Amazon CloudFront\n\nB. AWS Global Accelerator\n\nC. Amazon Route 53\n\nD. Amazon S3 Transfer Acceleration\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #159\n\nA company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS Lambda. The application’s traffic recently spiked due to fraudulent requests from botnets.\n\nWhich steps should a solutions architect take to block requests from unauthorized users? (Choose two.)\n\nA. Create a usage plan with an API key that is shared with genuine users only.\n\nB. Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses.\n\nC. Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.\n\nD. Convert the existing public API to a private API. Update the DNS records to redirect users to the new API endpoint.\n\nE. Create an IAM role for each user attempting to access the API. A user will assume the role when making the API call.\n\n<details>\n<summary>Show Answer</summary>\n\n> A, C\n\n</details>\n\n<br><hr>\n\n### Question #172\n\nA solutions architect is creating a new Amazon CloudFront distribution for an application. Some of the information submitted by users is sensitive. The application uses HTTPS but needs another layer of security. The sensitive information should be protected throughout the entire application stack, and access to the information should be restricted to certain applications.\n\nWhich action should the solutions architect take?\n\nA. Configure a CloudFront signed URL.\n\nB. Configure a CloudFront signed cookie.\n\nC. Configure a CloudFront field-level encryption profile.\n\nD. Configure CloudFront and set the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol Policy.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #182\n\nA company wants to migrate its MySQL database from on premises to AWS. The company recently experienced a database outage that significantly impacted the business. To ensure this does not happen again, the company wants a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes.\n\nWhich solution meets these requirements?\n\nA. Create an Amazon RDS DB instance with synchronous replication to three nodes in three Availability Zones.\n\nB. Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data.\n\nC. Create an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data.\n\nD. Create an Amazon EC2 instance with a MySQL engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #184\n\nA company has an AWS account used for software engineering. The AWS account has access to the company’s on-premises data center through a pair of AWS Direct Connect connections. All non-VPC traffic routes to the virtual private gateway.\n\nA development team recently created an AWS Lambda function through the console. The development team needs to allow the function to access a database that runs in a private subnet in the company’s data center.\n\nWhich solution will meet these requirements?\n\nA. Configure the Lambda function to run in the VPC with the appropriate security group.\n\nB. Set up a VPN connection from AWS to the data center. Route the traffic from the Lambda function through the VPN.\n\nC. Update the route tables in the VPC to allow the Lambda function to access the on-premises data center through Direct Connect.\n\nD. Create an Elastic IP address. Configure the Lambda function to send traffic through the Elastic IP address without an elastic network interface.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #188\n\nA company uses Amazon S3 as its data lake. The company has a new partner that must use SFTP to upload data files. A solutions architect needs to implement a highly available SFTP solution that minimizes operational overhead.\n\nWhich solution will meet these requirements?\n\nA. Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible endpoint. Choose the S3 data lake as the destination.\n\nB. Use Amazon S3 File Gateway as an SFTP server. Expose the S3 File Gateway endpoint URL to the new partner. Share the S3 File Gateway endpoint with the new partner.\n\nC. Launch an Amazon EC2 instance in a private subnet in a VPInstruct the new partner to upload files to the EC2 instance by using a VPN. Run a cron job script, on the EC2 instance to upload files to the S3 data lake.\n\nD. Launch Amazon EC2 instances in a private subnet in a VPC. Place a Network Load Balancer (NLB) in front of the EC2 instances. Create an SFTP listener port for the NLB. Share the NLB hostname with the new partner. Run a cron job script on the EC2 instances to upload files to the S3 data lake.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #189\n\nA company needs to store contract documents. A contract lasts for 5 years. During the 5-year period, the company must ensure that the documents cannot be overwritten or deleted. The company needs to encrypt the documents at rest and rotate the encryption keys automatically every year.\n\nWhich combination of steps should a solutions architect take to meet these requirements with the LEAST operational overhead? (Choose two.)\n\nA. Store the documents in Amazon S3. Use S3 Object Lock in governance mode.\n\nB. Store the documents in Amazon S3. Use S3 Object Lock in compliance mode.\n\nC. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure key rotation.\n\nD. Use server-side encryption with AWS Key Management Service (AWS KMS) customer managed keys. Configure key rotation.\n\nE. Use server-side encryption with AWS Key Management Service (AWS KMS) customer provided (imported) keys. Configure key rotation.\n\n<details>\n<summary>Show Answer</summary>\n\n> B, D\n\n</details>\n\n<br><hr>\n\n### Question #190\n\nA company has a web application that is based on Java and PHP. The company plans to move the application from on premises to AWS. The company needs the ability to test new site features frequently. The company also needs a highly available and managed solution that requires minimum operational overhead.\n\nWhich solution will meet these requirements?\n\nA. Create an Amazon S3 bucket. Enable static web hosting on the S3 bucket. Upload the static content to the S3 bucket. Use AWS Lambda to process all dynamic content.\n\nB. Deploy the web application to an AWS Elastic Beanstalk environment. Use URL swapping to switch between multiple Elastic Beanstalk environments for feature testing.\n\nC. Deploy the web application to Amazon EC2 instances that are configured with Java and PHP. Use Auto Scaling groups and an Application Load Balancer to manage the website’s availability.\n\nD. Containerize the web application. Deploy the web application to Amazon EC2 instances. Use the AWS Load Balancer Controller to dynamically route traffic between containers that contain the new site features for testing.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #194\n\nA company needs to run a critical application on AWS. The company needs to use Amazon EC2 for the application’s database. The database must be highly available and must fail over automatically if a disruptive event occurs.\n\nWhich solution will meet these requirements?\n\nA. Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Set up database replication.\n\nB. Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use AWS CloudFormation to automate provisioning of the EC2 instance if a disruptive event occurs.\n\nC. Launch two EC2 instances, each in a different AWS Region. Install the database on both EC2 instances. Set up database replication. Fail over the database to a second Region.\n\nD. Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use EC2 automatic recovery to recover the instance if a disruptive event occurs.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #201\n\nA company is developing a marketing communications service that targets mobile app users. The company needs to send confirmation messages with Short Message Service (SMS) to its users. The users must be able to reply to the SMS messages. The company must store the responses for a year for analysis.\n\nWhat should a solutions architect do to meet these requirements?\n\nA. Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to process the responses.\n\nB. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.\n\nC. Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda to process the responses.\n\nD. Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #208\n\nA company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must ensure that no API calls and no data are routed through public internet routes. Only the EC2 instance can have access to upload data to the S3 bucket.\n\nWhich solution will meet these requirements?\n\nA. Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.\n\nB. Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is located. Attach appropriate security groups to the endpoint. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.\n\nC. Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3 bucket’s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.\n\nD. Use the AWS provided, publicly available ip-ranges.json file to obtain the private IP address of the S3 bucket’s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #215\n\nA company has 700 TB of backup data stored in network attached storage (NAS) in its data center. This backup data need to be accessible for infrequent regulatory requests and must be retained 7 years. The company has decided to migrate this backup data from its data center to AWS. The migration must be complete within 1 month. The company has 500 Mbps of dedicated bandwidth on its public internet connection available for data transfer.\n\nWhat should a solutions architect do to migrate and store the data at the LOWEST cost?\n\nA. Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.\n\nB. Deploy a VPN connection between the data center and Amazon VPC. Use the AWS CLI to copy the data from on premises to Amazon S3 Glacier.\n\nC. Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.\n\nD. Use AWS DataSync to transfer the data and deploy a DataSync agent on premises. Use the DataSync task to copy files from the on-premises NAS storage to Amazon S3 Glacier.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #217\n\nA company runs a global web application on Amazon EC2 instances behind an Application Load Balancer. The application stores data in Amazon Aurora. The company needs to create a disaster recovery solution and can tolerate up to 30 minutes of downtime and potential data loss. The solution does not need to handle the load when the primary infrastructure is healthy.\n\nWhat should a solutions architect do to meet these requirements?\n\nA. Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in a second AWS Region.\n\nB. Host a scaled-down deployment of the application in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora Replica in the second Region.\n\nC. Replicate the primary infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora database that is restored from the latest snapshot.\n\nD. Back up data with AWS Backup. Use the backup to create the required infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-passive failover. Create an Aurora second primary instance in the second Region.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #218\n\nA company has a web server running on an Amazon EC2 instance in a public subnet with an Elastic IP address. The default security group is assigned to the EC2 instance. The default network ACL has been modified to block all traffic. A solutions architect needs to make the web server accessible from everywhere on port 443.\n\nWhich combination of steps will accomplish this task? (Choose two.)\n\nA. Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.\n\nB. Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0.\n\nC. Update the network ACL to allow TCP port 443 from source 0.0.0.0/0.\n\nD. Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0.\n\nE. Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0.\n\n<details>\n<summary>Show Answer</summary>\n\n> A, E \n\n</details>\n\n<br><hr>\n\n### Question #219\n\nA company’s application is having performance issues. The application is stateful and needs to complete in-memory tasks on Amazon EC2 instances. The company used AWS CloudFormation to deploy infrastructure and used the M5 EC2 instance family. As traffic increased, the application performance degraded. Users are reporting delays when the users attempt to access the application.\n\nWhich solution will resolve these issues in the MOST operationally efficient way?\n\nA. Replace the EC2 instances with T3 EC2 instances that run in an Auto Scaling group. Make the changes by using the AWS Management Console.\n\nB. Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group. Increase the desired capacity and the maximum capacity of the Auto Scaling group manually when an increase is necessary.\n\nC. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon CloudWatch built-in EC2 memory metrics to track the application performance for future capacity planning.\n\nD. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.\n\n<details>\n<summary>Show Answer</summary>\n\n> D\n\n</details>\n\n<br><hr>\n\n### Question #223\n\nA company has deployed a Java Spring Boot application as a pod that runs on Amazon Elastic Kubernetes Service (Amazon EKS) in private subnets. The application needs to write data to an Amazon DynamoDB table. A solutions architect must ensure that the application can interact with the DynamoDB table without exposing traffic to the internet.\n\nWhich combination of steps should the solutions architect take to accomplish this goal? (Choose two.)\n\nA. Attach an IAM role that has sufficient privileges to the EKS pod.\n\nB. Attach an IAM user that has sufficient privileges to the EKS pod.\n\nC. Allow outbound connectivity to the DynamoDB table through the private subnets’ network ACLs.\n\nD. Create a VPC endpoint for DynamoDB.\n\nE. Embed the access keys in the Java Spring Boot code.\n\n<details>\n<summary>Show Answer</summary>\n\n> A, D\n\n</details>\n\n<br><hr>\n\n### Question #224\n\nA company recently migrated its web application to AWS by rehosting the application on Amazon EC2 instances in a single AWS Region. The company wants to redesign its application architecture to be highly available and fault tolerant. Traffic must reach all running EC2 instances randomly.\n\nWhich combination of steps should the company take to meet these requirements? (Choose two.)\n\nA. Create an Amazon Route 53 failover routing policy.\n\nB. Create an Amazon Route 53 weighted routing policy.\n\nC. Create an Amazon Route 53 multivalue answer routing policy.\n\nD. Launch three EC2 instances: two instances in one Availability Zone and one instance in another Availability Zone.\n\nE. Launch four EC2 instances: two instances in one Availability Zone and two instances in another Availability Zone.\n\n<details>\n<summary>Show Answer</summary>\n\n> C, E\n\n</details>\n\n<br><hr>\n\n### Question #225\n\nA media company collects and analyzes user activity data on premises. The company wants to migrate this capability to AWS. The user activity data store will continue to grow and will be petabytes in size. The company needs to build a highly available data ingestion solution that facilitates on-demand analytics of existing data and new data with SQL.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n\nA. Send activity data to an Amazon Kinesis data stream. Configure the stream to deliver the data to an Amazon S3 bucket.\n\nB. Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver the data to an Amazon Redshift cluster.\n\nC. Place activity data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on the data as the data arrives in the S3 bucket.\n\nD. Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability Zones. Configure the service to forward data to an Amazon RDS Multi-AZ database.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #233\n\nA company has three VPCs named Development, Testing, and Production in the us-east-1 Region. The three VPCs need to be connected to an on-premises data center and are designed to be separate to maintain security and prevent any resource sharing. A solutions architect needs to find a scalable and secure solution.\n\nWhat should the solutions architect recommend?\n\nA. Create an AWS Direct Connect connection and a VPN connection for each VPC to connect back to the data center.\n\nB. Create VPC peers from all the VPCs to the Production VPC. Use an AWS Direct Connect connection from the Production VPC back to the data center.\n\nC. Connect VPN connections from all the VPCs to a VPN in the Production VPC. Use a VPN connection from the Production VPC back to the data center.\n\nD. Create a new VPC called Network. Within the Network VPC, create an AWS Transit Gateway with an AWS Direct Connect connection back to the data center. Attach all the other VPCs to the Network VPC.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #234\n\nA company is building a new web-based customer relationship management application. The application will use several Amazon EC2 instances that are backed by Amazon Elastic Block Store (Amazon EBS) volumes behind an Application Load Balancer (ALB). The application will also use an Amazon Aurora database. All data for the application must be encrypted at rest and in transit.\n\nWhich solution will meet these requirements?\n\nA. Use AWS Key Management Service (AWS KMS) certificates on the ALB to encrypt data in transit. Use AWS Certificate Manager (ACM) to encrypt the EBS volumes and Aurora database storage at rest.\n\nB. Use the AWS root account to log in to the AWS Management Console. Upload the company’s encryption certificates. While in the root account, select the option to turn on encryption for all data at rest and in transit for the account.\n\nC. Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database storage at rest. Attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in transit.\n\nD. Use BitLocker to encrypt all data at rest. Import the company’s TLS certificate keys to AWS Key Management Service (AWS KMS) Attach the KMS keys to the ALB to encrypt data in transit.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #266\n\nA company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups configured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect traffic to healthy endpoints.\n\nWhich solution meets these requirements?\n\nA. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.\n\nB. Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.\n\nC. Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.\n\nD. Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #276\n\nA company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An Amazon RDS for Oracle instance is the application’s data layer that uses Oracle-specific PL/SQL functions. Traffic to the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the RDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and defines the minimum healthy instance count only. The company predicts that traffic will continue to increase at a steady but unpredictable rate before leveling off.\n\nWhat should a solutions architect do to ensure the system can automatically scale for the increased traffic? (Choose two.)\n\nA. Configure storage Auto Scaling on the RDS for Oracle instance.\n\nB. Migrate the database to Amazon Aurora to use Auto Scaling storage.\n\nC. Configure an alarm on the RDS for Oracle instance for low free storage space.\n\nD. Configure the Auto Scaling group to use the average CPU as the scaling metric.\n\nE. Configure the Auto Scaling group to use the average free memory as the scaling metric.\n\n<details>\n<summary>Show Answer</summary>\n\n> A, D\n\n</details>\n\n<br><hr>\n\n### Question #287\n\nA company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company wants to use specific features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers.\n\nHow should a solutions architect design the architecture to meet these requirements?\n\nA. Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.\n\nB. Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.\n\nC. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.\n\nD. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #295\n\nAn ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally identifiable information (PII). The company wants to use the data in three applications. Only one of the applications needs to process the PII. The PII must be removed before the other two applications process the data.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n\nA. Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and process the data that each application requests.\n\nB. Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.\n\nC. Process the data and store the transformed data in three separate Amazon S3 buckets so that each application has its own custom dataset. Point each application to its respective S3 bucket.\n\nD. Process the data and store the transformed data in three separate Amazon DynamoDB tables so that each application has its own custom dataset. Point each application to its respective DynamoDB table.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #327\n\nA solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party’s URL. Other internet traffic must be blocked.\n\nWhich solution meets these requirements?\n\nA. Update the route table for the private subnet to route the outbound traffic to an AWS Network Firewall firewall. Configure domain list rule groups.\n\nB. Set up an AWS WAF web ACL. Create a custom set of rules that filter traffic requests based on source and destination IP address range sets.\n\nC. Implement strict inbound security group rules. Configure an outbound rule that allows traffic only to the authorized software repositories on the internet by specifying the URLs.\n\nD. Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound traffic to the ALB. Use a URL-based rule listener in the ALB’s target group for outbound access to the internet.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #328\n\nA company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously.\n\nThe company is expecting a significant and sudden increase in the number of sales requests during events for the launch of new products.\n\nWhat should a solutions architect recommend to ensure that all the requests are processed successfully?\n\nA. Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in traffic.\n\nB. Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network traffic.\n\nC. Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce traffic for the API to handle.\n\nD. Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances.\n\n<details>\n<summary>Show Answer</summary>\n\n> D\n\n</details>\n\n<br><hr>\n\n### Question #333\n\nA company’s application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of every month at midnight, the application becomes much slower when the month-end financial calculation batch runs. This causes the CPU utilization of the EC2 instances to immediately peak to 100%, which disrupts the application.\n\nWhat should a solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?\n\nA. Configure an Amazon CloudFront distribution in front of the ALB.\n\nB. Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.\n\nC. Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.\n\nD. Configure Amazon ElastiCache to remove some of the workload from the EC2 instances.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #335\n\nA company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand.\n\nWhich solution meets these requirements?\n\nA. Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step Functions to replace the AMI in the Auto Scaling group.\n\nB. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.\n\nC. Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM). Create an AWS Lambda function that modifies the AMI in the Auto Scaling group.\n\nD. Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure Auto Scaling group capacity limits as an event source in EventBridge.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #342\n\nA transaction processing company has weekly scripted batch jobs that run on Amazon EC2 instances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, but the baseline CPU utilization that is noted on each run is at least 60%. The company needs to provision the capacity 30 minutes before the jobs run.\n\nCurrently, engineers complete this task by manually modifying the Auto Scaling group parameters. The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts. The company needs an automated way to modify the Auto Scaling group’s desired capacity.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n\nA. Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based on the CPU utilization metric. Set the target value for the metric to 60%.\n\nB. Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired capacity, minimum capacity, and maximum capacity. Set the recurrence to weekly. Set the start time to 30 minutes before the batch jobs run.\n\nC. Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based on forecast. Set the scaling metric to CPU utilization. Set the target value for the metric to 60%. In the policy, set the instances to pre-launch 30 minutes before the jobs run.\n\nD. Create an Amazon EventBridge event to invoke an AWS Lambda function when the CPU utilization metric value for the Auto Scaling group reaches 60%. Configure the Lambda function to increase the Auto Scaling group’s desired capacity and maximum capacity by 20%.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #344\n\nA company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB.\n\nWhich solution will meet these requirements with the FEWEST changes to the code?\n\nA. Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.\n\nB. Use Amazon EventBridge to post large messages from the application instead of Amazon SQS.\n\nC. Change the limit in Amazon SQS to handle messages that are larger than 256 KB.\n\nD. Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS). Configure Amazon SQS to reference this location in the messages.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #346\n\nA company has an aging network-attached storage (NAS) array in its data center. The NAS array presents SMB shares and NFS shares to client workstations. The company does not want to purchase a new NAS array. The company also does not want to incur the cost of renewing the NAS array’s support contract. Some of the data is accessed frequently, but much of the data is inactive.\n\nA solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 Lifecycle policies, and maintains the same look and feel for the client workstations. The solutions architect has identified AWS Storage Gateway as part of the solution.\n\nWhich type of storage gateway should the solutions architect provision to meet these requirements?\n\nA. Volume Gateway\n\nB. Tape Gateway\n\nC. Amazon FSx File Gateway\n\nD. Amazon S3 File Gateway\n\n<details>\n<summary>Show Answer</summary>\n\n> D\n\n</details>\n\n<br><hr>\n\n### Question #349\n\nA company stores confidential data in an Amazon Aurora PostgreSQL database in the ap-southeast-3 Region. The database is encrypted with an AWS Key Management Service (AWS KMS) customer managed key. The company was recently acquired and must securely share a backup of the database with the acquiring company’s AWS account in ap-southeast-3.\n\nWhat should a solutions architect do to meet these requirements?\n\nA. Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new snapshot with the acquiring company’s AWS account.\n\nB. Create a database snapshot. Add the acquiring company’s AWS account to the KMS key policy. Share the snapshot with the acquiring company’s AWS account.\n\nC. Create a database snapshot that uses a different AWS managed KMS key. Add the acquiring company’s AWS account to the KMS key alias. Share the snapshot with the acquiring company's AWS account.\n\nD. Create a database snapshot. Download the database snapshot. Upload the database snapshot to an Amazon S3 bucket. Update the S3 bucket policy to allow access from the acquiring company’s AWS account.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #353\n\nA company hosts a three-tier web application on Amazon EC2 instances in a single Availability Zone. The web application uses a self-managed MySQL database that is hosted on an EC2 instance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL database currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects traffic of 1,000 IOPS for both reads and writes at peak traffic.\n\nThe company wants to minimize any disruptions, stabilize performance, and reduce costs while retaining the capacity for double the IOPS. The company wants to move the database tier to a fully managed solution that is highly available and fault tolerant.\n\nWhich solution will meet these requirements MOST cost-effectively?\n\nA. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block Express EBS volume.\n\nB. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.\n\nC. Use Amazon S3 Intelligent-Tiering access tiers.\n\nD. Use two large EC2 instances to host the database in active-passive mode.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #363\n\nA company is building a game system that needs to send unique events to separate leaderboard, matchmaking, and authentication services concurrently. The company needs an AWS event-driven system that guarantees the order of the events.\n\nWhich solution will meet these requirements?\n\nA. Amazon EventBridge event bus\n\nB. Amazon Simple Notification Service (Amazon SNS) FIFO topics\n\nC. Amazon Simple Notification Service (Amazon SNS) standard topics\n\nD. Amazon Simple Queue Service (Amazon SQS) FIFO queues\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #364\n\nA hospital is designing a new application that gathers symptoms from patients. The hospital has decided to use Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) in the architecture.\n\nA solutions architect is reviewing the infrastructure design. Data must be encrypted at rest and in transit. Only authorized personnel of the hospital should be able to access the data.\n\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)\n\nA. Turn on server-side encryption on the SQS components. Update the default key policy to restrict key usage to a set of authorized principals.\n\nB. Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals.\n\nC. Turn on encryption on the SNS components. Update the default key policy to restrict key usage to a set of authorized principals. Set a condition in the topic policy to allow only encrypted connections over TLS.\n\nD. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.\n\nE. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply an IAM policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.\n\n<details>\n<summary>Show Answer</summary>\n\n> B, D \n\n</details>\n\n<br><hr>\n\n### Question #366\n\nA company’s web application consists of an Amazon API Gateway API in front of an AWS Lambda function and an Amazon DynamoDB database. The Lambda function handles the business logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito user pools to identify the individual users of the application. A solutions architect needs to update the application so that only users who have a subscription can access premium content.\n\nWhich solution will meet this requirement with the LEAST operational overhead?\n\nA. Enable API caching and throttling on the API Gateway API.\n\nB. Set up AWS WAF on the API Gateway API. Create a rule to filter users who have a subscription.\n\nC. Apply fine-grained IAM permissions to the premium content in the DynamoDB table.\n\nD. Implement API usage plans and API keys to limit the access of users who do not have a subscription.\n\n<details>\n<summary>Show Answer</summary>\n\n> D\n\n</details>\n\n<br><hr>\n\n### Question #369\n\nA company has migrated an application to Amazon EC2 Linux instances. One of these EC2 instances runs several 1-hour tasks on a schedule. These tasks were written by different teams and have no common programming language. The company is concerned about performance and scalability while these tasks run on a single instance. A solutions architect needs to implement a solution to resolve these concerns.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n\nA. Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events).\n\nB. Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to run the tasks as jobs.\n\nC. Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events).\n\nD. Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto Scaling group with the AMI to run multiple copies of the instance.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #370\n\nA company runs a public three-tier web application in a VPC. The application runs on Amazon EC2 instances across multiple Availability Zones. The EC2 instances that run in private subnets need to communicate with a license server over the internet. The company needs a managed solution that minimizes operational maintenance.\n\nWhich solution meets these requirements?\n\nA. Provision a NAT instance in a public subnet. Modify each private subnet's route table with a default route that points to the NAT instance.\n\nB. Provision a NAT instance in a private subnet. Modify each private subnet's route table with a default route that points to the NAT instance.\n\nC. Provision a NAT gateway in a public subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.\n\nD. Provision a NAT gateway in a private subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #371\n\nA company needs to create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host a digital media streaming application. The EKS cluster will use a managed node group that is backed by Amazon Elastic Block Store (Amazon EBS) volumes for storage. The company must encrypt all data at rest by using a customer managed key that is stored in AWS Key Management Service (AWS KMS).\n\nWhich combination of actions will meet this requirement with the LEAST operational overhead? (Choose two.)\n\nA. Use a Kubernetes plugin that uses the customer managed key to perform data encryption.\n\nB. After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the customer managed key.\n\nC. Enable EBS encryption by default in the AWS Region where the EKS cluster will be created. Select the customer managed key as the default key.\n\nD. Create the EKS cluster. Create an IAM role that has a policy that grants permission to the customer managed key. Associate the role with the EKS cluster.\n\nE. Store the customer managed key as a Kubernetes secret in the EKS cluster. Use the customer managed key to encrypt the EBS volumes.\n\n<details>\n<summary>Show Answer</summary>\n\n> C, D\n\n</details>\n\n<br><hr>\n\n### Question #379\n\nA company hosts a frontend application that uses an Amazon API Gateway API backend that is integrated with AWS Lambda. When the API receives requests, the Lambda function loads many libraries. Then the Lambda function connects to an Amazon RDS database, processes the data, and returns the data to the frontend application. The company wants to ensure that response latency is as low as possible for all its users with the fewest number of changes to the company's operations.\n\nWhich solution will meet these requirements?\n\nA. Establish a connection between the frontend application and the database to make queries faster by bypassing the API.\n\nB. Configure provisioned concurrency for the Lambda function that handles the requests.\n\nC. Cache the results of the queries in Amazon S3 for faster retrieval of similar datasets.\n\nD. Increase the size of the database to increase the number of connections Lambda can establish at one time.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #385\n\nA solutions architect is creating a new VPC design. There are two public subnets for the load balancer, two private subnets for web servers, and two private subnets for MySQL. The web servers use only HTTPS. The solutions architect has already created a security group for the load balancer allowing port 443 from 0.0.0.0/0. Company policy requires that each resource has the least access required to still be able to perform its tasks.\n\nWhich additional configuration strategy should the solutions architect use to meet these requirements?\n\nA. Create a security group for the web servers and allow port 443 from 0.0.0.0/0. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.\n\nB. Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.\n\nC. Create a security group for the web servers and allow port 443 from the load balancer. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.\n\nD. Create a network ACL for the web servers and allow port 443 from the load balancer. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #388\n\nA company is deploying a two-tier web application in a VPC. The web tier is using an Amazon EC2 Auto Scaling group with public subnets that span multiple Availability Zones. The database tier consists of an Amazon RDS for MySQL DB instance in separate private subnets. The web tier requires access to the database to retrieve product information.\n\nThe web application is not working as intended. The web application reports that it cannot connect to the database. The database is confirmed to be up and running. All configurations for the network ACLs, security groups, and route tables are still in their default states.\n\nWhat should a solutions architect recommend to fix the application?\n\nA. Add an explicit rule to the private subnet’s network ACL to allow traffic from the web tier’s EC2 instances.\n\nB. Add a route in the VPC route table to allow traffic between the web tier’s EC2 instances and the database tier.\n\nC. Deploy the web tier's EC2 instances and the database tier’s RDS instance into two separate VPCs, and configure VPC peering.\n\nD. Add an inbound rule to the security group of the database tier’s RDS instance to allow traffic from the web tiers security group.\n\n<details>\n<summary>Show Answer</summary>\n\n> D\n\n</details>\n\n<br><hr>\n\n### Question #390\n\nA company hosts a three-tier ecommerce application on a fleet of Amazon EC2 instances. The instances run in an Auto Scaling group behind an Application Load Balancer (ALB). All ecommerce data is stored in an Amazon RDS for MariaDB Multi-AZ DB instance.\n\nThe company wants to optimize customer session management during transactions. The application must store session data durably.\n\nWhich solutions will meet these requirements? (Choose two.)\n\nA. Turn on the sticky sessions feature (session affinity) on the ALB.\n\nB. Use an Amazon DynamoDB table to store customer session information.\n\nC. Deploy an Amazon Cognito user pool to manage user session information.\n\nD. Deploy an Amazon ElastiCache for Redis cluster to store customer session information.\n\nE. Use AWS Systems Manager Application Manager in the application to manage user session information.\n\n<details>\n<summary>Show Answer</summary>\n\n> A, D\n\n</details>\n\n<br><hr>\n\n### Question #391\n\nA company needs a backup strategy for its three-tier stateless web application. The web application runs on Amazon EC2 instances in an Auto Scaling group with a dynamic scaling policy that is configured to respond to scaling events. The database tier runs on Amazon RDS for PostgreSQL. The web application does not require temporary local storage on the EC2 instances. The company’s recovery point objective (RPO) is 2 hours.\n\nThe backup strategy must maximize scalability and optimize resource utilization for this environment.\n\nWhich solution will meet these requirements?\n\nA. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and database every 2 hours to meet the RPO.\n\nB. Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS) snapshots. Enable automated backups in Amazon RDS to meet the RPO.\n\nC. Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.\n\nD. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances every 2 hours. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #399\n\nA financial company hosts a web application on AWS. The application uses an Amazon API Gateway Regional API endpoint to give users the ability to retrieve current stock prices. The company’s security team has noticed an increase in the number of API requests. The security team is concerned that HTTP flood attacks might take the application offline.\n\nA solutions architect must design a solution to protect the application from this type of attack.\n\nWhich solution meets these requirements with the LEAST operational overhead?\n\nA. Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24 hours.\n\nB. Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.\n\nC. Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when the predefined rate is reached.\n\nD. Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway Regional API endpoint. Create an AWS Lambda function to block requests from IP addresses that exceed the predefined rate.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #402\n\nA company needs to ingest and handle large amounts of streaming data that its application generates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is configured with default settings. Every other day, the application consumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the data that the application sends to Kinesis Data Streams.\n\nWhat should a solutions architect do to resolve this issue?\n\nA. Update the Kinesis Data Streams default settings by modifying the data retention period.\n\nB. Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.\n\nC. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.\n\nD. Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #406\n\nA solutions architect is designing a two-tiered architecture that includes a public subnet and a database subnet. The web servers in the public subnet must be open to the internet on port 443. The Amazon RDS for MySQL DB instance in the database subnet must be accessible only to the web servers on port 3306.\n\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)\n\nA. Create a network ACL for the public subnet. Add a rule to deny outbound traffic to 0.0.0.0/0 on port 3306.\n\nB. Create a security group for the DB instance. Add a rule to allow traffic from the public subnet CIDR block on port 3306.\n\nC. Create a security group for the web servers in the public subnet. Add a rule to allow traffic from 0.0.0.0/0 on port 443.\n\nD. Create a security group for the DB instance. Add a rule to allow traffic from the web servers’ security group on port 3306.\n\nE. Create a security group for the DB instance. Add a rule to deny all traffic except traffic from the web servers’ security group on port 3306.\n\n<details>\n<summary>Show Answer</summary>\n\n> C, D\n\n</details>\n\n<br><hr>\n\n### Question #407\n\nA company is using Site-to-Site VPN connections for secure connectivity to its AWS Cloud resources from on premises. Due to an increase in traffic across the\nVPN connections to the Amazon EC2 instances, users are experiencing slower VPN connectivity.\n\nWhich solution will improve the VPN throughput?\n\nA. Implement multiple customer gateways for the same network to scale the throughput.\n\nB. Use a transit gateway with equal cost multipath routing and add additional VPN tunnels.\n\nC. Configure a virtual private gateway with equal cost multipath routing and multiple channels.\n\nD. Increase the number of tunnels in the VPN configuration to scale the throughput beyond the default limit.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #410\n\nA company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to the EBS volumes is encrypted at rest.\n\nWhich solution will meet this requirement?\n\nA. Create an IAM role that specifies EBS encryption. Attach the role to the EC2 instances.\n\nB. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.\n\nC. Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that require encryption at the EBS level.\n\nD. Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption in the account. Ensure that the key policy is active.\n\n<details>\n<summary>Show Answer</summary>\n\n> B \n\n</details>\n\n<br><hr>\n\n### Question #414\n\nA company has a business system that generates hundreds of reports each day. The business system saves the reports to a network share in CSV format. The company needs to store this data in the AWS Cloud in near-real time for analysis.\n\nWhich solution will meet these requirements with the LEAST administrative overhead?\n\nA. Use AWS DataSync to transfer the files to Amazon S3. Create a scheduled task that runs at the end of each day.\n\nB. Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.\n\nC. Use AWS DataSync to transfer the files to Amazon S3. Create an application that uses the DataSync API in the automation workflow.\n\nD. Deploy an AWS Transfer for SFTP endpoint. Create a script that checks for new files on the network share and uploads the new files by using SFTP.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #419\n\nA company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 workloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that prevents any resources from being created in any other Region. A security policy requires the company to encrypt all data at rest.\n\nAn audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) volumes for EC2 instances without encrypting the volumes. The company wants any new EC2 instances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS volumes. The company wants a solution that will have minimal effect on employees who create EBS volumes.\n\nWhich combination of steps will meet these requirements? (Choose two.)\n\nA. In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key.\n\nB. Create an IAM permission boundary. Attach the permission boundary to the root organizational unit (OU). Define the boundary to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.\n\nC. Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.\n\nD. Update the IAM policies for each account to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.\n\nE. In the Organizations management account, specify the Default EBS volume encryption setting.\n\n<details>\n<summary>Show Answer</summary>\n\n> C, E\n\n</details>\n\n<br><hr>\n\n### Question #420\n\nA company wants to use an Amazon RDS for PostgreSQL DB cluster to simplify time-consuming database administrative tasks for production database workloads. The company wants to ensure that its database is highly available and will provide automatic failover support in most scenarios in less than 40 seconds. The company wants to offload reads off of the primary instance and keep costs as low as possible.\n\nWhich solution will meet these requirements?\n\nA. Use an Amazon RDS Multi-AZ DB instance deployment. Create one read replica and point the read workload to the read replica.\n\nB. Use an Amazon RDS Multi-AZ DB duster deployment Create two read replicas and point the read workload to the read replicas.\n\nC. Use an Amazon RDS Multi-AZ DB instance deployment. Point the read workload to the secondary instances in the Multi-AZ pair.\n\nD. Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint.\n\n<details>\n<summary>Show Answer</summary>\n\n> D \n\n</details>\n\n<br><hr>\n\n### Question #434\n\nA company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in anotherAWS Region with minimal downtime.\n\nWhat should a solutions architect do to meet these requirements with the LEAST amount of downtime?\n\nA. Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.\n\nB. Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be launched when needed Configure DNS failover to point to the new disaster recovery Region's load balancer.\n\nC. Create an AWS CloudFormation template to create EC2 instances and a load balancer to be launched when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.\n\nD. Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #439\n\nA solutions architect configured a VPC that has a small range of IP addresses. The number of Amazon EC2 instances that are in the VPC is increasing, and there is an insufficient number of IP addresses for future workloads.\n\nWhich solution resolves this issue with the LEAST operational overhead?\n\nA. Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC. Create new resources in the new subnets by using the new CIDR.\n\nB. Create a second VPC with additional subnets. Use a peering connection to connect the second VPC with the first VPC. Update the routes and create new resources in the subnets of the second VPC.\n\nC. Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first VPC. Update the routes of the transit gateway and VPCs. Create new resources in the subnets of the second VPC.\n\nD. Create a second VPC. Create a Site-to-Site VPN connection between the first VPC and the second VPC by using a VPN-hosted solution on Amazon EC2 and a virtual private gateway. Update the route between VPCs to the traffic through the VPN. Create new resources in the subnets of the second VPC.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #440\n\nA company used an Amazon RDS for MySQL DB instance during application testing. Before terminating the DB instance at the end of the test cycle, a solutions architect created two backups. The solutions architect created the first backup by using the mysqldump utility to create a database dump. The solutions architect created the second backup by enabling the final DB snapshot option on RDS termination.\n\nThe company is now planning for a new test cycle and wants to create a new DB instance from the most recent backup. The company has chosen a MySQL-compatible edition ofAmazon Aurora to host the DB instance.\n\nWhich solutions will create the new DB instance? (Choose two.)\n\nA. Import the RDS snapshot directly into Aurora.\n\nB. Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.\n\nC. Upload the database dump to Amazon S3. Then import the database dump into Aurora.\n\nD. Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora.\n\nE. Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS DMS) to import the database dump into Aurora.\n\n<details>\n<summary>Show Answer</summary>\n\n> A, C\n\n</details>\n\n<br><hr>\n\n### Question #442\n\nA company has two VPCs that are located in the us-west-2 Region within the same AWS account. The company needs to allow network traffic between these\nVPCs. Approximately 500 GB of data transfer will occur between the VPCs each month.\n\nWhat is the MOST cost-effective solution to connect these VPCs?\n\nA. Implement AWS Transit Gateway to connect the VPCs. Update the route tables of each VPC to use the transit gateway for inter-VPC communication.\n\nB. Implement an AWS Site-to-Site VPN tunnel between the VPCs. Update the route tables of each VPC to use the VPN tunnel for inter-VPC communication.\n\nC. Set up a VPC peering connection between the VPCs. Update the route tables of each VPC to use the VPC peering connection for inter-VPC communication.\n\nD. Set up a 1 GB AWS Direct Connect connection between the VPCs. Update the route tables of each VPC to use the Direct Connect connection for inter-VPC communication.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #443\n\nA company wants to host a scalable web application on AWS. The application will be accessed by users from different geographic regions of the world. Application users will be able to download and upload unique data up to gigabytes in size. The development team wants a cost-effective solution to minimize upload and download latency and maximize performance.\n\nWhat should a solutions architect do to accomplish this?\n\nA. Use Amazon S3 with Transfer Acceleration to host the application.\n\nB. Use Amazon S3 with CacheControl headers to host the application.\n\nC. Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.\n\nD. Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #443\n\nA company wants to host a scalable web application on AWS. The application will be accessed by users from different geographic regions of the world. Application users will be able to download and upload unique data up to gigabytes in size. The development team wants a cost-effective solution to minimize upload and download latency and maximize performance.\n\nWhat should a solutions architect do to accomplish this?\n\nA. Use Amazon S3 with Transfer Acceleration to host the application.\n\nB. Use Amazon S3 with CacheControl headers to host the application.\n\nC. Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.\n\nD. Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.\n\n<details>\n<summary>Show Answer</summary>\n\n> C \n\n</details>\n\n<br><hr>\n\n### Question #447\n\nA company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API Gateway. The company wants to deploy the application across multiple AWS Regions to provide Regional failover capabilities.\n\nWhat should a solutions architect do to route traffic to multiple Regions?\n\nA. Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.\n\nB. Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health checks to route traffic.\n\nC. Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region. Configure the transit gateway to route requests.\n\nD. Create an Application Load Balancer in the primary Region. Set the target group to point to the API Gateway endpoint hostnames in each Region.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #448\n\nA company has two VPCs named Management and Production. The Management VPC uses VPNs through a customer gateway to connect to a single device in the data center. The Production VPC uses a virtual private gateway with two attached AWS Direct Connect connections. The Management and Production VPCs both use a single VPC peering connection to allow communication between the applications.\n\nWhat should a solutions architect do to mitigate any single point of failure in this architecture?\n\nA. Add a set of VPNs between the Management and Production VPCs.\n\nB. Add a second virtual private gateway and attach it to the Management VPC.\n\nC. Add a second set of VPNs to the Management VPC from a second customer gateway device.\n\nD. Add a second VPC peering connection between the Management VPC and the Production VPC.\n\n<details>\n<summary>Show Answer</summary>\n\n> C \n\n</details>\n\n<br><hr>\n\n### Question #450\n\nA company has a three-tier web application that is in a single server. The company wants to migrate the application to the AWS Cloud. The company also wants the application to align with the AWS Well-Architected Framework and to be consistent with AWS recommended best practices for security, scalability, and resiliency.\n\nWhich combination of solutions will meet these requirements? (Choose three.)\n\nA. Create a VPC across two Availability Zones with the application's existing architecture. Host the application with existing architecture on an Amazon EC2 instance in a private subnet in each Availability Zone with EC2 Auto Scaling groups. Secure the EC2 instance with security groups and network access control lists (network ACLs).\n\nB. Set up security groups and network access control lists (network ACLs) to control access to the database layer. Set up a single Amazon RDS database in a private subnet.\n\nC. Create a VPC across two Availability Zones. Refactor the application to host the web tier, application tier, and database tier. Host each tier on its own private subnet with Auto Scaling groups for the web tier and application tier.\n\nD. Use a single Amazon RDS database. Allow database access only from the application tier security group.\n\nE. Use Elastic Load Balancers in front of the web tier. Control access by using security groups containing references to each layer's security groups.\n\nF. Use an Amazon RDS database Multi-AZ cluster deployment in private subnets. Allow database access only from application tier security groups.\n\n<details>\n<summary>Show Answer</summary>\n\n> C, E, F \n\n</details>\n\n<br><hr>\n\n### Question #451\n\nA company is migrating its applications and databases to the AWS Cloud. The company will use Amazon Elastic Container Service (Amazon ECS), AWS Direct Connect, and Amazon RDS.\n\nWhich activities will be managed by the company's operational team? (Choose three.)\n\nA. Management of the Amazon RDS infrastructure layer, operating system, and platforms\n\nB. Creation of an Amazon RDS DB instance and configuring the scheduled maintenance window\n\nC. Configuration of additional software components on Amazon ECS for monitoring, patch management, log management, and host intrusion detection\n\nD. Installation of patches for all minor and major database versions for Amazon RDS\n\nE. Ensure the physical security of the Amazon RDS infrastructure in the data center\n\nF. Encryption of the data that moves in transit through Direct Connect\n\n<details>\n<summary>Show Answer</summary>\n\n> B, C, F\n\n</details>\n\n<br><hr>\n\n### Question #455\n\nA company uses AWS Organizations. The company wants to operate some of its AWS accounts with different budgets. The company wants to receive alerts and automatically prevent provisioning of additional resources on AWS accounts when the allocated budget threshold is met during a specific period.\n\nWhich combination of solutions will meet these requirements? (Choose three.)\n\nA. Use AWS Budgets to create a budget. Set the budget amount under the Cost and Usage Reports section of the required AWS accounts.\n\nB. Use AWS Budgets to create a budget. Set the budget amount under the Billing dashboards of the required AWS accounts.\n\nC. Create an IAM user for AWS Budgets to run budget actions with the required permissions.\n\nD. Create an IAM role for AWS Budgets to run budget actions with the required permissions.\n\nE. Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate config rule to prevent provisioning of additional resources.\n\nF. Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate service control policy (SCP) to prevent provisioning of additional resources.\n\n<details>\n<summary>Show Answer</summary>\n\n> B, D, F\n\n</details>\n\n<br><hr>\n\n### Question #457\n\nA company that uses AWS is building an application to transfer data to a product manufacturer. The company has its own identity provider (IdP). The company wants the IdP to authenticate application users while the users use the application to transfer data. The company must use Applicability Statement 2 (AS2) protocol.\n\nWhich solution will meet these requirements?\n\nA. Use AWS DataSync to transfer the data. Create an AWS Lambda function for IdP authentication.\n\nB. Use Amazon AppFlow flows to transfer the data. Create an Amazon Elastic Container Service (Amazon ECS) task for IdP authentication.\n\nC. Use AWS Transfer Family to transfer the data. Create an AWS Lambda function for IdP authentication.\n\nD. Use AWS Storage Gateway to transfer the data. Create an Amazon Cognito identity pool for IdP authentication.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #471\n\nA company is creating an application that runs on containers in a VPC. The application stores and accesses data in an Amazon S3 bucket. During the development phase, the application will store and access 1 TB of data in Amazon S3 each day. The company wants to minimize costs and wants to prevent traffic from traversing the internet whenever possible.\n\nWhich solution will meet these requirements?\n\nA. Enable S3 Intelligent-Tiering for the S3 bucket\n\nB. Enable S3 Transfer Acceleration for the S3 bucket\n\nC. Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in the VPC\n\nD. Create an interface endpoint for Amazon S3 in the VPC. Associate this endpoint with all route tables in the VPC\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #468\n\nA company is developing a microservices application that will provide a search catalog for customers. The company must use REST APIs to present the frontend of the application to users. The REST APIs must access the backend services that the company hosts in containers in private VPC subnets.\n\nWhich solution will meet these requirements?\n\nA. Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.\n\nB. Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.\n\nC. Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS.\n\nD. Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #474\n\nA company has multiple VPCs across AWS Regions to support and run workloads that are isolated from workloads in other Regions. Because of a recent application launch requirement, the company’s VPCs must communicate with all other VPCs across all Regions.\n\nWhich solution will meet these requirements with the LEAST amount of administrative effort?\n\nA. Use VPC peering to manage VPC communication in a single Region. Use VPC peering across Regions to manage VPC communications.\n\nB. Use AWS Direct Connect gateways across all Regions to connect VPCs across regions and manage VPC communications.\n\nC. Use AWS Transit Gateway to manage VPC communication in a single Region and Transit Gateway peering across Regions to manage VPC communications.\n\nD. Use AWS PrivateLink across all Regions to connect VPCs across Regions and manage VPC communications\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #475\n\nA company is designing a containerized application that will use Amazon Elastic Container Service (Amazon ECS). The application needs to access a shared file system that is highly durable and can recover data to another AWS Region with a recovery point objective (RPO) of 8 hours. The file system needs to provide a mount target m each Availability Zone within a Region.\n\nA solutions architect wants to use AWS Backup to manage the replication to another Region.\n\nWhich solution will meet these requirements?\n\nA. Amazon FSx for Windows File Server with a Multi-AZ deployment\nB. Amazon FSx for NetApp ONTAP with a Multi-AZ deployment\nC. Amazon Elastic File System (Amazon EFS) with the Standard storage class\nD. Amazon FSx for OpenZFS\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #476\n\nA company is expecting rapid growth in the near future. A solutions architect needs to configure existing users and grant permissions to new users on AWS. The solutions architect has decided to create IAM groups. The solutions architect will add the new users to IAM groups based on department.\n\nWhich additional action is the MOST secure way to grant permissions to the new users?\n\nA. Apply service control policies (SCPs) to manage access permissions\n\nB. Create IAM roles that have least privilege permission. Attach the roles to the IAM groups\n\nC. Create an IAM policy that grants least privilege permission. Attach the policy to the IAM groups\n\nD. Create IAM roles. Associate the roles with a permissions boundary that defines the maximum permissions\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #479\n\nA company is making a prototype of the infrastructure for its new website by manually provisioning the necessary infrastructure. This infrastructure includes an Auto Scaling group, an Application Load Balancer and an Amazon RDS database. After the configuration has been thoroughly validated, the company wants the capability to immediately deploy the infrastructure for development and production use in two Availability Zones in an automated fashion.\n\nWhat should a solutions architect recommend to meet these requirements?\n\nA. Use AWS Systems Manager to replicate and provision the prototype infrastructure in two Availability Zones\n\nB. Define the infrastructure as a template by using the prototype infrastructure as a guide. Deploy the infrastructure with AWS CloudFormation.\n\nC. Use AWS Config to record the inventory of resources that are used in the prototype infrastructure. Use AWS Config to deploy the prototype infrastructure into two Availability Zones.\n\nD. Use AWS Elastic Beanstalk and configure it to use an automated reference to the prototype infrastructure to automatically deploy new environments in two Availability Zones.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #492\n\nA company has multiple AWS accounts for development work. Some staff consistently use oversized Amazon EC2 instances, which causes the company to exceed the yearly budget for the development accounts. The company wants to centrally restrict the creation of AWS resources in these accounts.\n\nWhich solution will meet these requirements with the LEAST development effort?\n\nA. Develop AWS Systems Manager templates that use an approved EC2 creation process. Use the approved Systems Manager templates to provision EC2 instances.\n\nB. Use AWS Organizations to organize the accounts into organizational units (OUs). Define and attach a service control policy (SCP) to control the usage of EC2 instance types.\n\nC. Configure an Amazon EventBridge rule that invokes an AWS Lambda function when an EC2 instance is created. Stop disallowed EC2 instance types.\n\nD. Set up AWS Service Catalog products for the staff to create the allowed EC2 instance types. Ensure that staff can deploy EC2 instances only by using the Service Catalog products.\n\n<details>\n<summary>Show Answer</summary>\n\n> B\n\n</details>\n\n<br><hr>\n\n### Question #500\n\nA company has multiple Windows file servers on premises. The company wants to migrate and consolidate its files into an Amazon FSx for Windows File Server file system. File permissions must be preserved to ensure that access rights do not change.\n\nWhich solutions will meet these requirements? (Choose two.)\n\nA. Deploy AWS DataSync agents on premises. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.\n\nB. Copy the shares on each file server into Amazon S3 buckets by using the AWS CLI. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.\n\nC. Remove the drives from each file server. Ship the drives to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.\n\nD. Order an AWS Snowcone device. Connect the device to the on-premises network. Launch AWS DataSync agents on the device. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.\n\nE. Order an AWS Snowball Edge Storage Optimized device. Connect the device to the on-premises network. Copy data to the device by using the AWS CLI. Ship the device back to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.\n\n<details>\n<summary>Show Answer</summary>\n\n> A, D\n\n</details>\n\n<br><hr>\n\n### Question #502\n\nA company runs a website that uses a content management system (CMS) on Amazon EC2. The CMS runs on a single EC2 instance and uses an Amazon Aurora MySQL Multi-AZ DB instance for the data tier. Website images are stored on an Amazon Elastic Block Store (Amazon EBS) volume that is mounted inside the EC2 instance.\n\nWhich combination of actions should a solutions architect take to improve the performance and resilience of the website? (Choose two.)\n\nA. Move the website images into an Amazon S3 bucket that is mounted on every EC2 instance\n\nB. Share the website images by using an NFS share from the primary EC2 instance. Mount this share on the other EC2 instances.\n\nC. Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.\n\nD. Create an Amazon Machine Image (AMI) from the existing EC2 instance. Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group. Configure the Auto Scaling group to maintain a minimum of two instances. Configure an accelerator in AWS Global Accelerator for the website\n\nE. Create an Amazon Machine Image (AMI) from the existing EC2 instance. Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group. Configure the Auto Scaling group to maintain a minimum of two instances. Configure an Amazon CloudFront distribution for the website.\n\n<details>\n<summary>Show Answer</summary>\n\n> A, E\n\n</details>\n\n<br><hr>\n\n### Question #504\n\nA company needs to connect several VPCs in the us-east-1 Region that span hundreds of AWS accounts. The company's networking team has its own AWS account to manage the cloud network.\n\nWhat is the MOST operationally efficient solution to connect the VPCs?\n\nA. Set up VPC peering connections between each VPC. Update each associated subnet’s route table\n\nB. Configure a NAT gateway and an internet gateway in each VPC to connect each VPC through the internet\n\nC. Create an AWS Transit Gateway in the networking team’s AWS account. Configure static routes from each VPC.\n\nD. Deploy VPN gateways in each VPC. Create a transit VPC in the networking team’s AWS account to connect to each VPC.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br><hr>\n\n### Question #544\n\nA retail company uses a regional Amazon API Gateway API for its public REST APIs. The API Gateway endpoint is a custom domain name that points to an Amazon Route 53 alias record. A solutions architect needs to create a solution that has minimal effects on customers and minimal data loss to release the new version of APIs.\n\nWhich solution will meet these requirements?\n\nA. Create a canary release deployment stage for API Gateway. Deploy the latest API version. Point an appropriate percentage of traffic to the canary stage. After API verification, promote the canary stage to the production stage.\n\nB. Create a new API Gateway endpoint with a new version of the API in OpenAPI YAML file format. Use the import-to-update operation in merge mode into the API in API Gateway. Deploy the new version of the API to the production stage.\n\nC. Create a new API Gateway endpoint with a new version of the API in OpenAPI JSON file format. Use the import-to-update operation in overwrite mode into the API in API Gateway. Deploy the new version of the API to the production stage.\n\nD. Create a new API Gateway endpoint with new versions of the API definitions. Create a custom domain name for the new API Gateway API. Point the Route 53 alias record to the new API Gateway API custom domain name.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br><hr>\n\n### Question #558\n\nA company has two VPCs that are located in the us-west-2 Region within the same AWS account. The company needs to allow network traffic between these VPCs. Approximately 500 GB of data transfer will occur between the VPCs each month.\n\nWhat is the MOST cost-effective solution to connect these VPCs?\n\nA. Implement AWS Transit Gateway to connect the VPCs. Update the route tables of each VPC to use the transit gateway for inter-VPC communication.\n\nB. Implement an AWS Site-to-Site VPN tunnel between the VPCs. Update the route tables of each VPC to use the VPN tunnel for inter-VPC communication.\n\nC. Set up a VPC peering connection between the VPCs. Update the route tables of each VPC to use the VPC peering connection for inter-VPC communication.\n\nD. Set up a 1 GB AWS Direct Connect connection between the VPCs. Update the route tables of each VPC to use the Direct Connect connection for inter-VPC communication.\n\n<details>\n<summary>Show Answer</summary>\n\n> C\n\n</details>\n\n<br>\n\n### Question #608\n\nA company has an application that serves clients that are deployed in more than 20.000 retail storefront locations around the world. The application consists of backend web services that are exposed over HTTPS on port 443. The application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The retail locations communicate with the web application over the public internet. The company allows each retail location to register the IP address that the retail location has been allocated by its local ISP.\n\nThe company's security team recommends to increase the security of the application endpoint by restricting access to only the IP addresses registered by the retail locations.\n\nWhat should a solutions architect do to meet these requirements?\n\nA. Associate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter traffic. Update the IP addresses in the rule to include the registered IP addresses.\n\nB. Deploy AWS Firewall Manager to manage the ALConfigure firewall rules to restrict traffic to the ALModify the firewall rules to include the registered IP addresses.\n\nC. Store the IP addresses in an Amazon DynamoDB table. Configure an AWS Lambda authorization function on the ALB to validate that incoming requests are from the registered IP addresses.\n\nD. Configure the network ACL on the subnet that contains the public interface of the ALB. Update the ingress rules on the network ACL with entries for each of the registered IP addresses.\n\n<details>\n<summary>Show Answer</summary>\n\n> A\n\n</details>\n\n<br>\n\n## References\n- https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate-saa-c03/","n":0.007}}},{"i":173,"$":{"0":{"v":"Amazon Redshift","n":0.707},"1":{"v":"\n## Amazon Refshift\n\n### What\n- AWS에서 제공되는 완전관리형 데이터 웨어하우스 서비스\n    - 열 지향 데이터 저장 및 고급 쿼리 최적화 기법을 사용하여 빠른 쿼리 성능을 제공; Massively Parallel Processing(MPP) 아키텍처를 사용하여 대량의 데이터를 효과적으로 처리\n- 표준 SQL을 사용하여 데이터를 간단하고 비용 효율적으로 분석할 수 있는 서비스\n- 고성능 BI 보고서, 대시보드, 데이터 탐색 및 분석과 같은 분석 워크로드에 사용\n- AWS 생태계와의 긴밀한 통합으로 인해 다양한 AWS 서비스(S3, Kinesis, Data Pipeline 등)와 원활하게 연동됨\n- AWS의 보안 기능을 이용하여 데이터를 안전하게 보관하며, 암호화, VPC 통합, IAM을 통한 접근 관리 등 다양한 보안 기능을 제공\n- 많은 사용자가 동시에 쿼리를 실행할 때 자동으로 리소스를 확장하여 대응 (Concurrency Scaling)\n\n### How\n#### Redshift로 데이터를 로딩하는 방법\n- Amazon S3에 있는 데이터를 Redshift로 로딩할 때는 Redshift에서 **COPY** 명령 사용\n- Amazon Kinesis로 인입되는 스트리밍 데이터의 경우 **Kinesis Firehose**를 사용하여 스트리밍 데이터를 직접 Redshift로 로드\n    * Kinesis Firehose는 streaming data를 Amazon S3에 스테이지하고, 이를 Redshift에 COPY하는 것임\n- **스트리밍 수집**을 사용하면 지연 시간이 짧고 빠른 속도로 Amazon Kinesis Data Streams에서 Amazon Redshift 구체화된 뷰로 스트림 데이터를 수집할 수 있음\n    * 스트리밍 수집을 사용하면 데이터를 Kinesis Data Streams에서 Amazon Redshift 데이터베이스의 구체화된 뷰로 직접 전송할 수 있으므로 Amazon Kinesis Data Firehose 전송 스트림으로 데이터를 전송할 필요가 없음\n\n## 스트리밍 수집\n### What\n- (준) 실시간 데이터 스트림을 낮은 지연 시간으로 Amazon Redshift에 로딩하기 위한 기능\n\n### How\n- Amazon Redshift 스트리밍 수집을 사용하면 Amazon S3에 데이터를 내리고 클러스터에 적재할 때 발생하는 적재 지연 시간과 복잡성 없이 Kinesis Data Streams에 직접 연결할 수 있음\n- SQL을 사용하여 스트림 데이터에 연결하고 스트림을 기반으로 구체화된 뷰를 생성하므로 데이터 파이프라인이 단순해짐\n    - 구체화된 뷰는 ELT(Extract(추출), Load(적재) 및 Transform(변환)) 파이프라인의 일부로 SQL을 이용한 데이터 변환도 할 수 있음\n- 구체화된 뷰를 새로 고침으로 최신 스트림 데이터를 쿼리할 수 있음\n- Amazon Redshift 스트리밍 수집은 스트림 소비자 역할을 수행\n\n### Where\n#### 스트리밍 수집 사용 사례\n- 실시간에 가까운 분석: 지속적으로 생성(스트리밍)되고 짧은 생성 기간(지연 시간) 내에 처리되어야 하는 데이터 작업\n- 데이터 소스는 다양할 수 있으며 IoT 디바이스, 시스템 원격 측정 데이터 또는 사용량이 많은 웹 사이트 또는 애플리케이션의 클릭스트림 데이터를 포함할 수 있음\n\n\n## Amazon Redshift Spectrum\n- Delta Lake에서 Amazon Redshift를 사용하여 Amazon S3 데이터 레이크의 테이블에 대해 읽기 쿼리를 실행할 수 있음\n- Redshift Spectrum은 ETL을 수행하거나 데이터를 로드할 필요 없이 Redshift, 레이크 하우스, 운영 데이터베이스에서 데이터를 쿼리할 수 있는 레이크 하우스 아키텍처를 지원\n\n## Amazon Redshift ML\n- Amazon Redshift의 데이터로 기계학습 모델 생성 및 예측 가능<br>\n    ㄴ 이전에는 Amazon Redshift에서 Amazon S3 버킷으로 훈련 데이터를 내보낸 다음, 기계학습 모델 훈련 과정을 구성하고 시작해야 했음 (예시: [Amazon SageMaker](https://aws.amazon.com/sagemaker/))\n\n## Azure Synapse Analytics\nAmazon Redshift에 대응되는 Microsoft Azure의 제품은 *Azure Synapse Analytics*\n- Azure Synapse Analytics는 대규모 데이터 웨어하우징 및 빅데이터 분석 솔루션을 제공\n- 사용자는 Petabyte 규모의 데이터에 대한 SQL 쿼리를 빠르게 실행할 수 있음\n- 실시간 분석, 머신러닝, 데이터 탐색과 같은 다양한 기능을 통합하여 제공\n\n\n## Reference\n- [Amazon Redshift : 스트리밍 수집](https://docs.aws.amazon.com/ko_kr/redshift/latest/dg/materialized-view-streaming-ingestion.html)\n- [Amazon Redshift ML 정식 출시 – SQL기반 기계 학습 모델 생성 및 예측 수행 (서울 리전 포함)](https://aws.amazon.com/ko/blogs/korea/amazon-redshift-ml-is-now-generally-available-use-sql-to-create-machine-learning-models-and-make-predictions-from-your-data/)","n":0.047}}},{"i":174,"$":{"0":{"v":"Amazon Redshift의 가격 대비 성능 벤치 마크 결과","n":0.354},"1":{"v":"\n## 가격 대비 성능이 중요한 이유\n- 사용자들은 지속적으로 생산되는 데이터를 분석하기 위해 더욱더 많은 데이터를 데이터 웨어하우스로 가져옴\n- 데이터 웨어하우스에 데이터가 증가할수록 비용이 증가하는 반면 성능은 떨어지게 됨\n- AWS에서는 가격 대비 성능의 비율을 지속적으로 개선하기 위한 활동을 전개하고 있음\n\n## Amazon Redshift의 가격 대비 성능을 개선하기 위한 활동들\n\n1. 낮은 지연시간과 높은 동시성 쿼리 성능 개선\n    - 오늘날의 분석 애플리케이션 트렌드로 사용자들은 낮은 지연시간과 높은 동시성 쿼리를 요구\n\n2. 진화하는 데이터 환경에 따른 새로운 기능들 추가 (추가 비용 없이 사용 가능)\n    - 예: AWS Nitro System을 이용한 동급 대비 최고 하드웨어, AQUA를 이용한 하드웨어 가속, 구체화된 뷰(Materialized view)를 이용하여 더 빠른 실행을 위한 자동 재작성 쿼리(auto-rewriting query), 스키마 최적화를 위한 자동 테이블 최적화 작업(Automatic Table Optimization, ATO), 다이내믹한 동시성과 리소스 활용 최적화를 위한 자동 워크로드 관리(Automatic Workload Management, WLM), 단기 실행 쿼리 가속, 자동 구체화된 보기(Automatic materialized view), 벡터화 및 단일 명령 다중 데이터(single instruction/multiple data, SIMD) 처리와 같은 기능들\n\n## 개선 활동의 결과\n1. 분석 워크로드의 최적화를 통해 경쟁사 대비 최대 8배의 더 나은 성능을 제공\n2. 성능 벤치마크를 실행하여 경쟁사 대비 최고의 가격 대비 성능을 제공하는 것을 확인함\n\n## References\nhttps://aws.amazon.com/ko/blogs/tech/amazon-redshift-continues-its-price-and-performance/\n","n":0.076}}},{"i":175,"$":{"0":{"v":"AWS Lake House","n":0.577},"1":{"v":"\n## AWS 데이터 레이크, 분석 및 ML 서비스\n\n![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2020/12/08/harness-the-power-5.jpg)\n\n### 데이터 저장\n#### **Amazon S3** 및 **Amazon Glacier**\nAmazon S3 및 Amazon Glacier를 사용하여 데이터를 원하는 형식으로 안전하고 대량으로 편리하게 저장할 수 있음\n- Amazon S3는 안전하고 확장이 가능하며 내구성이 우수한 객체 스토리지\n- Amazon Glacier는 데이터 보관 및 백업용 스토리지를 제공하는 온라인 파일 스토리지 웹 서비스\n- Amazon Glue는 최종 사용자가 분석을 위해 사용할 관련 데이터를 쉽게 찾을 수 있도록 사용자가 검색하고 쿼리할 수 있는 단일 카탈로그를 자동으로 생성\n\n### 데이터 통합\n#### **AWS Glue**\n데이터 분석, 기계 학습 및 애플리케이션 개발을 위해 여러 소스에서 데이터를 쉽게 탐색, 준비, 이동 및 통합할 수 있도록 해줌\n- 완전관리형 ETL(추출, 변환 및 로드) 서비스. AWS 관리 콘솔에서 클릭 몇 번으로 ETL 작업을 생성하고 실행할 수 있음\n- 빅데이터 분석 시 다양한 데이터 소스에 대한 전처리 작업을 할 때, 별도의 데이터 처리용 서버나 인프라를 관리할 필요가 없음\n#### **AWS Glue DataBrew**\n데이터 분석가와 데이터 사이언티스트가 손쉽게 데이터를 정리 및 정규화하여 분석 및 기계학습을 위해 준비할 수 있도록 지원하는 시각적 데이터 준비 도구\n- 사전 빌드된 250개 이상의 변환 구성 중에서 선택하여 코드 작성 없이도 데이터 준비 작업을 자동화할 수 있음\n- 이상 항목 필터링, 표준 형식으로 데이터 변환, 잘못된 값 수정, 기타 작업을 자동화할 수 있음\n- 데이터가 준비되면 분석 및 기계 학습 프로젝트에 바로 사용할 수 있음\n\n### **데이터 분석**\n#### **Amazon Athena**\n대화형 분석을 위해 표준 SQL 쿼리를 사용하여 S3 및 Glacier에서 편리하게 데이터를 분석할 수 있도록 해줌\n#### **Amazon EMR**\n방대한 데이터를 편리하고 신속하며 비용 효과적으로 처리할 수 있는 관리형 서비스를 제공\n#### **Amazon Redshift**\n페타바이트 규모의 정형 데이터에 대한 복잡한 분석 쿼리를 실행할 수 있는 기능을 제공\n- 불필요하게 데이터를 이동할 필요 없이 S3에서 엑사바이트 또는 더 작은 크기의 정형 또는 비정형 데이터에 대한 SQL 쿼리를 직접 실행하는 Redshift Spectrum을 포함하고 있음\n#### **Amazon Kinesis**\nIoT 텔레메트리 데이터, 애플리케이션 로그, 웹 사이트 클릭스트림 등과 같은 스트리밍 데이터를 편리하게 수집, 처리, 분석할 수 있도록 해줌\n#### **Amazon Elasticsearch Service**\n애플리케이션 모니터링, 로그 분석 및 클릭스트림 분석과 같은 운영 분석을 위해 사용<br>\n거의 실시간으로 데이터를 검색, 탐색, 필터링, 집계, 시각화할 수 있음\n#### **Amazon QuickSight**\n비즈니스 분석 서비스를 제공<br>\n시각화를 간소화하고, 모든 브라우저나 모바일 기기에서 액세스할 수 있는 강력한 대시보드를 제공\n\n### 예측 분석\n#### **AWS Deep Learning AMI**\nML 및 DL 최적화 GPU 인스턴스를 사용하여 편리하게 딥 러닝 모델을 개발하고 클러스터를 구축할 수 있음\n#### **Amazon SageMaker**\nML을 심층적으로 활용하려는 개발자를 위해 학습 데이터를 연결하고 최상의 알고리즘과 프레임워크를 선택하고 최적화하며 Amazon EC2의 자동 확장 클러스터에 모델을 배포하기 위해 기업에 필요한 모든 것을 제공하여 ML 모델의 구축, 학습, 배포 프로세스를 간소화하는 플랫폼 서비스\n- SageMaker에는 Amazon S3에 저장된 학습 데이터의 편리한 탐색과 시각화를 수행할 수 있도록 해주는 호스팅형 Jupyter 노트북도 포함됨\n- 이미 만들어진 AI 기능을 앱에 연결하려는 개발자를 위해 컴퓨터 비전 및 자연어 처리를 위한 솔루션 지향 API를 제공\n\n## AWS의 Lake House 접근 방식\n\n<img src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2021/04/27/bdb809-build-a-lake-house-4.png\" width=62%>\n\n![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2020/12/08/harness-the-power-4.jpg)\n\n## AWS의 Lake House Architecture\n\n![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2021/04/07/bdb809-build-a-lake-house-2_1.jpg)\n\n![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2021/04/07/bdb809-build-a-lake-house-3_1.jpg)\n\n## Reference\n- https://kr-resources.awscloud.com/data-resource-hub-kr/the-business-value-of-aws-data-lakes-analytics-and-ml-services-kr\n- [Build a Lake House Architecture on AWS](https://aws.amazon.com/ko/blogs/big-data/build-a-lake-house-architecture-on-aws/)\n- [Harness the power of your data with AWS Analytics](https://aws.amazon.com/ko/blogs/big-data/harness-the-power-of-your-data-with-aws-analytics/)\n- [AWS Glue DataBrew로 기술 분석을 위한 데이터 세트 강화하기](https://www.megazone.com/techblog_220303/#)","n":0.047}}},{"i":176,"$":{"0":{"v":"4가지 상호 VPC 연결 방법","n":0.447},"1":{"v":"\n## AWS Transit Gateway\n\n- **Scenario**: A multinational company has multiple VPCs across different regions and accounts, representing different departments and projects. They want a centralized way to manage connectivity among all these VPCs and their on-premises data centers.\n- **Implementation**:\n    * Create a Transit Gateway in the desired region.\n    * Attach all the VPCs to the Transit Gateway.\n    * Update route tables in each VPC to point to the Transit Gateway for inter-VPC communication.\n    * Optionally, connect the Transit Gateway to a VPN or Direct Connect for on-premises connectivity.\n\n## AWS Site-to-Site VPN\n\n- **Scenario**: A company has a VPC in the Asia Pacific (Tokyo) region and another VPC in the EU (Ireland) region. They want to establish secure communication between these VPCs.\n- **Implementation**:\n    * Set up a Virtual Private Gateway (VGW) in each VPC.\n    * Establish a Site-to-Site VPN connection between the two VGWs.\n    * Update route tables in each VPC to route traffic for the other VPC through the VPN connection.\n\n## VPC Peering\n\n- **Scenario**: A startup has two VPCs in the US West (Oregon) region: one for their development environment and one for production. They want to allow the development environment to access a database in the production VPC without exposing it to the internet.\n- **Implementation**:\n    * Initiate a VPC peering connection from the development VPC to the production VPC.\n    * Accept the peering connection in the production VPC.\n    * Update route tables in both VPCs to allow traffic to and from each VPC over the peering connection.\n\n## AWS Direct Connect\n\n- **Scenario**: A financial institution with a data center in New York wants to have a dedicated, low-latency connection to their VPC in the US East (N. Virginia) region for high-frequency trading applications.\n- **Implementation**:\n    * Work with an AWS Direct Connect partner to establish a dedicated line from their data center to an AWS Direct Connect location.\n    * Create a Direct Connect connection in the AWS Management Console.\n    * Set up a Virtual Interface (VIF) to connect to their VPC.\n    * Update the VPC route table to route traffic for the on-premises network over the Direct Connect connection.","n":0.054}}},{"i":177,"$":{"0":{"v":"Elastic Beanstalk","n":0.707},"1":{"v":"\nAWS의 일래스틱 빈스토크(Elastic Beanstalk)를 이용하면 서비스를 쉽게 VM으로 배포할 수 있음\n- 일래스틱 빈스토크는 실행 코드를 WAR 파일로 묶어 업로드하면, 서비스를 부하 분산된 하나 이상의 매니지드(managed) EC2 인스턴스로 배포\n- 자바, 루비, 닷넨 등 다양한 언어에 알맞은 다양한 패키징 포맷을 지원\n- 애플리케이션을 VM으로 배포하지만 AMI[^1]를 빌드하는 것이 아니라, 시동 시 애플리케이션을 설치하는 기초 이미지를 사용함\n- 쿠버네티스만큼 인기 있는 것은 아니지만, 마이크로서비스 애플리케이션을 EC2에 배포하는 간편한 방법 제공\n- 일래스틱 빈스토크는 도커 컨테이너로도 배포할 수 있음\n    - 각 EC2 인스턴스는 하나 이상의 컨테이너를 실행함\n    - 단, 도커 오케스트레이션 프레임워크와 달리, 확장 단위는 컨테이너가 아니라 EC2 인스턴스임\n\n## References\n- 크리스 리처드슨, 마이크로서비스 패턴, 길벗\n\n[^1]: AMI(Amazon Machine Image)","n":0.1}}},{"i":178,"$":{"0":{"v":"AWS의 목적별 데이터베이스","n":0.577},"1":{"v":"\nAWS는 데이터 중심으로 뛰어난 확장성을 가진 분산된 애플리케이션을 구축할 수 있도록 다양한 목적별 데이터베이스 포트폴리오를 제공\n\n이를 통해 신뢰성이 높고 안전한 데이터 인프라스트럭처를 구축할 수 있고 데이터베이스 운영에 들어가는 시간과 비용을 절약하고 규모에 맞게 성능을 개선하여 빠르게 혁신할 수 있음\n\n## AWS의 목적별 데이터베이스\n\n### 관계형 데이터베이스: \n- Aurora\n\t- 대용량 트래픽을 빠른 성능으로 동시에 처리할 수 있는 분산 스토리지 \n\t- 데이터베이스로 MySQL 또는 PostreSQL을 선택할 수 있음\n- RDS\n\t- 오라클, MS-SQL Server 등의 상용 데이터베이스와 MySQL, PostreSQL 등의 오픈소스 데이터베이스를 지원\n\t- 온프레미스에 있는 관계형 데이터베이스를 리프트앤시프로 전환하거나 금융 등 데이터 무결성과 트랜잭션을 보장해야 하는 경우 활용\n### NoSQL\n- DynamoDB\n\t- 초당 수백만의 높은 요청 처리와 10밀리세컨드의 응답 성능을 일정하게 보장\n\t- Key-value data store\n\t- NoSQL 데이터베이스이지만 보조 인덱스 및 ACID 트랜잭션을 제공하여 온라인 쇼핑의 장바구니나 제품 카탈로그 등의 서비스에서 주로 사용될 수 있음\n- DocumentDB\n\t- 문서형 데이터를 효율적으로 저장하고 모든 속성에 대해서 빠른 조회 성능을 제공할 수 있는 MongoDB와 호환성을 가지고 있음\n\t- 스키마리스 DB이므로 개발의 유연성을 최대한 활용할 수 있음\n\t- 초당 수백만의 요청량을 처리할 수 있고 밀리세컨드 수준의 응답시간 제공\n### 인메모리\n- ElasticCache\n\t- Redis, Memcached와 호환되며 마이크로세컨드 수준의 응답시간을 제공\n\t- 자주 사용되는 데이터를 캐싱하는 용도로 많이 사용\n\t- Redis의 경우 다양한 데이터 저장 방식을 제공하는데, Sorted Set를 사용하여 실시간 리더보드 서비스를 손쉽게 추가할 수 있음\n### 그래프\n- Neptune\n\t- 수십억개의 관계를 저장하고 데이터 간의 관계를 손쉽게 쿼리할 수 있게 해줌\n### 시계열\n- Timestream\n\t- IoT 디바이스에서 들어오는 데이터들을 시간 기준으로 빠르게 저장하고 조회할 수 있음\n### 원장\n- QLDB\n\t- 원장 데이터의 변조를 방지하고 모든 변경 이력을 체이닝 방식으로 관리\n3## 카산드라\n- Keyspaces\n\t- 카산드라와 호환성을 가짐\n\n## References\n- [일체형 데이터베이스, 목적에 맞게 MSA 구조로 전환하기 - 윤기원, AWS / 김윤섭 , AWS :: AWS Summit Korea 2022](https://www.youtube.com/watch?v=R4-mC4D_JN4)","n":0.062}}},{"i":179,"$":{"0":{"v":"Amazon CloudFront","n":0.707},"1":{"v":"\n## What\n비즈니스 및 웹 애플리케이션 개발자에게 짧은 지연 시간과 빠른 데이터 전송 속도를 사용하여 콘텐츠를 간편하고 비용 효율적으로 배포할 방법을 제공\n\n## When\n인기 웹 사이트 이미지, 비디오, 미디어 파일, 소프트웨어 다운로드 등 엣지 전송의 이점을 활용할 수 있고 액세스 빈도가 높은 정적 콘텐츠를 배포하는 데 적합\n\n## How\n콘텐츠의 사본을 (일정 시간 동안) 엣지 로케이션에 캐싱 -> 최종 사용자가 콘텐츠를 요청 시 (오리진에 있는 콘텐츠의 원본이 아니라) 사용자와 가장 가까운 엣지 로케이션에 캐시된 콘텐츠를 제공\n\n## Reference\nhttps://aws.amazon.com/ko/cloudfront/faqs/","n":0.117}}},{"i":180,"$":{"0":{"v":"Back-End","n":1}}},{"i":181,"$":{"0":{"v":"Spring Boot","n":0.707},"1":{"v":"\n## 개요\n\n- Java 진영에서 웹 애플리케이션 서버 구축를 위한 프레임워크로 Spring이란 넘이 있는데 그 넘의 lite 버전으로 이해해도 무방\n- Java 언어로 REST Service API를 개발 가능\n- Service API는 Tomcat 서버에 올려서 서비스 (HTTP 통신을 위해)\n- 참고로, 웹 애플리케이션 서버 구축을 위한 프레임워크로는 아래와 같은 것들이 있음; 역시 각각의 기술이 책 한 권 분량; 기술 간에 장단점이 있으나 경험과 지식이 없으니 일단 가장 익숙한 언어로 시작해도 무방 \n  - 전통 강자인 PHP\n  - Spring - Java \n  - Django - Python\n  - Ruby on Rails - Ruby\n  - Express (Node.js) - JavaScript\n\n\n## STS (Spring Tool Suite)\n  - 서버 실행\n    - 초록색 RUN 아이콘 클릭\n  - 프로젝트 생성\n    - http://start.spring.io/\n    - 에서 Spring Boot Project를 생성하고 다운받아 STS에서 연다\n      - STS :: File > Open Project From File System\n  - pom.xml 설정 파일? \n    - 여기에 tomcat을 추가했었나..?\n  - Model과 Controller 구현\n    - Model : CRUD하는 데이터를 담은 Java Object\n      - getter/setter\n    - RestController : RESTful API 구현부\n      - 예제에서는 storage가 아닌 memory로 구현\n      - '@'로 시작하는 이런 저런 annotation을 알아야 할 듯..\n  - application.properties에 로그 관련 config 추가\n \n## Tomcat에 배포\n  - 왼쪽 아래 Servers :: 마우스 오른쪽 > New > Server > Apache > Tomcat v8.5 Server\n  - 기타 설정은 교재 참조 !!\n","n":0.07}}},{"i":182,"$":{"0":{"v":"Node.js","n":1},"1":{"v":"\nhttps://nodejs.org/ko/about/\n\n비동기 이벤트 주도 JavaScript 런타임\n\nnpm init ~ project 생성\n\nnpm install ~ save express multer ~ 패키지 설치\n\nserver.js 생성 + 라우터 설정 - index.html\n\n$node server.js 서버 실행\n\n$express-generator 설치\nnpm install express-generator -g\n\n$express-generator로 express 프로젝트 생성\n\nexpress [appname] --ejs\ncd [appname]\nnpm install\n\n$생성한 프로젝트 실행\nnpm start\n\n$서버에 접속\nhttp://localhost:3000/\n\n$클라이언트 사이드\nviews/index.ejs 수정 - Ajax로 파일 전송\n\n$서버 사이드\nroutes/server.js 생성 후 POST 처리\napp.js에서 server.js router/use 설정","n":0.135}}},{"i":183,"$":{"0":{"v":"Django","n":1}}},{"i":184,"$":{"0":{"v":"Django에서 Tailwind CSS 사용하기","n":0.5},"1":{"v":"\nhttps://django-tailwind.readthedocs.io/en/latest/index.html#","n":1}}},{"i":185,"$":{"0":{"v":"Architecture","n":1}}},{"i":186,"$":{"0":{"v":"Microservices","n":1},"1":{"v":"\nThe term *microservices* is a new way to create applications. You need to see microservices as a way to decompose monolithic applications into different and independent components that follow the twelve-factor app guide.\n\n![](/assets/images/microservices.png)","n":0.174}}},{"i":187,"$":{"0":{"v":"일체형 데이터베이스, 목적에 맞게 MSA 구조로 전환하기","n":0.378},"1":{"v":"\n## 현대적인 애플리케이션의 요구사항\n- 수백만의 트래픽 수용과 밀리 세컨드 단위의 빠른 응답속도를 제공해야 함\n- 유연한 확장이 가능해야 하며 어떤 장애 상황이 오더라도 안전하게 시스템과 데이터를 보호할 수 있어야 함\n- 다양한 고객의 요구사항에 빠르게 대응하고 신속하게 서비스를 배포할 수 있어야 함\n\n## 기존의 모놀리식 아키텍처\n- 여러 서비스들이 오라클로 대표되는 하나의 큰 RDBMS를 메인 DB로 사용하고 또 각 서비스들은 서로 강한 의존성을 가짐\n- 장점\n\t- 관리 포인트가 줄어들고, 시스템을 표준화하기 쉬움\n- 단점\n\t- 개발자들은 새로운 서비스를 추가한다거나 기존의 서비스를 변경하는 것에 큰 어려움을 겪음\n\t- 사용자 요청이 폭증할 경우 이를 처리하기 위한 확장성을 가지는 것이 어려웠음\n\n## 모놀리식 아키텍처의 한계\n- 모놀리식 시스템 구조에서는 애플리케이션이나 DB에서 변경이 있을 경우 그에 따른 영향이 매우 큼\n\t- -> 새로운 개발을 하거나 변경을 할 경우에는 수많은 서비스 담당자들이 모두 함께 작업을 해야 함\n\t- -> 애플리케이션에 새로운 기술을 도입하는 것은 커녕 작은 배포조차 정해진 유지보수 시간에만 가능\n\t- -> 개발자와 운영자들 모두 효율성이 떨어지게 되고, 전체 시스템 관점에서도 기술 부채가 쌓이는 문제가 발생\n\n## 통합된 데이터베이스의 한계\n- 통합된 데이터베이스는 트래픽과 데이터의 증가를 처리하는데 효율적이지 않음\n\t- 전통적인 모놀리식 관계형 데이터베이스는 데이터의 변경 시 정규화된 테이블들 간에 ACID 트랜잭션을 유지해야 하고, 필요로 하는 데이터를 조회하기 위해 여러 테이블들을 조회해야 함\n\t- -> 이런 이유로 트래픽이 증가함에 따라 지속적으로 낮은 응답속도를 보장하기가 쉽지 않음\n\t- 또 폭증하는 데이터를 저장하고 관리하기 위해 데이터베이스의 용량을 확장해야하지만 관계형 데이터베이스의 확장 방식인 수직 확장은 디스크 용량이나 CPU 또는 memory 같은 자원을 추가하는데 많은 시간이 소요되고 확장 가능한 최대 용량의 한계를 가지고 있음\n\n  > 하나의 통합된 데이터베이스는 문제 해결을 위한 적절한 도구 선택의 폭을 제한하게 되고, 이것은 개발자의 생산성을 떨어트려 서비스의 품질을 저해하는 원인이 됨\n\n## 통합된 데이터베이스의 한계\n- 다양하고 빠르게 변화하는 고객의 요구사항을 만족시키기 위해 개발자들은 유연한 개발 환경에서 적절한 도구를 선택하고 애플리케이션을 빠르게 개발하여 대응해야 함\n- 하지만 하나의 통합된 데이터베이스는 문제 해결을 위한 적절한 도구 선택의 폭을 제한하게 되고, 이것은 개발자의 생산성을 떨어트려 서비스의 품질을 저해하는 원인이 됨\n\n    > 워크로드마다 데이터는 다양한 특성을 가지고 있기에, 하나의 통합된 데이터베이스가 다양한 특성의 데이터를 저장하고 그 데이터를 효율적으로 처리할 수는 없음\n\n## 마이크로서비스로의 전환\n- 모놀리식 아키텍처의 문제들을 해결하기 위해서 모놀리식 시스템을 서비스 단위로 분리하는 마이크로서비스 아키텍처가 등장\n- **마이크로서비스 아키텍처**는 서비스 단위로 개발팀을 좀 더 작게 나누고 개발과 운영에 대한 권한과 책임을 위임함으로써 더 빠른 개발과 배포를 하면서도 서비스 간의 영향을 줄일 수 있게 함\n- 마이크로서비스를 도입하게 되면 서로 독립적인 서비스를 구성할 수 있으며, 이를 통해서 개발이나 배포가 용이해지고 시스템의 확장이 용이해짐\n\n    > 마이크로서비스로의 전환을 위해 워크로드의 특성에 맞는 적절한 데이터베이스 선택이 필요 -> 서비스 별로 목적에 맞는 데이터베이스를 잘 선택함으로써 탄력적인 확장성, 빠른 성능, 신뢰할 수 있는 가용성을 확보할 수 있음\n\n## 목적별 데이터베이스 도입\n- 목적별 데이터베이스는 모놀리식 아키텍처의 통합된 데이터베이스의 한계의 현실적이고 효율적인 해결책이 될 수 있음\n\t- 최근 데이터베이스 산업 군에서는 데이터를 특성에 따라 관계형,키-밸류, 다큐먼트, 인메모리, 그래프, 타임시리즈, 원장, 와이드 컬럼 등 8개의 카테고리로 분류\n\t- 각각의 데이터베이스 서비스들도 효율적으로 처리할 수 있는 데이터가 있고 그렇게 하지 못하는 데이터가 있음\n\t- 서비스 별로 목적에 맞는 데이터베이스를 잘 선택함으로써 탄력적인 확장성, 빠른 성능, 신뢰할 수 있는 가용성을 확보할 수 있음\n\n## 목적별 데이터베이스 핸즈온 사례\n아래의 사례들은 기존의 모놀리식 애플리케이션에서 해당 서비스를 분리하고 서비스의 요구사항에 적합한 목적별 데이터베이스를 사용하여 해결 가능\n\n- 리포트\n\t- Oracle과 같은 RDBMS는 그 특성상 리포트나 Document 형식의 데이터를 처리하기 위해서 여러 table들을 조회하거나 조인을 해야 했음 \n\t- -> 여러 테이블간의 Join은 DB 전체의 부하를 크게 증가시키고 개발과 운영의 난이도를 높이게 됨\n- 리더보드\n\t- 일반적으로 관계형 데이터베이스에서는 랭크 함수를 이용하여 리더보드 데이터를 만들게 됨\n\t- -> 데이터가 많을 경우 리더보드를 생성하는 쿼리가 CPU, 디스크, 메모리 등의 많은 시스템 자원을 사용하기 때문에 이 쿼리를 실행할 경우 사용자로부터 유입되는 실시간 요청을 지연시키는 원인이 됨\n\t\t- 그래서 보통은 별도의 서버에 원본 데이터를 복사하고 거기서 리더보드 데이터를 만든 다음 다시 원래 서버로 데이터를 복사하는 방식을 취함. 이런 이유로 실시간 리더보드 서비스를 제공하기가 쉽지 않음.\n- 한정 판매 이벤트\n\t- 특히 한정 판매 이벤트나 주문 시스템 등은 동시에 수많은 사용자들의 요청을 받게 되고, 이에 따라 hot block이 발생할 가능성이 높아지게 됨\n\t- 데이터베이스에서 특정 데이터 블록에 대한 데이터 요청이 늘어나게 되면 hot block[^1]이 발생할 수 있음 -> 사용자 요청 처리를 느리게 만드는 원인이 됨\n\n![[dev.cloud.aws.databases#aws의-목적별-데이터베이스]]\n\n## References\n- [일체형 데이터베이스, 목적에 맞게 MSA 구조로 전환하기 - 윤기원, AWS / 김윤섭 , AWS :: AWS Summit Korea 2022](https://www.youtube.com/watch?v=R4-mC4D_JN4)\n\n---\n\n[^1] Hot block: 데이터베이스 시스템에서 자주 접근되는 데이터 블록","n":0.039}}},{"i":188,"$":{"0":{"v":"마이크로서비스 배포 패턴","n":0.577},"1":{"v":"\n## 1. 언어에 특정한 패키징 포맷 패턴\n언어에 특정한 패키지 형태로 프로덕션에 배포한다.\n\n장점:\n- 배포가 빠름\n- 리소스를 효율적으로 활용할 수 있음(특히 같은 머신이나 같은 프로세스 내에서 여러 인스턴스를 실행할 때)\n\n단점:\n- 기술 스택을 캡슐화할 수 없음\n- 서비스 인스턴스가 소비하는 리소스를 제한할 방법이 없음\n- 여러 서비스 인스턴스가 동일 머신에서 실행될 경우 서로 격리할 수 없음\n- 서비스 인스턴스를 어디에 둘지 자동으로 결정하기 어려움\n\n> 서비스를 언어에 특정한 패키지로 배포하는 방법은 서비스가 아주 소수인 경우를 제외하면 가급적 삼가는 것이 좋습니다.\n\n## 2. 가상 머신 패턴\n서비스를 VM 이미지로 묶어 프로덕션에 배포한다. 각 서비스 인스턴스가 하나의 VM이다.\n\n장점:\n- VM 이미지로 기술 스택을 캡슐화\n- 서비스 인스턴스가 격리됨\n- 성숙한 클라우드 인프라를 활용함\n\n단점:\n- 리소스를 효율적으로 활용할 수 없음\n- 배포가 비교적 느림\n- 시스템 관리 오버헤드가 발생함\n\n> 작고 간단한 애플리케이션이라면 국이 도커 오케스트레이션 프레임워크를 설정하지 말고 가상 머신으로 배포하는 것이 더 쉽습니다.\n\n## 3. 컨테이너 패턴\n서비스를 컨테이너 이미지로 묶어 프로덕션에 배포한다. 각 서비스 인스턴스가 곧 하나의 컨테이너다.\n\n장점:\n- 기술 스택의 캡슐화, 서비스 관리 API가 곧 컨테이너 API가 됨\n- 서비스 인스턴스가 격리됨\n- 서비스 인스턴스의 리소스를 제한할 수 있음\n\n단점:\n- 컨테이너 이미지를 직접 관리해야 하는 부담이 있음\n- OS와 런타임 패치도 정기적으로 해주어야 함\n\n> 어느 정도 서비스 개발이 진행되었다면, 쿠버네티스 같은 정교한 배포 인프라를 구성하는 것을 권장합니다.\n\n## 4. 서버리스 패턴\n퍼블릭 클라우드에서 제공하는 서버리스 배포 메커니즘을 이용하여 서비스를 배포한다.\n\n장점:\n- 다양한 AWS 서비스와의 연계\n- 시스템 관리 업무가 많이 경감됨\n- 탄력성\n- 사용량만큼 과금\n\n단점:\n- 긴-꼬리 지연(long-tail latency)\n- 제한된 이벤트/요청 기반 프로그래밍 모델","n":0.068}}},{"i":189,"$":{"0":{"v":"Data Query Patterns","n":0.577},"1":{"v":"\n## Question\n모놀리식 애플리케이션은 전체 데이터가 하나의 DB에 있기 때문에 SELECT 문으로 여러 테이블을 조인하는 쿼리를 작성할 수 있습니다. 마이크로서비스는 각 서비스마다 각자 DB를 갖는데 각 서비스가 소유한 데이터를 조인하는 쿼리를 어떻게 구현할 수 있을까요?\n\n## Answer\n마이크로서비스 아키텍처에서는 다음 두 가지 패턴으로 쿼리를 구현합니다.\n- API 조합(composition) 패턴: 서비스 클라이언트가 데이터를 가진 여러 서비스를 직접 호출하여 그 결과를 조합하는 패턴입니다.\n- CQRS(Command Query Responsibility Segregation, 커맨드 쿼리 책임 분산) 패턴: 쿼리만 지원하는 하나 이상의 뷰(view) 전용 데이터 레플리카(replica, 사본/복제본)를 유지하는 패턴입니다.\n\n### API 조합 패턴\n#### What\n여러 서비스에 있는 데이터를 API를 통해 조회하고 그 결과를 조합하여 쿼리를 구현합니다.\n\n<div style=\"text-align:center\"><img src=\"https://microservices.io/i/data/ApiBasedQueryBigPicture.png\" /></div>\n\n#### HOW\n데이터를 조합하는 API 조합기는 세 가지 옵션이 있습니다.\n\n첫째, 웹 애플리케이션처럼 웹 페이지에 데이터를 렌더링하는 서비스 클라이언트를 API 조합기로 임명하는 것입니다.\n\n둘째, 애플리케이션의 외부 API가 구현된 API 게이트웨이를 API 조합기로 만드는 것입니다.\n\n셋째, API 조합기를 스탠드얼론 서비스로 구현하는 것입니다.\n\n### CQRS 패턴\n#### What\n여러 서비스에 있는 데이터를 가져오는 쿼리는 (이벤트를 이용하여) 해당 서비스의 데이터를 복제한 읽기 전용 뷰를 유지합니다.\n\n<div style=\"text-align:center\"><img src=\"https://miro.medium.com/v2/resize:fit:3712/1*KtVJ1SVJSQWE9DOafLU8bw.png\"></div><br>\n\n이 패턴은 영속적 데이터 모델과 그것을 사용하는 모듈을 커맨드와 쿼리, 두 편으로 가릅니다. 조회(R) 기능(예: HTTP GET)은 쿼리 쪽 모듈 및 데이터 모델에, 생성/수정/삭제(CUD) 기능(예: HTTP POST, PUT, DELETE)은 커맨드 쪽 모듈 및 데이터 모델에 구현하는 것입니다. 양쪽 데이터 모델 사이의 동기화는 커맨드 쪽에서 발행한 이벤트를 쿼리 쪽에서 구독하는 식으로 이루어집니다.\n\n<div style=\"text-align:center\"><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*60XHay_uTUyV-hwDJBTIug.png\">이미지 출처: https://medium.com/the-legend/cqrs-81b2de39dfd5</div>\n\n### 패턴 선택\nAPI 조합 패턴을 이용하면 여러 서비스에 있는 데이터를 조회하는 쿼리를 쉽게 구현할 수 있습니다. 가급적 이 방법을 쓰는 것이 좋습니다.\n그러나, API 조합 패턴만으로는 효율적으로 구현하기 어려운 (가령 거대한 데이터 뭉치를 인-메모리 조인하는) 다중 서비스 쿼리인 경우 CQRS 패턴을 사용하는 것이 바람직합니다. CQRS 패턴은 API 조합 패턴보다 강력한 만큼 구현하기는 더 복잡합니다.\n\n## Reference\n- 크리스 리처드슨, <마이크로서비스 패턴>, 길벗","n":0.062}}},{"i":190,"$":{"0":{"v":"Event Vs Message","n":0.577},"1":{"v":"\n이벤트와 메시지의 차이는 무엇일까?\n\n## 이벤트 \n- 조건 또는 상태 변경에 대한 경량의 알림 \n- 이벤트 게시자는 이벤트 소비자가 이벤트를 어떻게 처리하는지에 대해 관심이 없음 \n- 예) 스토리지에 파일이 업로드되면 알림 (이때 파일에 대한 일반 정보를 포함할 수 있지만 파일 자체는 포함하지 않음) \n\n## 메시지 \n- 다른 서비스에서 소비하거나 저장할 목적으로 생성된 원시 데이터 \n- 메시지 게시자는 소비자가 메시지를 어떻게 처리할지에 대해 관심이 있음; 즉, 양측 사이에 계약이 존재 \n- 예) 게시자는 원시 데이터가 포함된 메시지를 보내고 소비자가 이 데이터로부터 파일을 만들고 작업이 완료되면 응답을 보낼 것으로 기대\n\n## References\n- https://learn.microsoft.com/ko-kr/azure/event-grid/compare-messaging-services","n":0.106}}},{"i":191,"$":{"0":{"v":"Event Driven","n":0.707},"1":{"v":"\n## 이벤트 기반 아키텍처\n\n이벤트 스트림을 생성하는 이벤트 생산자와 이벤트를 수신 대기하는 이벤트 소비자로 구성됨\n\n![Alt](https://learn.microsoft.com/ko-kr/azure/architecture/guide/architecture-styles/images/event-driven.svg)\n\n### 모델\n- 게시자/구독자 모델 : 이벤트가 게시되면 각 구독자에게 이벤트를 보냄, 이벤트를 받은 후에는 재생 불가\n- 이벤트 스트리밍 : 이벤트가 로그에 저장되며, 클라이언트는 스트림을 재생하는 방식으로 읽음\n\n### 처리 방식\n- 단순 이벤트 처리 : 이벤트가 즉시 소비자에서 작업을 트리거 (예: Azure Service Bus에 메시지가 게시되면 Azure Functions가 실행)\n- 복잡한 이벤트 처리 : 이벤트 데이터에서 패턴을 찾으면서 일련의 이벤트를 처리 (예: 디바이스에서 일정 기간 동안 읽은 값을 집계하여 이동 평균이 특정 임계값을 초과하면 알림 생성)\n- 이벤트 스트림 처리 : 데이터 스트리밍 플랫픔을 사용하여 이벤트를 수집하고 스트림 프로세서에 공급\n\n### 이점\n- 생산자와 소비자가 분리됨\n- 시스템에 새 소비자를 쉽게 추가할 수 있음\n- 이벤트가 도착하는 즉시 소비자가 이벤트에 응답할 수 있음\n- 확장성이 뒤어남\n\n## References\n- https://learn.microsoft.com/ko-kr/azure/architecture/guide/architecture-styles/event-driven","n":0.091}}}]}
