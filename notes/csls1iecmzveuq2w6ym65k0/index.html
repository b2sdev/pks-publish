<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/pks-publish/favicon.ico"/><title>AWS Certified Solutions Architect - Associate (SAA-C03) 오답 노트</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal Knowledge Space"/><meta property="og:title" content="AWS Certified Solutions Architect - Associate (SAA-C03) 오답 노트"/><meta property="og:description" content="Personal Knowledge Space"/><meta property="og:url" content="https://b2sdev.github.io/pks-publish/notes/csls1iecmzveuq2w6ym65k0/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="9/16/2023"/><meta property="article:modified_time" content="10/7/2023"/><link rel="canonical" href="https://b2sdev.github.io/pks-publish/notes/csls1iecmzveuq2w6ym65k0/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/pks-publish/_next/static/css/3646adbe9f4978fb.css" as="style"/><link rel="stylesheet" href="/pks-publish/_next/static/css/3646adbe9f4978fb.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/pks-publish/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/pks-publish/_next/static/chunks/webpack-76e99ab115ade130.js" defer=""></script><script src="/pks-publish/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/pks-publish/_next/static/chunks/main-599200d9a16403bb.js" defer=""></script><script src="/pks-publish/_next/static/chunks/pages/_app-6eff3c921e9661a4.js" defer=""></script><script src="/pks-publish/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/pks-publish/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/pks-publish/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/pks-publish/_next/static/MBpMZiPEoVJLwa6Uvfsj2/_buildManifest.js" defer=""></script><script src="/pks-publish/_next/static/MBpMZiPEoVJLwa6Uvfsj2/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="aws-certified-solutions-architect---associate-saa-c03-오답-노트">AWS Certified Solutions Architect - Associate (SAA-C03) 오답 노트<a aria-hidden="true" class="anchor-heading icon-link" href="#aws-certified-solutions-architect---associate-saa-c03-오답-노트"></a></h1>
<p><code>AWS Certified Solutions Architect - Associate (SAA-C03) Exam Practice Questions</code> 오답 노트</p>
<h2 id="questions">Questions<a aria-hidden="true" class="anchor-heading icon-link" href="#questions"></a></h2>
<hr>
<h3 id="question-3">Question #3<a aria-hidden="true" class="anchor-heading icon-link" href="#question-3"></a></h3>
<p>A company uses AWS Organizations to manage multiple AWS accounts for different departments. The management account has an Amazon S3 bucket that contains project reports. The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations.</p>
<p>Which solution meets these requirements with the LEAST amount of operational overhead?</p>
<p>A. Add the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.</p>
<p>B. Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.</p>
<p>C. Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly.</p>
<p>D. Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-9">Question #9<a aria-hidden="true" class="anchor-heading icon-link" href="#question-9"></a></h3>
<p>A company is running an SMB file server in its data center. The file server stores large files that are accessed frequently for the first few days after the files are created. After 7 days the files are rarely accessed.
The total data size is increasing and is close to the company's total storage capacity. A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.</p>
<p>B. Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.</p>
<p>C. Create an Amazon FSx for Windows File Server file system to extend the company's storage space.</p>
<p>D. Install a utility on each user's computer to access Amazon S3. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-12">Question #12<a aria-hidden="true" class="anchor-heading icon-link" href="#question-12"></a></h3>
<p>A global company hosts its web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The web application has static data and dynamic data. The company stores its static data in an Amazon S3 bucket. The company wants to improve performance and reduce latency for the static data and dynamic data. The company is using its own domain name registered with Amazon Route 53.</p>
<p>What should a solutions architect do to meet these requirements?</p>
<p>A. Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins. Configure Route 53 to route traffic to the CloudFront distribution.</p>
<p>B. Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint Configure Route 53 to route traffic to the CloudFront distribution.</p>
<p>C. Create an Amazon CloudFront distribution that has the S3 bucket as an origin. Create an AWS Global Accelerator standard accelerator that has the ALB and the CloudFront distribution as endpoints. Create a custom domain name that points to the accelerator DNS name. Use the custom domain name as an endpoint for the web application.</p>
<p>D. Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint. Create two domain names. Point one domain name to the CloudFront DNS name for dynamic content. Point the other domain name to the accelerator DNS name for static content. Use the domain names as endpoints for the web application.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-15">Question #15<a aria-hidden="true" class="anchor-heading icon-link" href="#question-15"></a></h3>
<p>A company recently migrated to AWS and wants to implement a solution to protect the traffic that flows in and out of the production VPC. The company had an inspection server in its on-premises data center. The inspection server performed specific operations such as traffic flow inspection and traffic filtering. The company wants to have the same functionalities in the AWS Cloud.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Use Amazon GuardDuty for traffic inspection and traffic filtering in the production VPC.</p>
<p>B. Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering.</p>
<p>C. Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for the production VPC.</p>
<p>D. Use AWS Firewall Manager to create the required rules for traffic inspection and traffic filtering for the production VPC.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-16">Question #16<a aria-hidden="true" class="anchor-heading icon-link" href="#question-16"></a></h3>
<p>A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations. The rest of the company should have only limited access.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.</p>
<p>B. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.</p>
<p>C. Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.</p>
<p>D. Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-19">Question #19<a aria-hidden="true" class="anchor-heading icon-link" href="#question-19"></a></h3>
<p>A company has a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC. The application servers and database servers are deployed in private subnets in the same VPC. The company has deployed a third-party virtual firewall appliance from AWS Marketplace in an inspection VPC. The appliance is configured with an IP interface that can accept IP packets.
A solutions architect needs to integrate the web application with the appliance to inspect all traffic to the application before the traffic reaches the web server.
Which solution will meet these requirements with the LEAST operational overhead?</p>
<p>A. Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.</p>
<p>B. Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.</p>
<p>C. Deploy a transit gateway in the inspection VPConfigure route tables to route the incoming packets through the transit gateway.</p>
<p>D. Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-34">Question #34<a aria-hidden="true" class="anchor-heading icon-link" href="#question-34"></a></h3>
<p>A company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the company must track configuration changes on its AWS resources and record a history of API calls made to these resources.</p>
<p>What should a solutions architect do to meet these requirements?</p>
<p>A. Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.</p>
<p>B. Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.</p>
<p>C. Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.</p>
<p>D. Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-37">Question #37<a aria-hidden="true" class="anchor-heading icon-link" href="#question-37"></a></h3>
<p>A company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The company needs to create a strategy to access and administer the instances remotely and securely. The company needs to implement a repeatable process that works with native AWS services and follows the AWS Well-Architected Framework.</p>
<p>Which solution will meet these requirements with the LEAST operational overhead?</p>
<p>A. Use the EC2 serial console to directly access the terminal interface of each instance for administration.</p>
<p>B. Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager Session Manager to establish a remote SSH session.</p>
<p>C. Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion host in a public subnet to provide a tunnel for administration of each instance.</p>
<p>D. Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises machines to connect directly to the instances by using SSH keys across the VPN tunnel.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote></details>

<p><br></p><hr><p></p>
<h3 id="question-50">Question #50<a aria-hidden="true" class="anchor-heading icon-link" href="#question-50"></a></h3>
<p>A company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The workload is powered by third-party software. The company needs to patch the third-party software on all EC2 instances as quickly as possible to remediate a critical security vulnerability.</p>
<p>What should a solutions architect do to meet these requirements?</p>
<p>A. Create an AWS Lambda function to apply the patch to all EC2 instances.</p>
<p>B. Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.</p>
<p>C. Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.</p>
<p>D. Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-56">Question #56<a aria-hidden="true" class="anchor-heading icon-link" href="#question-56"></a></h3>
<p>A company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway in the ca-central-1 Region as a public interface for its backend microservice APIs. Third-party services consume the APIs securely. The company wants to design its API Gateway URL with the company's domain name and corresponding certificate so that the third-party services can use HTTPS.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Create stage variables in API Gateway with Name="Endpoint-URL" and Value="Company Domain Name" to overwrite the default URL. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM).</p>
<p>B. Create Route 53 DNS records with the company's domain name. Point the alias record to the Regional API Gateway stage endpoint. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region.</p>
<p>C. Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.</p>
<p>D. Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region. Attach the certificate to the API Gateway APIs. Create Route 53 DNS records with the company's domain name. Point an A record to the company's domain name.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-64">Question #64<a aria-hidden="true" class="anchor-heading icon-link" href="#question-64"></a></h3>
<p>A company has more than 5 TB of file data on Windows file servers that run on premises. Users and applications interact with the data each day.
The company is moving its Windows workloads to AWS. As the company continues this process, the company requires access to AWS and on-premises file storage with minimum latency. The company needs a solution that minimizes operational overhead and requires no significant changes to the existing file access patterns. The company uses an AWS Site-to-Site VPN connection for connectivity to AWS.</p>
<p>What should a solutions architect do to meet these requirements?</p>
<p>A. Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data to FSx for Windows File Server. Reconfigure the workloads to use FSx for Windows File Server on AWS.</p>
<p>B. Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to the S3 File Gateway. Reconfigure the on-premises workloads and the cloud workloads to use the S3 File Gateway.</p>
<p>C. Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to Amazon S3. Reconfigure the workloads to use either Amazon S3 directly or the S3 File Gateway. depending on each workload's location.</p>
<p>D. Deploy and configure Amazon FSx for Windows File Server on AWS. Deploy and configure an Amazon FSx File Gateway on premises. Move the on-premises file data to the FSx File Gateway. Configure the cloud workloads to use FSx for Windows File Server on AWS. Configure the on-premises workloads to use the FSx File Gateway.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-66">Question #66<a aria-hidden="true" class="anchor-heading icon-link" href="#question-66"></a></h3>
<p>A company has an application that generates a large number of files, each approximately 5 MB in size. The files are stored in Amazon S3. Company policy requires the files to be stored for 4 years before they can be deleted. Immediate accessibility is always required as the files contain critical business data that is not easy to reproduce. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days.</p>
<p>Which storage solution is MOST cost-effective?</p>
<p>A. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Glacier 30 days from object creation. Delete the files 4 years after object creation.</p>
<p>B. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days from object creation. Delete the files 4 years after object creation.</p>
<p>C. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Delete the files 4 years after object creation.</p>
<p>D. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Move the files to S3 Glacier 4 years after object creation.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-68">Question #68<a aria-hidden="true" class="anchor-heading icon-link" href="#question-68"></a></h3>
<p>A solutions architect is designing a new hybrid architecture to extend a company's on-premises infrastructure to AWS. The company requires a highly available connection with consistent low latency to an AWS Region. The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails.</p>
<p>What should the solutions architect do to meet these requirements?</p>
<p>A. Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the primary Direct Connect connection fails.</p>
<p>B. Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN tunnel for private connectivity and as a backup if the primary VPN connection fails.</p>
<p>C. Provision an AWS Direct Connect connection to a Region. Provision a second Direct Connect connection to the same Region as a backup if the primary Direct Connect connection fails.</p>
<p>D. Provision an AWS Direct Connect connection to a Region. Use the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-71">Question #71<a aria-hidden="true" class="anchor-heading icon-link" href="#question-71"></a></h3>
<p>A company runs a shopping application that uses Amazon DynamoDB to store customer information. In case of data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour.</p>
<p>What should the solutions architect recommend to meet these requirements?</p>
<p>A. Configure DynamoDB global tables. For RPO recovery, point the application to a different AWS Region.</p>
<p>B. Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.</p>
<p>C. Export the DynamoDB data to Amazon S3 Glacier on a daily basis. For RPO recovery, import the data from S3 Glacier to DynamoDB.</p>
<p>D. Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the DynamoDB table by using the EBS snapshot.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B </p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-82">Question #82<a aria-hidden="true" class="anchor-heading icon-link" href="#question-82"></a></h3>
<p>A company hosts its web applications in the AWS Cloud. The company configures Elastic Load Balancers to use certificates that are imported into AWS Certificate Manager (ACM). The company's security team must be notified 30 days before the expiration of each certificate.</p>
<p>What should a solutions architect recommend to meet this requirement?</p>
<p>A. Add a rule in ACM to publish a custom message to an Amazon Simple Notification Service (Amazon SNS) topic every day, beginning 30 days before any certificate will expire.</p>
<p>B. Create an AWS Config rule that checks for certificates that will expire within 30 days. Configure Amazon EventBridge (Amazon CloudWatch Events) to invoke a custom alert by way of Amazon Simple Notification Service (Amazon SNS) when AWS Config reports a noncompliant resource.</p>
<p>C. Use AWS Trusted Advisor to check for certificates that will expire within 30 days. Create an Amazon CloudWatch alarm that is based on Trusted Advisor metrics for check status changes. Configure the alarm to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS).</p>
<p>D. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect any certificates that will expire within 30 days. Configure the rule to invoke an AWS Lambda function. Configure the Lambda function to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS).</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-83">Question #83<a aria-hidden="true" class="anchor-heading icon-link" href="#question-83"></a></h3>
<p>A company's dynamic website is hosted using on-premises servers in the United States. The company is launching its product in Europe, and it wants to optimize site loading times for new European users. The site's backend must remain in the United States. The product is being launched in a few days, and an immediate solution is needed.</p>
<p>What should the solutions architect recommend?</p>
<p>A. Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.</p>
<p>B. Move the website to Amazon S3. Use Cross-Region Replication between Regions.</p>
<p>C. Use Amazon CloudFront with a custom origin pointing to the on-premises servers.</p>
<p>D. Use an Amazon Route 53 geoproximity routing policy pointing to on-premises servers.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-84">Question #84<a aria-hidden="true" class="anchor-heading icon-link" href="#question-84"></a></h3>
<p>A company wants to reduce the cost of its existing three-tier web architecture. The web, application, and database servers are running on Amazon EC2 instances for the development, test, and production environments. The EC2 instances average 30% CPU utilization during peak hours and 10% CPU utilization during non-peak hours.</p>
<p>The production EC2 instances run 24 hours a day. The development and test EC2 instances run for at least 8 hours each day. The company plans to implement automation to stop the development and test EC2 instances when they are not in use.</p>
<p>Which EC2 instance purchasing solution will meet the company's requirements MOST cost-effectively?</p>
<p>A. Use Spot Instances for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.</p>
<p>B. Use Reserved Instances for the production EC2 instances. Use On-Demand Instances for the development and test EC2 instances.</p>
<p>C. Use Spot blocks for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.</p>
<p>D. Use On-Demand Instances for the production EC2 instances. Use Spot blocks for the development and test EC2 instances.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-88">Question #88<a aria-hidden="true" class="anchor-heading icon-link" href="#question-88"></a></h3>
<p>A survey company has gathered data for several years from areas in the United States. The company hosts the data in an Amazon S3 bucket that is 3 TB in size and growing. The company has started to share the data with a European marketing firm that has S3 buckets. The company wants to ensure that its data transfer costs remain as low as possible.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Configure the Requester Pays feature on the company's S3 bucket.</p>
<p>B. Configure S3 Cross-Region Replication from the company's S3 bucket to one of the marketing firm's S3 buckets.</p>
<p>C. Configure cross-account access for the marketing firm so that the marketing firm has access to the company's S3 bucket.</p>
<p>D. Configure the company's S3 bucket to use S3 Intelligent-Tiering. Sync the S3 bucket to one of the marketing firm's S3 buckets.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-93">Question #93<a aria-hidden="true" class="anchor-heading icon-link" href="#question-93"></a></h3>
<p>A company runs an on-premises application that is powered by a MySQL database. The company is migrating the application to AWS to increase the application's elasticity and availability.</p>
<p>The current architecture shows heavy read activity on the database during times of normal operation. Every 4 hours, the company's development team pulls a full export of the production database to populate a database in the staging environment. During this period, users experience unacceptable application latency. The development team is unable to use the staging environment until the procedure completes.</p>
<p>A solutions architect must recommend replacement architecture that alleviates the application latency issue. The replacement architecture also must give the development team the ability to continue using the staging environment without delay.</p>
<p>Which solution meets these requirements?</p>
<p>A. Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility.</p>
<p>B. Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to create the staging database on-demand.</p>
<p>C. Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Use the standby instance for the staging database.</p>
<p>D. Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-101">Question #101<a aria-hidden="true" class="anchor-heading icon-link" href="#question-101"></a></h3>
<p>A solutions architect is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates.</p>
<p>What should the solutions architect do to enable Internet access for the private subnets?</p>
<p>A. Create three NAT gateways, one for each public subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT gateway in its AZ.</p>
<p>B. Create three NAT instances, one for each private subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT instance in its AZ.</p>
<p>C. Create a second internet gateway on one of the private subnets. Update the route table for the private subnets that forward non-VPC traffic to the private internet gateway.</p>
<p>D. Create an egress-only internet gateway on one of the public subnets. Update the route table for the private subnets that forward non-VPC traffic to the egress-only Internet gateway.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A </p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-113">Question #113<a aria-hidden="true" class="anchor-heading icon-link" href="#question-113"></a></h3>
<p>A company uses 50 TB of data for reporting. The company wants to move this data from on premises to AWS. A custom application in the company’s data center runs a weekly data transformation job. The company plans to pause the application until the data transfer is complete and needs to begin the transfer process as soon as possible.</p>
<p>The data center does not have any available network bandwidth for additional workloads. A solutions architect must transfer the data and must configure the transformation job to continue to run in the AWS Cloud.</p>
<p>Which solution will meet these requirements with the LEAST operational overhead?</p>
<p>A. Use AWS DataSync to move the data. Create a custom transformation job by using AWS Glue.</p>
<p>B. Order an AWS Snowcone device to move the data. Deploy the transformation application to the device.</p>
<p>C. Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue.</p>
<p>D. Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data to the device. Create a new EC2 instance on AWS to run the transformation application.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-116">Question #116<a aria-hidden="true" class="anchor-heading icon-link" href="#question-116"></a></h3>
<p>A company uses a popular content management system (CMS) for its corporate website. However, the required patching and maintenance are burdensome. The company is redesigning its website and wants a new solution. The website will be updated four times a year and does not need to have any dynamic content available. The solution must provide high scalability and enhanced security.</p>
<p>Which combination of changes will meet these requirements with the LEAST operational overhead? (Choose two.)</p>
<p>A. Configure Amazon CloudFront in front of the website to use HTTPS functionality.</p>
<p>B. Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality.</p>
<p>C. Create and deploy an AWS Lambda function to manage and serve the website content.</p>
<p>D. Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.</p>
<p>E. Create the new website. Deploy the website by using an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A, D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-119">Question #119<a aria-hidden="true" class="anchor-heading icon-link" href="#question-119"></a></h3>
<p>A global company is using Amazon API Gateway to design REST APIs for its loyalty club users in the us-east-1 Region and the ap-southeast-2 Region. A solutions architect must design a solution to protect these API Gateway managed REST APIs across multiple accounts from SQL injection and cross-site scripting attacks.</p>
<p>Which solution will meet these requirements with the LEAST amount of administrative effort?</p>
<p>A. Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage.</p>
<p>B. Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules.</p>
<p>C. Set up AWS Shield in bath Regions. Associate Regional web ACLs with an API stage.</p>
<p>D. Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-120">Question #120<a aria-hidden="true" class="anchor-heading icon-link" href="#question-120"></a></h3>
<p>A company has implemented a self-managed DNS solution on three Amazon EC2 instances behind a Network Load Balancer (NLB) in the us-west-2 Region. Most of the company's users are located in the United States and Europe. The company wants to improve the performance and availability of the solution. The company launches and configures three EC2 instances in the eu-west-1 Region and adds the EC2 instances as targets for a new NLB.</p>
<p>Which solution can the company use to route traffic to all the EC2 instances?</p>
<p>A. Create an Amazon Route 53 geolocation routing policy to route requests to one of the two NLBs. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution’s origin.</p>
<p>B. Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in us-west-2 and eu-west-1. Add the two NLBs as endpoints for the endpoint groups.</p>
<p>C. Attach Elastic IP addresses to the six EC2 instances. Create an Amazon Route 53 geolocation routing policy to route requests to one of the six EC2 instances. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution's origin.</p>
<p>D. Replace the two NLBs with two Application Load Balancers (ALBs). Create an Amazon Route 53 latency routing policy to route requests to one of the two ALBs. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution’s origin.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-125">Question #125<a aria-hidden="true" class="anchor-heading icon-link" href="#question-125"></a></h3>
<p>A company runs its two-tier ecommerce website on AWS. The web tier consists of a load balancer that sends traffic to Amazon EC2 instances. The database tier uses an Amazon RDS DB instance. The EC2 instances and the RDS DB instance should not be exposed to the public internet. The EC2 instances require internet access to complete payment processing of orders through a third-party web service. The application must be highly available.</p>
<p>Which combination of configuration options will meet these requirements? (Choose two.)</p>
<p>A. Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-AZ DB instance in private subnets.</p>
<p>B. Configure a VPC with two private subnets and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the private subnets.</p>
<p>C. Use an Auto Scaling group to launch the EC2 instances in public subnets across two Availability Zones. Deploy an RDS Multi-AZ DB instance in private subnets.</p>
<p>D. Configure a VPC with one public subnet, one private subnet, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnet.</p>
<p>E. Configure a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnets.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A, D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-129">Question #129<a aria-hidden="true" class="anchor-heading icon-link" href="#question-129"></a></h3>
<p>A company is running a multi-tier web application on premises. The web application is containerized and runs on a number of Linux hosts connected to a PostgreSQL database that contains user records. The operational overhead of maintaining the infrastructure and capacity planning is limiting the company's growth. A solutions architect must improve the application's infrastructure.</p>
<p>Which combination of actions should the solutions architect take to accomplish this? (Choose two.)</p>
<p>A. Migrate the PostgreSQL database to Amazon Aurora.</p>
<p>B. Migrate the web application to be hosted on Amazon EC2 instances.</p>
<p>C. Set up an Amazon CloudFront distribution for the web application content.</p>
<p>D. Set up Amazon ElastiCache between the web application and the PostgreSQL database.</p>
<p>E. Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS).</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A, E</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-134">Question #134<a aria-hidden="true" class="anchor-heading icon-link" href="#question-134"></a></h3>
<p>A company wants to move its application to a serverless solution. The serverless solution needs to analyze existing and new data by using SL. The company stores the data in an Amazon S3 bucket. The data requires encryption and must be replicated to a different AWS Region.</p>
<p>Which solution will meet these requirements with the LEAST operational overhead?</p>
<p>A. Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region kays (SSE-KMS). Use Amazon Athena to query the data.</p>
<p>B. Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region keys (SSE-KMS). Use Amazon RDS to query the data.</p>
<p>C. Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon Athena to query the data.</p>
<p>D. Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon RDS to query the data.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-139">Question #139<a aria-hidden="true" class="anchor-heading icon-link" href="#question-139"></a></h3>
<p>A reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon QuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket.</p>
<p>The reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3 bucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker Pipelines.</p>
<p>What should a solutions architect do to meet these requirements with the LEAST operational overhead?</p>
<p>A. Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.</p>
<p>B. Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.</p>
<p>C. Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.</p>
<p>D. Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>D </p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-140">Question #140<a aria-hidden="true" class="anchor-heading icon-link" href="#question-140"></a></h3>
<p>A solutions architect needs to help a company optimize the cost of running an application on AWS. The application will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for compute within the architecture.</p>
<p>The EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic and unpredictable. Workloads that run on EC2 instances can be interrupted at any time. The application front end will run on Fargate, and Lambda will serve the API layer. The front-end utilization and API layer utilization will be predictable over the course of the next year.</p>
<p>Which combination of purchasing options will provide the MOST cost-effective solution for hosting this application? (Choose two.)</p>
<p>A. Use Spot Instances for the data ingestion layer</p>
<p>B. Use On-Demand Instances for the data ingestion layer</p>
<p>C. Purchase a 1-year Compute Savings Plan for the front end and API layer.</p>
<p>D. Purchase 1-year All Upfront Reserved instances for the data ingestion layer.</p>
<p>E. Purchase a 1-year EC2 instance Savings Plan for the front end and API layer.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A, C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-143">Question #143<a aria-hidden="true" class="anchor-heading icon-link" href="#question-143"></a></h3>
<p>A company wants to migrate its existing on-premises monolithic application to AWS. The company wants to keep as much of the front-end code and the backend code as possible. However, the company wants to break the application into smaller applications. A different team will manage each application. The company needs a highly scalable solution that minimizes operational overhead.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Host the application on AWS Lambda. Integrate the application with Amazon API Gateway.</p>
<p>B. Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API that is integrated with AWS Lambda.</p>
<p>C. Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2 instances in an Auto Scaling group as targets.</p>
<p>D. Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-144">Question #144<a aria-hidden="true" class="anchor-heading icon-link" href="#question-144"></a></h3>
<p>A company recently started using Amazon Aurora as the data store for its global ecommerce application. When large reports are run, developers report that the ecommerce application is performing poorly. After reviewing metrics in Amazon CloudWatch, a solutions architect finds that the ReadIOPS and CPUUtilizalion metrics are spiking when monthly reports run.</p>
<p>What is the MOST cost-effective solution?</p>
<p>A. Migrate the monthly reporting to Amazon Redshift.</p>
<p>B. Migrate the monthly reporting to an Aurora Replica.</p>
<p>C. Migrate the Aurora database to a larger instance class.</p>
<p>D. Increase the Provisioned IOPS on the Aurora instance.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-145">Question #145<a aria-hidden="true" class="anchor-heading icon-link" href="#question-145"></a></h3>
<p>A company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. The analytics software is written in PHP and uses a MySQL database. The analytics software, the web server that provides PHP, and the database server are all hosted on the EC2 instance. The application is showing signs of performance degradation during busy times and is presenting 5xx errors. The company needs to make the application scale seamlessly.</p>
<p>Which solution will meet these requirements MOST cost-effectively?</p>
<p>A. Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load to each EC2 instance.</p>
<p>B. Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances.</p>
<p>C. Migrate the database to an Amazon Aurora MySQL DB instance. Create an AWS Lambda function to stop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization surpasses 75%.</p>
<p>D. Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web application. Apply the AMI to a launch template. Create an Auto Scaling group with the launch template Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-150">Question #150<a aria-hidden="true" class="anchor-heading icon-link" href="#question-150"></a></h3>
<p>A company is migrating an application from on-premises servers to Amazon EC2 instances. As part of the migration design requirements, a solutions architect must implement infrastructure metric alarms. The company does not need to take action if CPU utilization increases to more than 50% for a short burst of time. However, if the CPU utilization increases to more than 50% and read IOPS on the disk are high at the same time, the company needs to act as soon as possible. The solutions architect also must reduce false alarms.</p>
<p>What should the solutions architect do to meet these requirements?</p>
<p>A. Create Amazon CloudWatch composite alarms where possible.</p>
<p>B. Create Amazon CloudWatch dashboards to visualize the metrics and react to issues quickly.</p>
<p>C. Create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm.</p>
<p>D. Create single Amazon CloudWatch metric alarms with multiple metric thresholds where possible.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-151">Question #151<a aria-hidden="true" class="anchor-heading icon-link" href="#question-151"></a></h3>
<p>A company wants to migrate its on-premises data center to AWS. According to the company's compliance requirements, the company can use only the ap-northeast-3 Region. Company administrators are not permitted to connect VPCs to the internet.</p>
<p>Which solutions will meet these requirements? (Choose two.)</p>
<p>A. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3.</p>
<p>B. Use rules in AWS WAF to prevent internet access. Deny access to all AWS Regions except ap-northeast-3 in the AWS account settings.</p>
<p>C. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.</p>
<p>D. Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0. Create an IAM policy for each user to prevent the use of any AWS Region other than ap-northeast-3.</p>
<p>E. Use AWS Config to activate managed rules to detect and alert for internet gateways and to detect and alert for new resources deployed outside of ap-northeast-3.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A, C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-157">Question #157<a aria-hidden="true" class="anchor-heading icon-link" href="#question-157"></a></h3>
<p>A company stores data in an Amazon Aurora PostgreSQL DB cluster. The company must store all the data for 5 years and must delete all the data after 5 years. The company also must indefinitely keep audit logs of actions that are performed within the database. Currently, the company has automated backups configured for Aurora.</p>
<p>Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)</p>
<p>A. Take a manual snapshot of the DB cluster.</p>
<p>B. Create a lifecycle policy for the automated backups.</p>
<p>C. Configure automated backup retention for 5 years.</p>
<p>D. Configure an Amazon CloudWatch Logs export for the DB cluster.</p>
<p>E. Use AWS Backup to take the backups and to keep the backups for 5 years.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>D, E</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-158">Question #158<a aria-hidden="true" class="anchor-heading icon-link" href="#question-158"></a></h3>
<p>A solutions architect is optimizing a website for an upcoming musical event. Videos of the performances will be streamed in real time and then will be available on demand. The event is expected to attract a global online audience.</p>
<p>Which service will improve the performance of both the real-time and on-demand streaming?</p>
<p>A. Amazon CloudFront</p>
<p>B. AWS Global Accelerator</p>
<p>C. Amazon Route 53</p>
<p>D. Amazon S3 Transfer Acceleration</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-159">Question #159<a aria-hidden="true" class="anchor-heading icon-link" href="#question-159"></a></h3>
<p>A company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS Lambda. The application’s traffic recently spiked due to fraudulent requests from botnets.</p>
<p>Which steps should a solutions architect take to block requests from unauthorized users? (Choose two.)</p>
<p>A. Create a usage plan with an API key that is shared with genuine users only.</p>
<p>B. Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses.</p>
<p>C. Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.</p>
<p>D. Convert the existing public API to a private API. Update the DNS records to redirect users to the new API endpoint.</p>
<p>E. Create an IAM role for each user attempting to access the API. A user will assume the role when making the API call.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A, C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-172">Question #172<a aria-hidden="true" class="anchor-heading icon-link" href="#question-172"></a></h3>
<p>A solutions architect is creating a new Amazon CloudFront distribution for an application. Some of the information submitted by users is sensitive. The application uses HTTPS but needs another layer of security. The sensitive information should be protected throughout the entire application stack, and access to the information should be restricted to certain applications.</p>
<p>Which action should the solutions architect take?</p>
<p>A. Configure a CloudFront signed URL.</p>
<p>B. Configure a CloudFront signed cookie.</p>
<p>C. Configure a CloudFront field-level encryption profile.</p>
<p>D. Configure CloudFront and set the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol Policy.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-182">Question #182<a aria-hidden="true" class="anchor-heading icon-link" href="#question-182"></a></h3>
<p>A company wants to migrate its MySQL database from on premises to AWS. The company recently experienced a database outage that significantly impacted the business. To ensure this does not happen again, the company wants a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes.</p>
<p>Which solution meets these requirements?</p>
<p>A. Create an Amazon RDS DB instance with synchronous replication to three nodes in three Availability Zones.</p>
<p>B. Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data.</p>
<p>C. Create an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data.</p>
<p>D. Create an Amazon EC2 instance with a MySQL engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-184">Question #184<a aria-hidden="true" class="anchor-heading icon-link" href="#question-184"></a></h3>
<p>A company has an AWS account used for software engineering. The AWS account has access to the company’s on-premises data center through a pair of AWS Direct Connect connections. All non-VPC traffic routes to the virtual private gateway.</p>
<p>A development team recently created an AWS Lambda function through the console. The development team needs to allow the function to access a database that runs in a private subnet in the company’s data center.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Configure the Lambda function to run in the VPC with the appropriate security group.</p>
<p>B. Set up a VPN connection from AWS to the data center. Route the traffic from the Lambda function through the VPN.</p>
<p>C. Update the route tables in the VPC to allow the Lambda function to access the on-premises data center through Direct Connect.</p>
<p>D. Create an Elastic IP address. Configure the Lambda function to send traffic through the Elastic IP address without an elastic network interface.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-188">Question #188<a aria-hidden="true" class="anchor-heading icon-link" href="#question-188"></a></h3>
<p>A company uses Amazon S3 as its data lake. The company has a new partner that must use SFTP to upload data files. A solutions architect needs to implement a highly available SFTP solution that minimizes operational overhead.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible endpoint. Choose the S3 data lake as the destination.</p>
<p>B. Use Amazon S3 File Gateway as an SFTP server. Expose the S3 File Gateway endpoint URL to the new partner. Share the S3 File Gateway endpoint with the new partner.</p>
<p>C. Launch an Amazon EC2 instance in a private subnet in a VPInstruct the new partner to upload files to the EC2 instance by using a VPN. Run a cron job script, on the EC2 instance to upload files to the S3 data lake.</p>
<p>D. Launch Amazon EC2 instances in a private subnet in a VPC. Place a Network Load Balancer (NLB) in front of the EC2 instances. Create an SFTP listener port for the NLB. Share the NLB hostname with the new partner. Run a cron job script on the EC2 instances to upload files to the S3 data lake.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-189">Question #189<a aria-hidden="true" class="anchor-heading icon-link" href="#question-189"></a></h3>
<p>A company needs to store contract documents. A contract lasts for 5 years. During the 5-year period, the company must ensure that the documents cannot be overwritten or deleted. The company needs to encrypt the documents at rest and rotate the encryption keys automatically every year.</p>
<p>Which combination of steps should a solutions architect take to meet these requirements with the LEAST operational overhead? (Choose two.)</p>
<p>A. Store the documents in Amazon S3. Use S3 Object Lock in governance mode.</p>
<p>B. Store the documents in Amazon S3. Use S3 Object Lock in compliance mode.</p>
<p>C. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure key rotation.</p>
<p>D. Use server-side encryption with AWS Key Management Service (AWS KMS) customer managed keys. Configure key rotation.</p>
<p>E. Use server-side encryption with AWS Key Management Service (AWS KMS) customer provided (imported) keys. Configure key rotation.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B, D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-190">Question #190<a aria-hidden="true" class="anchor-heading icon-link" href="#question-190"></a></h3>
<p>A company has a web application that is based on Java and PHP. The company plans to move the application from on premises to AWS. The company needs the ability to test new site features frequently. The company also needs a highly available and managed solution that requires minimum operational overhead.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Create an Amazon S3 bucket. Enable static web hosting on the S3 bucket. Upload the static content to the S3 bucket. Use AWS Lambda to process all dynamic content.</p>
<p>B. Deploy the web application to an AWS Elastic Beanstalk environment. Use URL swapping to switch between multiple Elastic Beanstalk environments for feature testing.</p>
<p>C. Deploy the web application to Amazon EC2 instances that are configured with Java and PHP. Use Auto Scaling groups and an Application Load Balancer to manage the website’s availability.</p>
<p>D. Containerize the web application. Deploy the web application to Amazon EC2 instances. Use the AWS Load Balancer Controller to dynamically route traffic between containers that contain the new site features for testing.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-194">Question #194<a aria-hidden="true" class="anchor-heading icon-link" href="#question-194"></a></h3>
<p>A company needs to run a critical application on AWS. The company needs to use Amazon EC2 for the application’s database. The database must be highly available and must fail over automatically if a disruptive event occurs.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Set up database replication.</p>
<p>B. Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use AWS CloudFormation to automate provisioning of the EC2 instance if a disruptive event occurs.</p>
<p>C. Launch two EC2 instances, each in a different AWS Region. Install the database on both EC2 instances. Set up database replication. Fail over the database to a second Region.</p>
<p>D. Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use EC2 automatic recovery to recover the instance if a disruptive event occurs.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-201">Question #201<a aria-hidden="true" class="anchor-heading icon-link" href="#question-201"></a></h3>
<p>A company is developing a marketing communications service that targets mobile app users. The company needs to send confirmation messages with Short Message Service (SMS) to its users. The users must be able to reply to the SMS messages. The company must store the responses for a year for analysis.</p>
<p>What should a solutions architect do to meet these requirements?</p>
<p>A. Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to process the responses.</p>
<p>B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.</p>
<p>C. Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda to process the responses.</p>
<p>D. Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-208">Question #208<a aria-hidden="true" class="anchor-heading icon-link" href="#question-208"></a></h3>
<p>A company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must ensure that no API calls and no data are routed through public internet routes. Only the EC2 instance can have access to upload data to the S3 bucket.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.</p>
<p>B. Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is located. Attach appropriate security groups to the endpoint. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.</p>
<p>C. Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3 bucket’s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.</p>
<p>D. Use the AWS provided, publicly available ip-ranges.json file to obtain the private IP address of the S3 bucket’s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-215">Question #215<a aria-hidden="true" class="anchor-heading icon-link" href="#question-215"></a></h3>
<p>A company has 700 TB of backup data stored in network attached storage (NAS) in its data center. This backup data need to be accessible for infrequent regulatory requests and must be retained 7 years. The company has decided to migrate this backup data from its data center to AWS. The migration must be complete within 1 month. The company has 500 Mbps of dedicated bandwidth on its public internet connection available for data transfer.</p>
<p>What should a solutions architect do to migrate and store the data at the LOWEST cost?</p>
<p>A. Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.</p>
<p>B. Deploy a VPN connection between the data center and Amazon VPC. Use the AWS CLI to copy the data from on premises to Amazon S3 Glacier.</p>
<p>C. Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.</p>
<p>D. Use AWS DataSync to transfer the data and deploy a DataSync agent on premises. Use the DataSync task to copy files from the on-premises NAS storage to Amazon S3 Glacier.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-217">Question #217<a aria-hidden="true" class="anchor-heading icon-link" href="#question-217"></a></h3>
<p>A company runs a global web application on Amazon EC2 instances behind an Application Load Balancer. The application stores data in Amazon Aurora. The company needs to create a disaster recovery solution and can tolerate up to 30 minutes of downtime and potential data loss. The solution does not need to handle the load when the primary infrastructure is healthy.</p>
<p>What should a solutions architect do to meet these requirements?</p>
<p>A. Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in a second AWS Region.</p>
<p>B. Host a scaled-down deployment of the application in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora Replica in the second Region.</p>
<p>C. Replicate the primary infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora database that is restored from the latest snapshot.</p>
<p>D. Back up data with AWS Backup. Use the backup to create the required infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-passive failover. Create an Aurora second primary instance in the second Region.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-218">Question #218<a aria-hidden="true" class="anchor-heading icon-link" href="#question-218"></a></h3>
<p>A company has a web server running on an Amazon EC2 instance in a public subnet with an Elastic IP address. The default security group is assigned to the EC2 instance. The default network ACL has been modified to block all traffic. A solutions architect needs to make the web server accessible from everywhere on port 443.</p>
<p>Which combination of steps will accomplish this task? (Choose two.)</p>
<p>A. Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.</p>
<p>B. Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0.</p>
<p>C. Update the network ACL to allow TCP port 443 from source 0.0.0.0/0.</p>
<p>D. Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0.</p>
<p>E. Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A, E </p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-219">Question #219<a aria-hidden="true" class="anchor-heading icon-link" href="#question-219"></a></h3>
<p>A company’s application is having performance issues. The application is stateful and needs to complete in-memory tasks on Amazon EC2 instances. The company used AWS CloudFormation to deploy infrastructure and used the M5 EC2 instance family. As traffic increased, the application performance degraded. Users are reporting delays when the users attempt to access the application.</p>
<p>Which solution will resolve these issues in the MOST operationally efficient way?</p>
<p>A. Replace the EC2 instances with T3 EC2 instances that run in an Auto Scaling group. Make the changes by using the AWS Management Console.</p>
<p>B. Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group. Increase the desired capacity and the maximum capacity of the Auto Scaling group manually when an increase is necessary.</p>
<p>C. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon CloudWatch built-in EC2 memory metrics to track the application performance for future capacity planning.</p>
<p>D. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-223">Question #223<a aria-hidden="true" class="anchor-heading icon-link" href="#question-223"></a></h3>
<p>A company has deployed a Java Spring Boot application as a pod that runs on Amazon Elastic Kubernetes Service (Amazon EKS) in private subnets. The application needs to write data to an Amazon DynamoDB table. A solutions architect must ensure that the application can interact with the DynamoDB table without exposing traffic to the internet.</p>
<p>Which combination of steps should the solutions architect take to accomplish this goal? (Choose two.)</p>
<p>A. Attach an IAM role that has sufficient privileges to the EKS pod.</p>
<p>B. Attach an IAM user that has sufficient privileges to the EKS pod.</p>
<p>C. Allow outbound connectivity to the DynamoDB table through the private subnets’ network ACLs.</p>
<p>D. Create a VPC endpoint for DynamoDB.</p>
<p>E. Embed the access keys in the Java Spring Boot code.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A, D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-224">Question #224<a aria-hidden="true" class="anchor-heading icon-link" href="#question-224"></a></h3>
<p>A company recently migrated its web application to AWS by rehosting the application on Amazon EC2 instances in a single AWS Region. The company wants to redesign its application architecture to be highly available and fault tolerant. Traffic must reach all running EC2 instances randomly.</p>
<p>Which combination of steps should the company take to meet these requirements? (Choose two.)</p>
<p>A. Create an Amazon Route 53 failover routing policy.</p>
<p>B. Create an Amazon Route 53 weighted routing policy.</p>
<p>C. Create an Amazon Route 53 multivalue answer routing policy.</p>
<p>D. Launch three EC2 instances: two instances in one Availability Zone and one instance in another Availability Zone.</p>
<p>E. Launch four EC2 instances: two instances in one Availability Zone and two instances in another Availability Zone.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C, E</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-225">Question #225<a aria-hidden="true" class="anchor-heading icon-link" href="#question-225"></a></h3>
<p>A media company collects and analyzes user activity data on premises. The company wants to migrate this capability to AWS. The user activity data store will continue to grow and will be petabytes in size. The company needs to build a highly available data ingestion solution that facilitates on-demand analytics of existing data and new data with SQL.</p>
<p>Which solution will meet these requirements with the LEAST operational overhead?</p>
<p>A. Send activity data to an Amazon Kinesis data stream. Configure the stream to deliver the data to an Amazon S3 bucket.</p>
<p>B. Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver the data to an Amazon Redshift cluster.</p>
<p>C. Place activity data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on the data as the data arrives in the S3 bucket.</p>
<p>D. Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability Zones. Configure the service to forward data to an Amazon RDS Multi-AZ database.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-233">Question #233<a aria-hidden="true" class="anchor-heading icon-link" href="#question-233"></a></h3>
<p>A company has three VPCs named Development, Testing, and Production in the us-east-1 Region. The three VPCs need to be connected to an on-premises data center and are designed to be separate to maintain security and prevent any resource sharing. A solutions architect needs to find a scalable and secure solution.</p>
<p>What should the solutions architect recommend?</p>
<p>A. Create an AWS Direct Connect connection and a VPN connection for each VPC to connect back to the data center.</p>
<p>B. Create VPC peers from all the VPCs to the Production VPC. Use an AWS Direct Connect connection from the Production VPC back to the data center.</p>
<p>C. Connect VPN connections from all the VPCs to a VPN in the Production VPC. Use a VPN connection from the Production VPC back to the data center.</p>
<p>D. Create a new VPC called Network. Within the Network VPC, create an AWS Transit Gateway with an AWS Direct Connect connection back to the data center. Attach all the other VPCs to the Network VPC.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-234">Question #234<a aria-hidden="true" class="anchor-heading icon-link" href="#question-234"></a></h3>
<p>A company is building a new web-based customer relationship management application. The application will use several Amazon EC2 instances that are backed by Amazon Elastic Block Store (Amazon EBS) volumes behind an Application Load Balancer (ALB). The application will also use an Amazon Aurora database. All data for the application must be encrypted at rest and in transit.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Use AWS Key Management Service (AWS KMS) certificates on the ALB to encrypt data in transit. Use AWS Certificate Manager (ACM) to encrypt the EBS volumes and Aurora database storage at rest.</p>
<p>B. Use the AWS root account to log in to the AWS Management Console. Upload the company’s encryption certificates. While in the root account, select the option to turn on encryption for all data at rest and in transit for the account.</p>
<p>C. Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database storage at rest. Attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in transit.</p>
<p>D. Use BitLocker to encrypt all data at rest. Import the company’s TLS certificate keys to AWS Key Management Service (AWS KMS) Attach the KMS keys to the ALB to encrypt data in transit.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-266">Question #266<a aria-hidden="true" class="anchor-heading icon-link" href="#question-266"></a></h3>
<p>A company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups configured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect traffic to healthy endpoints.</p>
<p>Which solution meets these requirements?</p>
<p>A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.</p>
<p>B. Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.</p>
<p>C. Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.</p>
<p>D. Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-276">Question #276<a aria-hidden="true" class="anchor-heading icon-link" href="#question-276"></a></h3>
<p>A company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An Amazon RDS for Oracle instance is the application’s data layer that uses Oracle-specific PL/SQL functions. Traffic to the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the RDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and defines the minimum healthy instance count only. The company predicts that traffic will continue to increase at a steady but unpredictable rate before leveling off.</p>
<p>What should a solutions architect do to ensure the system can automatically scale for the increased traffic? (Choose two.)</p>
<p>A. Configure storage Auto Scaling on the RDS for Oracle instance.</p>
<p>B. Migrate the database to Amazon Aurora to use Auto Scaling storage.</p>
<p>C. Configure an alarm on the RDS for Oracle instance for low free storage space.</p>
<p>D. Configure the Auto Scaling group to use the average CPU as the scaling metric.</p>
<p>E. Configure the Auto Scaling group to use the average free memory as the scaling metric.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A, D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-287">Question #287<a aria-hidden="true" class="anchor-heading icon-link" href="#question-287"></a></h3>
<p>A company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company wants to use specific features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers.</p>
<p>How should a solutions architect design the architecture to meet these requirements?</p>
<p>A. Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.</p>
<p>B. Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.</p>
<p>C. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.</p>
<p>D. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-295">Question #295<a aria-hidden="true" class="anchor-heading icon-link" href="#question-295"></a></h3>
<p>An ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally identifiable information (PII). The company wants to use the data in three applications. Only one of the applications needs to process the PII. The PII must be removed before the other two applications process the data.</p>
<p>Which solution will meet these requirements with the LEAST operational overhead?</p>
<p>A. Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and process the data that each application requests.</p>
<p>B. Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.</p>
<p>C. Process the data and store the transformed data in three separate Amazon S3 buckets so that each application has its own custom dataset. Point each application to its respective S3 bucket.</p>
<p>D. Process the data and store the transformed data in three separate Amazon DynamoDB tables so that each application has its own custom dataset. Point each application to its respective DynamoDB table.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-327">Question #327<a aria-hidden="true" class="anchor-heading icon-link" href="#question-327"></a></h3>
<p>A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party’s URL. Other internet traffic must be blocked.</p>
<p>Which solution meets these requirements?</p>
<p>A. Update the route table for the private subnet to route the outbound traffic to an AWS Network Firewall firewall. Configure domain list rule groups.</p>
<p>B. Set up an AWS WAF web ACL. Create a custom set of rules that filter traffic requests based on source and destination IP address range sets.</p>
<p>C. Implement strict inbound security group rules. Configure an outbound rule that allows traffic only to the authorized software repositories on the internet by specifying the URLs.</p>
<p>D. Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound traffic to the ALB. Use a URL-based rule listener in the ALB’s target group for outbound access to the internet.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-328">Question #328<a aria-hidden="true" class="anchor-heading icon-link" href="#question-328"></a></h3>
<p>A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously.</p>
<p>The company is expecting a significant and sudden increase in the number of sales requests during events for the launch of new products.</p>
<p>What should a solutions architect recommend to ensure that all the requests are processed successfully?</p>
<p>A. Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in traffic.</p>
<p>B. Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network traffic.</p>
<p>C. Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce traffic for the API to handle.</p>
<p>D. Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-333">Question #333<a aria-hidden="true" class="anchor-heading icon-link" href="#question-333"></a></h3>
<p>A company’s application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of every month at midnight, the application becomes much slower when the month-end financial calculation batch runs. This causes the CPU utilization of the EC2 instances to immediately peak to 100%, which disrupts the application.</p>
<p>What should a solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?</p>
<p>A. Configure an Amazon CloudFront distribution in front of the ALB.</p>
<p>B. Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.</p>
<p>C. Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.</p>
<p>D. Configure Amazon ElastiCache to remove some of the workload from the EC2 instances.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-335">Question #335<a aria-hidden="true" class="anchor-heading icon-link" href="#question-335"></a></h3>
<p>A company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand.</p>
<p>Which solution meets these requirements?</p>
<p>A. Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step Functions to replace the AMI in the Auto Scaling group.</p>
<p>B. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.</p>
<p>C. Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM). Create an AWS Lambda function that modifies the AMI in the Auto Scaling group.</p>
<p>D. Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure Auto Scaling group capacity limits as an event source in EventBridge.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-342">Question #342<a aria-hidden="true" class="anchor-heading icon-link" href="#question-342"></a></h3>
<p>A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 instances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, but the baseline CPU utilization that is noted on each run is at least 60%. The company needs to provision the capacity 30 minutes before the jobs run.</p>
<p>Currently, engineers complete this task by manually modifying the Auto Scaling group parameters. The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts. The company needs an automated way to modify the Auto Scaling group’s desired capacity.</p>
<p>Which solution will meet these requirements with the LEAST operational overhead?</p>
<p>A. Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based on the CPU utilization metric. Set the target value for the metric to 60%.</p>
<p>B. Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired capacity, minimum capacity, and maximum capacity. Set the recurrence to weekly. Set the start time to 30 minutes before the batch jobs run.</p>
<p>C. Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based on forecast. Set the scaling metric to CPU utilization. Set the target value for the metric to 60%. In the policy, set the instances to pre-launch 30 minutes before the jobs run.</p>
<p>D. Create an Amazon EventBridge event to invoke an AWS Lambda function when the CPU utilization metric value for the Auto Scaling group reaches 60%. Configure the Lambda function to increase the Auto Scaling group’s desired capacity and maximum capacity by 20%.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-344">Question #344<a aria-hidden="true" class="anchor-heading icon-link" href="#question-344"></a></h3>
<p>A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB.</p>
<p>Which solution will meet these requirements with the FEWEST changes to the code?</p>
<p>A. Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.</p>
<p>B. Use Amazon EventBridge to post large messages from the application instead of Amazon SQS.</p>
<p>C. Change the limit in Amazon SQS to handle messages that are larger than 256 KB.</p>
<p>D. Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS). Configure Amazon SQS to reference this location in the messages.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-346">Question #346<a aria-hidden="true" class="anchor-heading icon-link" href="#question-346"></a></h3>
<p>A company has an aging network-attached storage (NAS) array in its data center. The NAS array presents SMB shares and NFS shares to client workstations. The company does not want to purchase a new NAS array. The company also does not want to incur the cost of renewing the NAS array’s support contract. Some of the data is accessed frequently, but much of the data is inactive.</p>
<p>A solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 Lifecycle policies, and maintains the same look and feel for the client workstations. The solutions architect has identified AWS Storage Gateway as part of the solution.</p>
<p>Which type of storage gateway should the solutions architect provision to meet these requirements?</p>
<p>A. Volume Gateway</p>
<p>B. Tape Gateway</p>
<p>C. Amazon FSx File Gateway</p>
<p>D. Amazon S3 File Gateway</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-349">Question #349<a aria-hidden="true" class="anchor-heading icon-link" href="#question-349"></a></h3>
<p>A company stores confidential data in an Amazon Aurora PostgreSQL database in the ap-southeast-3 Region. The database is encrypted with an AWS Key Management Service (AWS KMS) customer managed key. The company was recently acquired and must securely share a backup of the database with the acquiring company’s AWS account in ap-southeast-3.</p>
<p>What should a solutions architect do to meet these requirements?</p>
<p>A. Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new snapshot with the acquiring company’s AWS account.</p>
<p>B. Create a database snapshot. Add the acquiring company’s AWS account to the KMS key policy. Share the snapshot with the acquiring company’s AWS account.</p>
<p>C. Create a database snapshot that uses a different AWS managed KMS key. Add the acquiring company’s AWS account to the KMS key alias. Share the snapshot with the acquiring company's AWS account.</p>
<p>D. Create a database snapshot. Download the database snapshot. Upload the database snapshot to an Amazon S3 bucket. Update the S3 bucket policy to allow access from the acquiring company’s AWS account.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-353">Question #353<a aria-hidden="true" class="anchor-heading icon-link" href="#question-353"></a></h3>
<p>A company hosts a three-tier web application on Amazon EC2 instances in a single Availability Zone. The web application uses a self-managed MySQL database that is hosted on an EC2 instance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL database currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects traffic of 1,000 IOPS for both reads and writes at peak traffic.</p>
<p>The company wants to minimize any disruptions, stabilize performance, and reduce costs while retaining the capacity for double the IOPS. The company wants to move the database tier to a fully managed solution that is highly available and fault tolerant.</p>
<p>Which solution will meet these requirements MOST cost-effectively?</p>
<p>A. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block Express EBS volume.</p>
<p>B. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.</p>
<p>C. Use Amazon S3 Intelligent-Tiering access tiers.</p>
<p>D. Use two large EC2 instances to host the database in active-passive mode.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-363">Question #363<a aria-hidden="true" class="anchor-heading icon-link" href="#question-363"></a></h3>
<p>A company is building a game system that needs to send unique events to separate leaderboard, matchmaking, and authentication services concurrently. The company needs an AWS event-driven system that guarantees the order of the events.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Amazon EventBridge event bus</p>
<p>B. Amazon Simple Notification Service (Amazon SNS) FIFO topics</p>
<p>C. Amazon Simple Notification Service (Amazon SNS) standard topics</p>
<p>D. Amazon Simple Queue Service (Amazon SQS) FIFO queues</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-364">Question #364<a aria-hidden="true" class="anchor-heading icon-link" href="#question-364"></a></h3>
<p>A hospital is designing a new application that gathers symptoms from patients. The hospital has decided to use Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) in the architecture.</p>
<p>A solutions architect is reviewing the infrastructure design. Data must be encrypted at rest and in transit. Only authorized personnel of the hospital should be able to access the data.</p>
<p>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)</p>
<p>A. Turn on server-side encryption on the SQS components. Update the default key policy to restrict key usage to a set of authorized principals.</p>
<p>B. Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals.</p>
<p>C. Turn on encryption on the SNS components. Update the default key policy to restrict key usage to a set of authorized principals. Set a condition in the topic policy to allow only encrypted connections over TLS.</p>
<p>D. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.</p>
<p>E. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply an IAM policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B, D </p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-366">Question #366<a aria-hidden="true" class="anchor-heading icon-link" href="#question-366"></a></h3>
<p>A company’s web application consists of an Amazon API Gateway API in front of an AWS Lambda function and an Amazon DynamoDB database. The Lambda function handles the business logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito user pools to identify the individual users of the application. A solutions architect needs to update the application so that only users who have a subscription can access premium content.</p>
<p>Which solution will meet this requirement with the LEAST operational overhead?</p>
<p>A. Enable API caching and throttling on the API Gateway API.</p>
<p>B. Set up AWS WAF on the API Gateway API. Create a rule to filter users who have a subscription.</p>
<p>C. Apply fine-grained IAM permissions to the premium content in the DynamoDB table.</p>
<p>D. Implement API usage plans and API keys to limit the access of users who do not have a subscription.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-369">Question #369<a aria-hidden="true" class="anchor-heading icon-link" href="#question-369"></a></h3>
<p>A company has migrated an application to Amazon EC2 Linux instances. One of these EC2 instances runs several 1-hour tasks on a schedule. These tasks were written by different teams and have no common programming language. The company is concerned about performance and scalability while these tasks run on a single instance. A solutions architect needs to implement a solution to resolve these concerns.</p>
<p>Which solution will meet these requirements with the LEAST operational overhead?</p>
<p>A. Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events).</p>
<p>B. Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to run the tasks as jobs.</p>
<p>C. Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events).</p>
<p>D. Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto Scaling group with the AMI to run multiple copies of the instance.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-370">Question #370<a aria-hidden="true" class="anchor-heading icon-link" href="#question-370"></a></h3>
<p>A company runs a public three-tier web application in a VPC. The application runs on Amazon EC2 instances across multiple Availability Zones. The EC2 instances that run in private subnets need to communicate with a license server over the internet. The company needs a managed solution that minimizes operational maintenance.</p>
<p>Which solution meets these requirements?</p>
<p>A. Provision a NAT instance in a public subnet. Modify each private subnet's route table with a default route that points to the NAT instance.</p>
<p>B. Provision a NAT instance in a private subnet. Modify each private subnet's route table with a default route that points to the NAT instance.</p>
<p>C. Provision a NAT gateway in a public subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.</p>
<p>D. Provision a NAT gateway in a private subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-371">Question #371<a aria-hidden="true" class="anchor-heading icon-link" href="#question-371"></a></h3>
<p>A company needs to create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host a digital media streaming application. The EKS cluster will use a managed node group that is backed by Amazon Elastic Block Store (Amazon EBS) volumes for storage. The company must encrypt all data at rest by using a customer managed key that is stored in AWS Key Management Service (AWS KMS).</p>
<p>Which combination of actions will meet this requirement with the LEAST operational overhead? (Choose two.)</p>
<p>A. Use a Kubernetes plugin that uses the customer managed key to perform data encryption.</p>
<p>B. After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the customer managed key.</p>
<p>C. Enable EBS encryption by default in the AWS Region where the EKS cluster will be created. Select the customer managed key as the default key.</p>
<p>D. Create the EKS cluster. Create an IAM role that has a policy that grants permission to the customer managed key. Associate the role with the EKS cluster.</p>
<p>E. Store the customer managed key as a Kubernetes secret in the EKS cluster. Use the customer managed key to encrypt the EBS volumes.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C, D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-379">Question #379<a aria-hidden="true" class="anchor-heading icon-link" href="#question-379"></a></h3>
<p>A company hosts a frontend application that uses an Amazon API Gateway API backend that is integrated with AWS Lambda. When the API receives requests, the Lambda function loads many libraries. Then the Lambda function connects to an Amazon RDS database, processes the data, and returns the data to the frontend application. The company wants to ensure that response latency is as low as possible for all its users with the fewest number of changes to the company's operations.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Establish a connection between the frontend application and the database to make queries faster by bypassing the API.</p>
<p>B. Configure provisioned concurrency for the Lambda function that handles the requests.</p>
<p>C. Cache the results of the queries in Amazon S3 for faster retrieval of similar datasets.</p>
<p>D. Increase the size of the database to increase the number of connections Lambda can establish at one time.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-385">Question #385<a aria-hidden="true" class="anchor-heading icon-link" href="#question-385"></a></h3>
<p>A solutions architect is creating a new VPC design. There are two public subnets for the load balancer, two private subnets for web servers, and two private subnets for MySQL. The web servers use only HTTPS. The solutions architect has already created a security group for the load balancer allowing port 443 from 0.0.0.0/0. Company policy requires that each resource has the least access required to still be able to perform its tasks.</p>
<p>Which additional configuration strategy should the solutions architect use to meet these requirements?</p>
<p>A. Create a security group for the web servers and allow port 443 from 0.0.0.0/0. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.</p>
<p>B. Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.</p>
<p>C. Create a security group for the web servers and allow port 443 from the load balancer. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.</p>
<p>D. Create a network ACL for the web servers and allow port 443 from the load balancer. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-388">Question #388<a aria-hidden="true" class="anchor-heading icon-link" href="#question-388"></a></h3>
<p>A company is deploying a two-tier web application in a VPC. The web tier is using an Amazon EC2 Auto Scaling group with public subnets that span multiple Availability Zones. The database tier consists of an Amazon RDS for MySQL DB instance in separate private subnets. The web tier requires access to the database to retrieve product information.</p>
<p>The web application is not working as intended. The web application reports that it cannot connect to the database. The database is confirmed to be up and running. All configurations for the network ACLs, security groups, and route tables are still in their default states.</p>
<p>What should a solutions architect recommend to fix the application?</p>
<p>A. Add an explicit rule to the private subnet’s network ACL to allow traffic from the web tier’s EC2 instances.</p>
<p>B. Add a route in the VPC route table to allow traffic between the web tier’s EC2 instances and the database tier.</p>
<p>C. Deploy the web tier's EC2 instances and the database tier’s RDS instance into two separate VPCs, and configure VPC peering.</p>
<p>D. Add an inbound rule to the security group of the database tier’s RDS instance to allow traffic from the web tiers security group.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-390">Question #390<a aria-hidden="true" class="anchor-heading icon-link" href="#question-390"></a></h3>
<p>A company hosts a three-tier ecommerce application on a fleet of Amazon EC2 instances. The instances run in an Auto Scaling group behind an Application Load Balancer (ALB). All ecommerce data is stored in an Amazon RDS for MariaDB Multi-AZ DB instance.</p>
<p>The company wants to optimize customer session management during transactions. The application must store session data durably.</p>
<p>Which solutions will meet these requirements? (Choose two.)</p>
<p>A. Turn on the sticky sessions feature (session affinity) on the ALB.</p>
<p>B. Use an Amazon DynamoDB table to store customer session information.</p>
<p>C. Deploy an Amazon Cognito user pool to manage user session information.</p>
<p>D. Deploy an Amazon ElastiCache for Redis cluster to store customer session information.</p>
<p>E. Use AWS Systems Manager Application Manager in the application to manage user session information.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A, D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-391">Question #391<a aria-hidden="true" class="anchor-heading icon-link" href="#question-391"></a></h3>
<p>A company needs a backup strategy for its three-tier stateless web application. The web application runs on Amazon EC2 instances in an Auto Scaling group with a dynamic scaling policy that is configured to respond to scaling events. The database tier runs on Amazon RDS for PostgreSQL. The web application does not require temporary local storage on the EC2 instances. The company’s recovery point objective (RPO) is 2 hours.</p>
<p>The backup strategy must maximize scalability and optimize resource utilization for this environment.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and database every 2 hours to meet the RPO.</p>
<p>B. Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS) snapshots. Enable automated backups in Amazon RDS to meet the RPO.</p>
<p>C. Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.</p>
<p>D. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances every 2 hours. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-399">Question #399<a aria-hidden="true" class="anchor-heading icon-link" href="#question-399"></a></h3>
<p>A financial company hosts a web application on AWS. The application uses an Amazon API Gateway Regional API endpoint to give users the ability to retrieve current stock prices. The company’s security team has noticed an increase in the number of API requests. The security team is concerned that HTTP flood attacks might take the application offline.</p>
<p>A solutions architect must design a solution to protect the application from this type of attack.</p>
<p>Which solution meets these requirements with the LEAST operational overhead?</p>
<p>A. Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24 hours.</p>
<p>B. Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.</p>
<p>C. Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when the predefined rate is reached.</p>
<p>D. Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway Regional API endpoint. Create an AWS Lambda function to block requests from IP addresses that exceed the predefined rate.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-402">Question #402<a aria-hidden="true" class="anchor-heading icon-link" href="#question-402"></a></h3>
<p>A company needs to ingest and handle large amounts of streaming data that its application generates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is configured with default settings. Every other day, the application consumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the data that the application sends to Kinesis Data Streams.</p>
<p>What should a solutions architect do to resolve this issue?</p>
<p>A. Update the Kinesis Data Streams default settings by modifying the data retention period.</p>
<p>B. Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.</p>
<p>C. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.</p>
<p>D. Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-406">Question #406<a aria-hidden="true" class="anchor-heading icon-link" href="#question-406"></a></h3>
<p>A solutions architect is designing a two-tiered architecture that includes a public subnet and a database subnet. The web servers in the public subnet must be open to the internet on port 443. The Amazon RDS for MySQL DB instance in the database subnet must be accessible only to the web servers on port 3306.</p>
<p>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)</p>
<p>A. Create a network ACL for the public subnet. Add a rule to deny outbound traffic to 0.0.0.0/0 on port 3306.</p>
<p>B. Create a security group for the DB instance. Add a rule to allow traffic from the public subnet CIDR block on port 3306.</p>
<p>C. Create a security group for the web servers in the public subnet. Add a rule to allow traffic from 0.0.0.0/0 on port 443.</p>
<p>D. Create a security group for the DB instance. Add a rule to allow traffic from the web servers’ security group on port 3306.</p>
<p>E. Create a security group for the DB instance. Add a rule to deny all traffic except traffic from the web servers’ security group on port 3306.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C, D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-407">Question #407<a aria-hidden="true" class="anchor-heading icon-link" href="#question-407"></a></h3>
<p>A company is using Site-to-Site VPN connections for secure connectivity to its AWS Cloud resources from on premises. Due to an increase in traffic across the
VPN connections to the Amazon EC2 instances, users are experiencing slower VPN connectivity.</p>
<p>Which solution will improve the VPN throughput?</p>
<p>A. Implement multiple customer gateways for the same network to scale the throughput.</p>
<p>B. Use a transit gateway with equal cost multipath routing and add additional VPN tunnels.</p>
<p>C. Configure a virtual private gateway with equal cost multipath routing and multiple channels.</p>
<p>D. Increase the number of tunnels in the VPN configuration to scale the throughput beyond the default limit.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-410">Question #410<a aria-hidden="true" class="anchor-heading icon-link" href="#question-410"></a></h3>
<p>A company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to the EBS volumes is encrypted at rest.</p>
<p>Which solution will meet this requirement?</p>
<p>A. Create an IAM role that specifies EBS encryption. Attach the role to the EC2 instances.</p>
<p>B. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.</p>
<p>C. Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that require encryption at the EBS level.</p>
<p>D. Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption in the account. Ensure that the key policy is active.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B </p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-414">Question #414<a aria-hidden="true" class="anchor-heading icon-link" href="#question-414"></a></h3>
<p>A company has a business system that generates hundreds of reports each day. The business system saves the reports to a network share in CSV format. The company needs to store this data in the AWS Cloud in near-real time for analysis.</p>
<p>Which solution will meet these requirements with the LEAST administrative overhead?</p>
<p>A. Use AWS DataSync to transfer the files to Amazon S3. Create a scheduled task that runs at the end of each day.</p>
<p>B. Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.</p>
<p>C. Use AWS DataSync to transfer the files to Amazon S3. Create an application that uses the DataSync API in the automation workflow.</p>
<p>D. Deploy an AWS Transfer for SFTP endpoint. Create a script that checks for new files on the network share and uploads the new files by using SFTP.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-419">Question #419<a aria-hidden="true" class="anchor-heading icon-link" href="#question-419"></a></h3>
<p>A company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 workloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that prevents any resources from being created in any other Region. A security policy requires the company to encrypt all data at rest.</p>
<p>An audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) volumes for EC2 instances without encrypting the volumes. The company wants any new EC2 instances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS volumes. The company wants a solution that will have minimal effect on employees who create EBS volumes.</p>
<p>Which combination of steps will meet these requirements? (Choose two.)</p>
<p>A. In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key.</p>
<p>B. Create an IAM permission boundary. Attach the permission boundary to the root organizational unit (OU). Define the boundary to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.</p>
<p>C. Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.</p>
<p>D. Update the IAM policies for each account to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.</p>
<p>E. In the Organizations management account, specify the Default EBS volume encryption setting.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C, E</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-420">Question #420<a aria-hidden="true" class="anchor-heading icon-link" href="#question-420"></a></h3>
<p>A company wants to use an Amazon RDS for PostgreSQL DB cluster to simplify time-consuming database administrative tasks for production database workloads. The company wants to ensure that its database is highly available and will provide automatic failover support in most scenarios in less than 40 seconds. The company wants to offload reads off of the primary instance and keep costs as low as possible.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Use an Amazon RDS Multi-AZ DB instance deployment. Create one read replica and point the read workload to the read replica.</p>
<p>B. Use an Amazon RDS Multi-AZ DB duster deployment Create two read replicas and point the read workload to the read replicas.</p>
<p>C. Use an Amazon RDS Multi-AZ DB instance deployment. Point the read workload to the secondary instances in the Multi-AZ pair.</p>
<p>D. Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>D </p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-434">Question #434<a aria-hidden="true" class="anchor-heading icon-link" href="#question-434"></a></h3>
<p>A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in anotherAWS Region with minimal downtime.</p>
<p>What should a solutions architect do to meet these requirements with the LEAST amount of downtime?</p>
<p>A. Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.</p>
<p>B. Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be launched when needed Configure DNS failover to point to the new disaster recovery Region's load balancer.</p>
<p>C. Create an AWS CloudFormation template to create EC2 instances and a load balancer to be launched when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.</p>
<p>D. Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-439">Question #439<a aria-hidden="true" class="anchor-heading icon-link" href="#question-439"></a></h3>
<p>A solutions architect configured a VPC that has a small range of IP addresses. The number of Amazon EC2 instances that are in the VPC is increasing, and there is an insufficient number of IP addresses for future workloads.</p>
<p>Which solution resolves this issue with the LEAST operational overhead?</p>
<p>A. Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC. Create new resources in the new subnets by using the new CIDR.</p>
<p>B. Create a second VPC with additional subnets. Use a peering connection to connect the second VPC with the first VPC. Update the routes and create new resources in the subnets of the second VPC.</p>
<p>C. Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first VPC. Update the routes of the transit gateway and VPCs. Create new resources in the subnets of the second VPC.</p>
<p>D. Create a second VPC. Create a Site-to-Site VPN connection between the first VPC and the second VPC by using a VPN-hosted solution on Amazon EC2 and a virtual private gateway. Update the route between VPCs to the traffic through the VPN. Create new resources in the subnets of the second VPC.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-440">Question #440<a aria-hidden="true" class="anchor-heading icon-link" href="#question-440"></a></h3>
<p>A company used an Amazon RDS for MySQL DB instance during application testing. Before terminating the DB instance at the end of the test cycle, a solutions architect created two backups. The solutions architect created the first backup by using the mysqldump utility to create a database dump. The solutions architect created the second backup by enabling the final DB snapshot option on RDS termination.</p>
<p>The company is now planning for a new test cycle and wants to create a new DB instance from the most recent backup. The company has chosen a MySQL-compatible edition ofAmazon Aurora to host the DB instance.</p>
<p>Which solutions will create the new DB instance? (Choose two.)</p>
<p>A. Import the RDS snapshot directly into Aurora.</p>
<p>B. Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.</p>
<p>C. Upload the database dump to Amazon S3. Then import the database dump into Aurora.</p>
<p>D. Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora.</p>
<p>E. Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS DMS) to import the database dump into Aurora.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A, C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-442">Question #442<a aria-hidden="true" class="anchor-heading icon-link" href="#question-442"></a></h3>
<p>A company has two VPCs that are located in the us-west-2 Region within the same AWS account. The company needs to allow network traffic between these
VPCs. Approximately 500 GB of data transfer will occur between the VPCs each month.</p>
<p>What is the MOST cost-effective solution to connect these VPCs?</p>
<p>A. Implement AWS Transit Gateway to connect the VPCs. Update the route tables of each VPC to use the transit gateway for inter-VPC communication.</p>
<p>B. Implement an AWS Site-to-Site VPN tunnel between the VPCs. Update the route tables of each VPC to use the VPN tunnel for inter-VPC communication.</p>
<p>C. Set up a VPC peering connection between the VPCs. Update the route tables of each VPC to use the VPC peering connection for inter-VPC communication.</p>
<p>D. Set up a 1 GB AWS Direct Connect connection between the VPCs. Update the route tables of each VPC to use the Direct Connect connection for inter-VPC communication.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-443">Question #443<a aria-hidden="true" class="anchor-heading icon-link" href="#question-443"></a></h3>
<p>A company wants to host a scalable web application on AWS. The application will be accessed by users from different geographic regions of the world. Application users will be able to download and upload unique data up to gigabytes in size. The development team wants a cost-effective solution to minimize upload and download latency and maximize performance.</p>
<p>What should a solutions architect do to accomplish this?</p>
<p>A. Use Amazon S3 with Transfer Acceleration to host the application.</p>
<p>B. Use Amazon S3 with CacheControl headers to host the application.</p>
<p>C. Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.</p>
<p>D. Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-443-1">Question #443<a aria-hidden="true" class="anchor-heading icon-link" href="#question-443-1"></a></h3>
<p>A company wants to host a scalable web application on AWS. The application will be accessed by users from different geographic regions of the world. Application users will be able to download and upload unique data up to gigabytes in size. The development team wants a cost-effective solution to minimize upload and download latency and maximize performance.</p>
<p>What should a solutions architect do to accomplish this?</p>
<p>A. Use Amazon S3 with Transfer Acceleration to host the application.</p>
<p>B. Use Amazon S3 with CacheControl headers to host the application.</p>
<p>C. Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.</p>
<p>D. Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C </p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-447">Question #447<a aria-hidden="true" class="anchor-heading icon-link" href="#question-447"></a></h3>
<p>A company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API Gateway. The company wants to deploy the application across multiple AWS Regions to provide Regional failover capabilities.</p>
<p>What should a solutions architect do to route traffic to multiple Regions?</p>
<p>A. Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.</p>
<p>B. Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health checks to route traffic.</p>
<p>C. Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region. Configure the transit gateway to route requests.</p>
<p>D. Create an Application Load Balancer in the primary Region. Set the target group to point to the API Gateway endpoint hostnames in each Region.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-448">Question #448<a aria-hidden="true" class="anchor-heading icon-link" href="#question-448"></a></h3>
<p>A company has two VPCs named Management and Production. The Management VPC uses VPNs through a customer gateway to connect to a single device in the data center. The Production VPC uses a virtual private gateway with two attached AWS Direct Connect connections. The Management and Production VPCs both use a single VPC peering connection to allow communication between the applications.</p>
<p>What should a solutions architect do to mitigate any single point of failure in this architecture?</p>
<p>A. Add a set of VPNs between the Management and Production VPCs.</p>
<p>B. Add a second virtual private gateway and attach it to the Management VPC.</p>
<p>C. Add a second set of VPNs to the Management VPC from a second customer gateway device.</p>
<p>D. Add a second VPC peering connection between the Management VPC and the Production VPC.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C </p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-450">Question #450<a aria-hidden="true" class="anchor-heading icon-link" href="#question-450"></a></h3>
<p>A company has a three-tier web application that is in a single server. The company wants to migrate the application to the AWS Cloud. The company also wants the application to align with the AWS Well-Architected Framework and to be consistent with AWS recommended best practices for security, scalability, and resiliency.</p>
<p>Which combination of solutions will meet these requirements? (Choose three.)</p>
<p>A. Create a VPC across two Availability Zones with the application's existing architecture. Host the application with existing architecture on an Amazon EC2 instance in a private subnet in each Availability Zone with EC2 Auto Scaling groups. Secure the EC2 instance with security groups and network access control lists (network ACLs).</p>
<p>B. Set up security groups and network access control lists (network ACLs) to control access to the database layer. Set up a single Amazon RDS database in a private subnet.</p>
<p>C. Create a VPC across two Availability Zones. Refactor the application to host the web tier, application tier, and database tier. Host each tier on its own private subnet with Auto Scaling groups for the web tier and application tier.</p>
<p>D. Use a single Amazon RDS database. Allow database access only from the application tier security group.</p>
<p>E. Use Elastic Load Balancers in front of the web tier. Control access by using security groups containing references to each layer's security groups.</p>
<p>F. Use an Amazon RDS database Multi-AZ cluster deployment in private subnets. Allow database access only from application tier security groups.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C, E, F </p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-451">Question #451<a aria-hidden="true" class="anchor-heading icon-link" href="#question-451"></a></h3>
<p>A company is migrating its applications and databases to the AWS Cloud. The company will use Amazon Elastic Container Service (Amazon ECS), AWS Direct Connect, and Amazon RDS.</p>
<p>Which activities will be managed by the company's operational team? (Choose three.)</p>
<p>A. Management of the Amazon RDS infrastructure layer, operating system, and platforms</p>
<p>B. Creation of an Amazon RDS DB instance and configuring the scheduled maintenance window</p>
<p>C. Configuration of additional software components on Amazon ECS for monitoring, patch management, log management, and host intrusion detection</p>
<p>D. Installation of patches for all minor and major database versions for Amazon RDS</p>
<p>E. Ensure the physical security of the Amazon RDS infrastructure in the data center</p>
<p>F. Encryption of the data that moves in transit through Direct Connect</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B, C, F</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-455">Question #455<a aria-hidden="true" class="anchor-heading icon-link" href="#question-455"></a></h3>
<p>A company uses AWS Organizations. The company wants to operate some of its AWS accounts with different budgets. The company wants to receive alerts and automatically prevent provisioning of additional resources on AWS accounts when the allocated budget threshold is met during a specific period.</p>
<p>Which combination of solutions will meet these requirements? (Choose three.)</p>
<p>A. Use AWS Budgets to create a budget. Set the budget amount under the Cost and Usage Reports section of the required AWS accounts.</p>
<p>B. Use AWS Budgets to create a budget. Set the budget amount under the Billing dashboards of the required AWS accounts.</p>
<p>C. Create an IAM user for AWS Budgets to run budget actions with the required permissions.</p>
<p>D. Create an IAM role for AWS Budgets to run budget actions with the required permissions.</p>
<p>E. Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate config rule to prevent provisioning of additional resources.</p>
<p>F. Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate service control policy (SCP) to prevent provisioning of additional resources.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B, D, F</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-457">Question #457<a aria-hidden="true" class="anchor-heading icon-link" href="#question-457"></a></h3>
<p>A company that uses AWS is building an application to transfer data to a product manufacturer. The company has its own identity provider (IdP). The company wants the IdP to authenticate application users while the users use the application to transfer data. The company must use Applicability Statement 2 (AS2) protocol.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Use AWS DataSync to transfer the data. Create an AWS Lambda function for IdP authentication.</p>
<p>B. Use Amazon AppFlow flows to transfer the data. Create an Amazon Elastic Container Service (Amazon ECS) task for IdP authentication.</p>
<p>C. Use AWS Transfer Family to transfer the data. Create an AWS Lambda function for IdP authentication.</p>
<p>D. Use AWS Storage Gateway to transfer the data. Create an Amazon Cognito identity pool for IdP authentication.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-471">Question #471<a aria-hidden="true" class="anchor-heading icon-link" href="#question-471"></a></h3>
<p>A company is creating an application that runs on containers in a VPC. The application stores and accesses data in an Amazon S3 bucket. During the development phase, the application will store and access 1 TB of data in Amazon S3 each day. The company wants to minimize costs and wants to prevent traffic from traversing the internet whenever possible.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Enable S3 Intelligent-Tiering for the S3 bucket</p>
<p>B. Enable S3 Transfer Acceleration for the S3 bucket</p>
<p>C. Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in the VPC</p>
<p>D. Create an interface endpoint for Amazon S3 in the VPC. Associate this endpoint with all route tables in the VPC</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-468">Question #468<a aria-hidden="true" class="anchor-heading icon-link" href="#question-468"></a></h3>
<p>A company is developing a microservices application that will provide a search catalog for customers. The company must use REST APIs to present the frontend of the application to users. The REST APIs must access the backend services that the company hosts in containers in private VPC subnets.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.</p>
<p>B. Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.</p>
<p>C. Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS.</p>
<p>D. Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-474">Question #474<a aria-hidden="true" class="anchor-heading icon-link" href="#question-474"></a></h3>
<p>A company has multiple VPCs across AWS Regions to support and run workloads that are isolated from workloads in other Regions. Because of a recent application launch requirement, the company’s VPCs must communicate with all other VPCs across all Regions.</p>
<p>Which solution will meet these requirements with the LEAST amount of administrative effort?</p>
<p>A. Use VPC peering to manage VPC communication in a single Region. Use VPC peering across Regions to manage VPC communications.</p>
<p>B. Use AWS Direct Connect gateways across all Regions to connect VPCs across regions and manage VPC communications.</p>
<p>C. Use AWS Transit Gateway to manage VPC communication in a single Region and Transit Gateway peering across Regions to manage VPC communications.</p>
<p>D. Use AWS PrivateLink across all Regions to connect VPCs across Regions and manage VPC communications</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-475">Question #475<a aria-hidden="true" class="anchor-heading icon-link" href="#question-475"></a></h3>
<p>A company is designing a containerized application that will use Amazon Elastic Container Service (Amazon ECS). The application needs to access a shared file system that is highly durable and can recover data to another AWS Region with a recovery point objective (RPO) of 8 hours. The file system needs to provide a mount target m each Availability Zone within a Region.</p>
<p>A solutions architect wants to use AWS Backup to manage the replication to another Region.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Amazon FSx for Windows File Server with a Multi-AZ deployment
B. Amazon FSx for NetApp ONTAP with a Multi-AZ deployment
C. Amazon Elastic File System (Amazon EFS) with the Standard storage class
D. Amazon FSx for OpenZFS</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-476">Question #476<a aria-hidden="true" class="anchor-heading icon-link" href="#question-476"></a></h3>
<p>A company is expecting rapid growth in the near future. A solutions architect needs to configure existing users and grant permissions to new users on AWS. The solutions architect has decided to create IAM groups. The solutions architect will add the new users to IAM groups based on department.</p>
<p>Which additional action is the MOST secure way to grant permissions to the new users?</p>
<p>A. Apply service control policies (SCPs) to manage access permissions</p>
<p>B. Create IAM roles that have least privilege permission. Attach the roles to the IAM groups</p>
<p>C. Create an IAM policy that grants least privilege permission. Attach the policy to the IAM groups</p>
<p>D. Create IAM roles. Associate the roles with a permissions boundary that defines the maximum permissions</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-479">Question #479<a aria-hidden="true" class="anchor-heading icon-link" href="#question-479"></a></h3>
<p>A company is making a prototype of the infrastructure for its new website by manually provisioning the necessary infrastructure. This infrastructure includes an Auto Scaling group, an Application Load Balancer and an Amazon RDS database. After the configuration has been thoroughly validated, the company wants the capability to immediately deploy the infrastructure for development and production use in two Availability Zones in an automated fashion.</p>
<p>What should a solutions architect recommend to meet these requirements?</p>
<p>A. Use AWS Systems Manager to replicate and provision the prototype infrastructure in two Availability Zones</p>
<p>B. Define the infrastructure as a template by using the prototype infrastructure as a guide. Deploy the infrastructure with AWS CloudFormation.</p>
<p>C. Use AWS Config to record the inventory of resources that are used in the prototype infrastructure. Use AWS Config to deploy the prototype infrastructure into two Availability Zones.</p>
<p>D. Use AWS Elastic Beanstalk and configure it to use an automated reference to the prototype infrastructure to automatically deploy new environments in two Availability Zones.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-492">Question #492<a aria-hidden="true" class="anchor-heading icon-link" href="#question-492"></a></h3>
<p>A company has multiple AWS accounts for development work. Some staff consistently use oversized Amazon EC2 instances, which causes the company to exceed the yearly budget for the development accounts. The company wants to centrally restrict the creation of AWS resources in these accounts.</p>
<p>Which solution will meet these requirements with the LEAST development effort?</p>
<p>A. Develop AWS Systems Manager templates that use an approved EC2 creation process. Use the approved Systems Manager templates to provision EC2 instances.</p>
<p>B. Use AWS Organizations to organize the accounts into organizational units (OUs). Define and attach a service control policy (SCP) to control the usage of EC2 instance types.</p>
<p>C. Configure an Amazon EventBridge rule that invokes an AWS Lambda function when an EC2 instance is created. Stop disallowed EC2 instance types.</p>
<p>D. Set up AWS Service Catalog products for the staff to create the allowed EC2 instance types. Ensure that staff can deploy EC2 instances only by using the Service Catalog products.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>B</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-500">Question #500<a aria-hidden="true" class="anchor-heading icon-link" href="#question-500"></a></h3>
<p>A company has multiple Windows file servers on premises. The company wants to migrate and consolidate its files into an Amazon FSx for Windows File Server file system. File permissions must be preserved to ensure that access rights do not change.</p>
<p>Which solutions will meet these requirements? (Choose two.)</p>
<p>A. Deploy AWS DataSync agents on premises. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.</p>
<p>B. Copy the shares on each file server into Amazon S3 buckets by using the AWS CLI. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.</p>
<p>C. Remove the drives from each file server. Ship the drives to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.</p>
<p>D. Order an AWS Snowcone device. Connect the device to the on-premises network. Launch AWS DataSync agents on the device. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.</p>
<p>E. Order an AWS Snowball Edge Storage Optimized device. Connect the device to the on-premises network. Copy data to the device by using the AWS CLI. Ship the device back to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A, D</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-502">Question #502<a aria-hidden="true" class="anchor-heading icon-link" href="#question-502"></a></h3>
<p>A company runs a website that uses a content management system (CMS) on Amazon EC2. The CMS runs on a single EC2 instance and uses an Amazon Aurora MySQL Multi-AZ DB instance for the data tier. Website images are stored on an Amazon Elastic Block Store (Amazon EBS) volume that is mounted inside the EC2 instance.</p>
<p>Which combination of actions should a solutions architect take to improve the performance and resilience of the website? (Choose two.)</p>
<p>A. Move the website images into an Amazon S3 bucket that is mounted on every EC2 instance</p>
<p>B. Share the website images by using an NFS share from the primary EC2 instance. Mount this share on the other EC2 instances.</p>
<p>C. Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.</p>
<p>D. Create an Amazon Machine Image (AMI) from the existing EC2 instance. Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group. Configure the Auto Scaling group to maintain a minimum of two instances. Configure an accelerator in AWS Global Accelerator for the website</p>
<p>E. Create an Amazon Machine Image (AMI) from the existing EC2 instance. Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group. Configure the Auto Scaling group to maintain a minimum of two instances. Configure an Amazon CloudFront distribution for the website.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A, E</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-504">Question #504<a aria-hidden="true" class="anchor-heading icon-link" href="#question-504"></a></h3>
<p>A company needs to connect several VPCs in the us-east-1 Region that span hundreds of AWS accounts. The company's networking team has its own AWS account to manage the cloud network.</p>
<p>What is the MOST operationally efficient solution to connect the VPCs?</p>
<p>A. Set up VPC peering connections between each VPC. Update each associated subnet’s route table</p>
<p>B. Configure a NAT gateway and an internet gateway in each VPC to connect each VPC through the internet</p>
<p>C. Create an AWS Transit Gateway in the networking team’s AWS account. Configure static routes from each VPC.</p>
<p>D. Deploy VPN gateways in each VPC. Create a transit VPC in the networking team’s AWS account to connect to each VPC.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-544">Question #544<a aria-hidden="true" class="anchor-heading icon-link" href="#question-544"></a></h3>
<p>A retail company uses a regional Amazon API Gateway API for its public REST APIs. The API Gateway endpoint is a custom domain name that points to an Amazon Route 53 alias record. A solutions architect needs to create a solution that has minimal effects on customers and minimal data loss to release the new version of APIs.</p>
<p>Which solution will meet these requirements?</p>
<p>A. Create a canary release deployment stage for API Gateway. Deploy the latest API version. Point an appropriate percentage of traffic to the canary stage. After API verification, promote the canary stage to the production stage.</p>
<p>B. Create a new API Gateway endpoint with a new version of the API in OpenAPI YAML file format. Use the import-to-update operation in merge mode into the API in API Gateway. Deploy the new version of the API to the production stage.</p>
<p>C. Create a new API Gateway endpoint with a new version of the API in OpenAPI JSON file format. Use the import-to-update operation in overwrite mode into the API in API Gateway. Deploy the new version of the API to the production stage.</p>
<p>D. Create a new API Gateway endpoint with new versions of the API definitions. Create a custom domain name for the new API Gateway API. Point the Route 53 alias record to the new API Gateway API custom domain name.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<p><br></p><hr><p></p>
<h3 id="question-558">Question #558<a aria-hidden="true" class="anchor-heading icon-link" href="#question-558"></a></h3>
<p>A company has two VPCs that are located in the us-west-2 Region within the same AWS account. The company needs to allow network traffic between these VPCs. Approximately 500 GB of data transfer will occur between the VPCs each month.</p>
<p>What is the MOST cost-effective solution to connect these VPCs?</p>
<p>A. Implement AWS Transit Gateway to connect the VPCs. Update the route tables of each VPC to use the transit gateway for inter-VPC communication.</p>
<p>B. Implement an AWS Site-to-Site VPN tunnel between the VPCs. Update the route tables of each VPC to use the VPN tunnel for inter-VPC communication.</p>
<p>C. Set up a VPC peering connection between the VPCs. Update the route tables of each VPC to use the VPC peering connection for inter-VPC communication.</p>
<p>D. Set up a 1 GB AWS Direct Connect connection between the VPCs. Update the route tables of each VPC to use the Direct Connect connection for inter-VPC communication.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>C</p>
</blockquote>
</details>
<br>
<h3 id="question-608">Question #608<a aria-hidden="true" class="anchor-heading icon-link" href="#question-608"></a></h3>
<p>A company has an application that serves clients that are deployed in more than 20.000 retail storefront locations around the world. The application consists of backend web services that are exposed over HTTPS on port 443. The application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The retail locations communicate with the web application over the public internet. The company allows each retail location to register the IP address that the retail location has been allocated by its local ISP.</p>
<p>The company's security team recommends to increase the security of the application endpoint by restricting access to only the IP addresses registered by the retail locations.</p>
<p>What should a solutions architect do to meet these requirements?</p>
<p>A. Associate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter traffic. Update the IP addresses in the rule to include the registered IP addresses.</p>
<p>B. Deploy AWS Firewall Manager to manage the ALConfigure firewall rules to restrict traffic to the ALModify the firewall rules to include the registered IP addresses.</p>
<p>C. Store the IP addresses in an Amazon DynamoDB table. Configure an AWS Lambda authorization function on the ALB to validate that incoming requests are from the registered IP addresses.</p>
<p>D. Configure the network ACL on the subnet that contains the public interface of the ALB. Update the ingress rules on the network ACL with entries for each of the registered IP addresses.</p>
<details>
<summary>Show Answer</summary>
<blockquote>
<p>A</p>
</blockquote>
</details>
<br>
<h2 id="references">References<a aria-hidden="true" class="anchor-heading icon-link" href="#references"></a></h2>
<ul>
<li><a href="https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate-saa-c03/">https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate-saa-c03/</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#questions" title="Questions">Questions</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-3" title="Question #3">Question #3</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-9" title="Question #9">Question #9</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-12" title="Question #12">Question #12</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-15" title="Question #15">Question #15</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-16" title="Question #16">Question #16</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-19" title="Question #19">Question #19</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-34" title="Question #34">Question #34</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-37" title="Question #37">Question #37</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-50" title="Question #50">Question #50</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-56" title="Question #56">Question #56</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-64" title="Question #64">Question #64</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-66" title="Question #66">Question #66</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-68" title="Question #68">Question #68</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-71" title="Question #71">Question #71</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-82" title="Question #82">Question #82</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-83" title="Question #83">Question #83</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-84" title="Question #84">Question #84</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-88" title="Question #88">Question #88</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-93" title="Question #93">Question #93</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-101" title="Question #101">Question #101</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-113" title="Question #113">Question #113</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-116" title="Question #116">Question #116</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-119" title="Question #119">Question #119</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-120" title="Question #120">Question #120</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-125" title="Question #125">Question #125</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-129" title="Question #129">Question #129</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-134" title="Question #134">Question #134</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-139" title="Question #139">Question #139</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-140" title="Question #140">Question #140</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-143" title="Question #143">Question #143</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-144" title="Question #144">Question #144</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-145" title="Question #145">Question #145</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-150" title="Question #150">Question #150</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-151" title="Question #151">Question #151</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-157" title="Question #157">Question #157</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-158" title="Question #158">Question #158</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-159" title="Question #159">Question #159</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-172" title="Question #172">Question #172</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-182" title="Question #182">Question #182</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-184" title="Question #184">Question #184</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-188" title="Question #188">Question #188</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-189" title="Question #189">Question #189</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-190" title="Question #190">Question #190</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-194" title="Question #194">Question #194</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-201" title="Question #201">Question #201</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-208" title="Question #208">Question #208</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-215" title="Question #215">Question #215</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-217" title="Question #217">Question #217</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-218" title="Question #218">Question #218</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-219" title="Question #219">Question #219</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-223" title="Question #223">Question #223</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-224" title="Question #224">Question #224</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-225" title="Question #225">Question #225</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-233" title="Question #233">Question #233</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-234" title="Question #234">Question #234</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-266" title="Question #266">Question #266</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-276" title="Question #276">Question #276</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-287" title="Question #287">Question #287</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-295" title="Question #295">Question #295</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-327" title="Question #327">Question #327</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-328" title="Question #328">Question #328</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-333" title="Question #333">Question #333</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-335" title="Question #335">Question #335</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-342" title="Question #342">Question #342</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-344" title="Question #344">Question #344</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-346" title="Question #346">Question #346</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-349" title="Question #349">Question #349</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-353" title="Question #353">Question #353</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-363" title="Question #363">Question #363</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-364" title="Question #364">Question #364</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-366" title="Question #366">Question #366</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-369" title="Question #369">Question #369</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-370" title="Question #370">Question #370</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-371" title="Question #371">Question #371</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-379" title="Question #379">Question #379</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-385" title="Question #385">Question #385</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-388" title="Question #388">Question #388</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-390" title="Question #390">Question #390</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-391" title="Question #391">Question #391</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-399" title="Question #399">Question #399</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-402" title="Question #402">Question #402</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-406" title="Question #406">Question #406</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-407" title="Question #407">Question #407</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-410" title="Question #410">Question #410</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-414" title="Question #414">Question #414</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-419" title="Question #419">Question #419</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-420" title="Question #420">Question #420</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-434" title="Question #434">Question #434</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-439" title="Question #439">Question #439</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-440" title="Question #440">Question #440</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-442" title="Question #442">Question #442</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-443" title="Question #443">Question #443</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-443-1" title="Question #443">Question #443</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-447" title="Question #447">Question #447</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-448" title="Question #448">Question #448</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-450" title="Question #450">Question #450</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-451" title="Question #451">Question #451</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-455" title="Question #455">Question #455</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-457" title="Question #457">Question #457</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-471" title="Question #471">Question #471</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-468" title="Question #468">Question #468</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-474" title="Question #474">Question #474</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-475" title="Question #475">Question #475</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-476" title="Question #476">Question #476</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-479" title="Question #479">Question #479</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-492" title="Question #492">Question #492</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-500" title="Question #500">Question #500</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-502" title="Question #502">Question #502</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-504" title="Question #504">Question #504</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-544" title="Question #544">Question #544</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-558" title="Question #558">Question #558</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#question-608" title="Question #608">Question #608</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#references" title="References">References</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"csls1iecmzveuq2w6ym65k0","title":"AWS Certified Solutions Architect - Associate (SAA-C03) 오답 노트","desc":"","updated":1696667709232,"created":1694842069036,"custom":{},"fname":"dev.cloud.aws.certification.SSA-C03","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"0cd577a6279d5e4e357fb96d22161388","links":[],"anchors":{"questions":{"type":"header","text":"Questions","value":"questions","line":10,"column":0,"depth":2},"question-3":{"type":"header","text":"Question #3","value":"question-3","line":14,"column":0,"depth":3},"question-9":{"type":"header","text":"Question #9","value":"question-9","line":37,"column":0,"depth":3},"question-12":{"type":"header","text":"Question #12","value":"question-12","line":61,"column":0,"depth":3},"question-15":{"type":"header","text":"Question #15","value":"question-15","line":84,"column":0,"depth":3},"question-16":{"type":"header","text":"Question #16","value":"question-16","line":107,"column":0,"depth":3},"question-19":{"type":"header","text":"Question #19","value":"question-19","line":130,"column":0,"depth":3},"question-34":{"type":"header","text":"Question #34","value":"question-34","line":153,"column":0,"depth":3},"question-37":{"type":"header","text":"Question #37","value":"question-37","line":176,"column":0,"depth":3},"question-50":{"type":"header","text":"Question #50","value":"question-50","line":198,"column":0,"depth":3},"question-56":{"type":"header","text":"Question #56","value":"question-56","line":221,"column":0,"depth":3},"question-64":{"type":"header","text":"Question #64","value":"question-64","line":243,"column":0,"depth":3},"question-66":{"type":"header","text":"Question #66","value":"question-66","line":269,"column":0,"depth":3},"question-68":{"type":"header","text":"Question #68","value":"question-68","line":292,"column":0,"depth":3},"question-71":{"type":"header","text":"Question #71","value":"question-71","line":315,"column":0,"depth":3},"question-82":{"type":"header","text":"Question #82","value":"question-82","line":338,"column":0,"depth":3},"question-83":{"type":"header","text":"Question #83","value":"question-83","line":361,"column":0,"depth":3},"question-84":{"type":"header","text":"Question #84","value":"question-84","line":384,"column":0,"depth":3},"question-88":{"type":"header","text":"Question #88","value":"question-88","line":409,"column":0,"depth":3},"question-93":{"type":"header","text":"Question #93","value":"question-93","line":432,"column":0,"depth":3},"question-101":{"type":"header","text":"Question #101","value":"question-101","line":459,"column":0,"depth":3},"question-113":{"type":"header","text":"Question #113","value":"question-113","line":482,"column":0,"depth":3},"question-116":{"type":"header","text":"Question #116","value":"question-116","line":507,"column":0,"depth":3},"question-119":{"type":"header","text":"Question #119","value":"question-119","line":534,"column":0,"depth":3},"question-120":{"type":"header","text":"Question #120","value":"question-120","line":557,"column":0,"depth":3},"question-125":{"type":"header","text":"Question #125","value":"question-125","line":580,"column":0,"depth":3},"question-129":{"type":"header","text":"Question #129","value":"question-129","line":605,"column":0,"depth":3},"question-134":{"type":"header","text":"Question #134","value":"question-134","line":630,"column":0,"depth":3},"question-139":{"type":"header","text":"Question #139","value":"question-139","line":653,"column":0,"depth":3},"question-140":{"type":"header","text":"Question #140","value":"question-140","line":678,"column":0,"depth":3},"question-143":{"type":"header","text":"Question #143","value":"question-143","line":705,"column":0,"depth":3},"question-144":{"type":"header","text":"Question #144","value":"question-144","line":728,"column":0,"depth":3},"question-145":{"type":"header","text":"Question #145","value":"question-145","line":751,"column":0,"depth":3},"question-150":{"type":"header","text":"Question #150","value":"question-150","line":774,"column":0,"depth":3},"question-151":{"type":"header","text":"Question #151","value":"question-151","line":797,"column":0,"depth":3},"question-157":{"type":"header","text":"Question #157","value":"question-157","line":824,"column":0,"depth":3},"question-158":{"type":"header","text":"Question #158","value":"question-158","line":849,"column":0,"depth":3},"question-159":{"type":"header","text":"Question #159","value":"question-159","line":872,"column":0,"depth":3},"question-172":{"type":"header","text":"Question #172","value":"question-172","line":897,"column":0,"depth":3},"question-182":{"type":"header","text":"Question #182","value":"question-182","line":920,"column":0,"depth":3},"question-184":{"type":"header","text":"Question #184","value":"question-184","line":943,"column":0,"depth":3},"question-188":{"type":"header","text":"Question #188","value":"question-188","line":968,"column":0,"depth":3},"question-189":{"type":"header","text":"Question #189","value":"question-189","line":991,"column":0,"depth":3},"question-190":{"type":"header","text":"Question #190","value":"question-190","line":1016,"column":0,"depth":3},"question-194":{"type":"header","text":"Question #194","value":"question-194","line":1039,"column":0,"depth":3},"question-201":{"type":"header","text":"Question #201","value":"question-201","line":1062,"column":0,"depth":3},"question-208":{"type":"header","text":"Question #208","value":"question-208","line":1085,"column":0,"depth":3},"question-215":{"type":"header","text":"Question #215","value":"question-215","line":1108,"column":0,"depth":3},"question-217":{"type":"header","text":"Question #217","value":"question-217","line":1131,"column":0,"depth":3},"question-218":{"type":"header","text":"Question #218","value":"question-218","line":1154,"column":0,"depth":3},"question-219":{"type":"header","text":"Question #219","value":"question-219","line":1179,"column":0,"depth":3},"question-223":{"type":"header","text":"Question #223","value":"question-223","line":1202,"column":0,"depth":3},"question-224":{"type":"header","text":"Question #224","value":"question-224","line":1227,"column":0,"depth":3},"question-225":{"type":"header","text":"Question #225","value":"question-225","line":1252,"column":0,"depth":3},"question-233":{"type":"header","text":"Question #233","value":"question-233","line":1275,"column":0,"depth":3},"question-234":{"type":"header","text":"Question #234","value":"question-234","line":1298,"column":0,"depth":3},"question-266":{"type":"header","text":"Question #266","value":"question-266","line":1321,"column":0,"depth":3},"question-276":{"type":"header","text":"Question #276","value":"question-276","line":1344,"column":0,"depth":3},"question-287":{"type":"header","text":"Question #287","value":"question-287","line":1369,"column":0,"depth":3},"question-295":{"type":"header","text":"Question #295","value":"question-295","line":1392,"column":0,"depth":3},"question-327":{"type":"header","text":"Question #327","value":"question-327","line":1415,"column":0,"depth":3},"question-328":{"type":"header","text":"Question #328","value":"question-328","line":1438,"column":0,"depth":3},"question-333":{"type":"header","text":"Question #333","value":"question-333","line":1463,"column":0,"depth":3},"question-335":{"type":"header","text":"Question #335","value":"question-335","line":1486,"column":0,"depth":3},"question-342":{"type":"header","text":"Question #342","value":"question-342","line":1509,"column":0,"depth":3},"question-344":{"type":"header","text":"Question #344","value":"question-344","line":1534,"column":0,"depth":3},"question-346":{"type":"header","text":"Question #346","value":"question-346","line":1557,"column":0,"depth":3},"question-349":{"type":"header","text":"Question #349","value":"question-349","line":1582,"column":0,"depth":3},"question-353":{"type":"header","text":"Question #353","value":"question-353","line":1605,"column":0,"depth":3},"question-363":{"type":"header","text":"Question #363","value":"question-363","line":1630,"column":0,"depth":3},"question-364":{"type":"header","text":"Question #364","value":"question-364","line":1653,"column":0,"depth":3},"question-366":{"type":"header","text":"Question #366","value":"question-366","line":1680,"column":0,"depth":3},"question-369":{"type":"header","text":"Question #369","value":"question-369","line":1703,"column":0,"depth":3},"question-370":{"type":"header","text":"Question #370","value":"question-370","line":1726,"column":0,"depth":3},"question-371":{"type":"header","text":"Question #371","value":"question-371","line":1749,"column":0,"depth":3},"question-379":{"type":"header","text":"Question #379","value":"question-379","line":1774,"column":0,"depth":3},"question-385":{"type":"header","text":"Question #385","value":"question-385","line":1797,"column":0,"depth":3},"question-388":{"type":"header","text":"Question #388","value":"question-388","line":1820,"column":0,"depth":3},"question-390":{"type":"header","text":"Question #390","value":"question-390","line":1845,"column":0,"depth":3},"question-391":{"type":"header","text":"Question #391","value":"question-391","line":1872,"column":0,"depth":3},"question-399":{"type":"header","text":"Question #399","value":"question-399","line":1897,"column":0,"depth":3},"question-402":{"type":"header","text":"Question #402","value":"question-402","line":1922,"column":0,"depth":3},"question-406":{"type":"header","text":"Question #406","value":"question-406","line":1945,"column":0,"depth":3},"question-407":{"type":"header","text":"Question #407","value":"question-407","line":1970,"column":0,"depth":3},"question-410":{"type":"header","text":"Question #410","value":"question-410","line":1994,"column":0,"depth":3},"question-414":{"type":"header","text":"Question #414","value":"question-414","line":2017,"column":0,"depth":3},"question-419":{"type":"header","text":"Question #419","value":"question-419","line":2040,"column":0,"depth":3},"question-420":{"type":"header","text":"Question #420","value":"question-420","line":2067,"column":0,"depth":3},"question-434":{"type":"header","text":"Question #434","value":"question-434","line":2090,"column":0,"depth":3},"question-439":{"type":"header","text":"Question #439","value":"question-439","line":2113,"column":0,"depth":3},"question-440":{"type":"header","text":"Question #440","value":"question-440","line":2136,"column":0,"depth":3},"question-442":{"type":"header","text":"Question #442","value":"question-442","line":2163,"column":0,"depth":3},"question-443":{"type":"header","text":"Question #443","value":"question-443","line":2187,"column":0,"depth":3},"question-443-1":{"type":"header","text":"Question #443","value":"question-443-1","line":2210,"column":0,"depth":3},"question-447":{"type":"header","text":"Question #447","value":"question-447","line":2233,"column":0,"depth":3},"question-448":{"type":"header","text":"Question #448","value":"question-448","line":2256,"column":0,"depth":3},"question-450":{"type":"header","text":"Question #450","value":"question-450","line":2279,"column":0,"depth":3},"question-451":{"type":"header","text":"Question #451","value":"question-451","line":2306,"column":0,"depth":3},"question-455":{"type":"header","text":"Question #455","value":"question-455","line":2333,"column":0,"depth":3},"question-457":{"type":"header","text":"Question #457","value":"question-457","line":2360,"column":0,"depth":3},"question-471":{"type":"header","text":"Question #471","value":"question-471","line":2383,"column":0,"depth":3},"question-468":{"type":"header","text":"Question #468","value":"question-468","line":2406,"column":0,"depth":3},"question-474":{"type":"header","text":"Question #474","value":"question-474","line":2429,"column":0,"depth":3},"question-475":{"type":"header","text":"Question #475","value":"question-475","line":2452,"column":0,"depth":3},"question-476":{"type":"header","text":"Question #476","value":"question-476","line":2474,"column":0,"depth":3},"question-479":{"type":"header","text":"Question #479","value":"question-479","line":2497,"column":0,"depth":3},"question-492":{"type":"header","text":"Question #492","value":"question-492","line":2520,"column":0,"depth":3},"question-500":{"type":"header","text":"Question #500","value":"question-500","line":2543,"column":0,"depth":3},"question-502":{"type":"header","text":"Question #502","value":"question-502","line":2568,"column":0,"depth":3},"question-504":{"type":"header","text":"Question #504","value":"question-504","line":2593,"column":0,"depth":3},"question-544":{"type":"header","text":"Question #544","value":"question-544","line":2616,"column":0,"depth":3},"question-558":{"type":"header","text":"Question #558","value":"question-558","line":2639,"column":0,"depth":3},"question-608":{"type":"header","text":"Question #608","value":"question-608","line":2662,"column":0,"depth":3},"references":{"type":"header","text":"References","value":"references","line":2687,"column":0,"depth":2}},"children":[],"parent":"uyrn9wfgt8zqanwnh2ywi8d","data":{}},"body":"\u003ch1 id=\"aws-certified-solutions-architect---associate-saa-c03-오답-노트\"\u003eAWS Certified Solutions Architect - Associate (SAA-C03) 오답 노트\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#aws-certified-solutions-architect---associate-saa-c03-오답-노트\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003ccode\u003eAWS Certified Solutions Architect - Associate (SAA-C03) Exam Practice Questions\u003c/code\u003e 오답 노트\u003c/p\u003e\n\u003ch2 id=\"questions\"\u003eQuestions\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#questions\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003chr\u003e\n\u003ch3 id=\"question-3\"\u003eQuestion #3\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-3\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company uses AWS Organizations to manage multiple AWS accounts for different departments. The management account has an Amazon S3 bucket that contains project reports. The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations.\u003c/p\u003e\n\u003cp\u003eWhich solution meets these requirements with the LEAST amount of operational overhead?\u003c/p\u003e\n\u003cp\u003eA. Add the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.\u003c/p\u003e\n\u003cp\u003eB. Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.\u003c/p\u003e\n\u003cp\u003eC. Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly.\u003c/p\u003e\n\u003cp\u003eD. Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-9\"\u003eQuestion #9\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-9\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is running an SMB file server in its data center. The file server stores large files that are accessed frequently for the first few days after the files are created. After 7 days the files are rarely accessed.\nThe total data size is increasing and is close to the company's total storage capacity. A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.\u003c/p\u003e\n\u003cp\u003eB. Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.\u003c/p\u003e\n\u003cp\u003eC. Create an Amazon FSx for Windows File Server file system to extend the company's storage space.\u003c/p\u003e\n\u003cp\u003eD. Install a utility on each user's computer to access Amazon S3. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-12\"\u003eQuestion #12\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-12\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA global company hosts its web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The web application has static data and dynamic data. The company stores its static data in an Amazon S3 bucket. The company wants to improve performance and reduce latency for the static data and dynamic data. The company is using its own domain name registered with Amazon Route 53.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins. Configure Route 53 to route traffic to the CloudFront distribution.\u003c/p\u003e\n\u003cp\u003eB. Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint Configure Route 53 to route traffic to the CloudFront distribution.\u003c/p\u003e\n\u003cp\u003eC. Create an Amazon CloudFront distribution that has the S3 bucket as an origin. Create an AWS Global Accelerator standard accelerator that has the ALB and the CloudFront distribution as endpoints. Create a custom domain name that points to the accelerator DNS name. Use the custom domain name as an endpoint for the web application.\u003c/p\u003e\n\u003cp\u003eD. Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint. Create two domain names. Point one domain name to the CloudFront DNS name for dynamic content. Point the other domain name to the accelerator DNS name for static content. Use the domain names as endpoints for the web application.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-15\"\u003eQuestion #15\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-15\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company recently migrated to AWS and wants to implement a solution to protect the traffic that flows in and out of the production VPC. The company had an inspection server in its on-premises data center. The inspection server performed specific operations such as traffic flow inspection and traffic filtering. The company wants to have the same functionalities in the AWS Cloud.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Use Amazon GuardDuty for traffic inspection and traffic filtering in the production VPC.\u003c/p\u003e\n\u003cp\u003eB. Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering.\u003c/p\u003e\n\u003cp\u003eC. Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for the production VPC.\u003c/p\u003e\n\u003cp\u003eD. Use AWS Firewall Manager to create the required rules for traffic inspection and traffic filtering for the production VPC.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-16\"\u003eQuestion #16\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-16\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations. The rest of the company should have only limited access.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.\u003c/p\u003e\n\u003cp\u003eB. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.\u003c/p\u003e\n\u003cp\u003eC. Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.\u003c/p\u003e\n\u003cp\u003eD. Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-19\"\u003eQuestion #19\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-19\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC. The application servers and database servers are deployed in private subnets in the same VPC. The company has deployed a third-party virtual firewall appliance from AWS Marketplace in an inspection VPC. The appliance is configured with an IP interface that can accept IP packets.\nA solutions architect needs to integrate the web application with the appliance to inspect all traffic to the application before the traffic reaches the web server.\nWhich solution will meet these requirements with the LEAST operational overhead?\u003c/p\u003e\n\u003cp\u003eA. Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.\u003c/p\u003e\n\u003cp\u003eB. Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.\u003c/p\u003e\n\u003cp\u003eC. Deploy a transit gateway in the inspection VPConfigure route tables to route the incoming packets through the transit gateway.\u003c/p\u003e\n\u003cp\u003eD. Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eD\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-34\"\u003eQuestion #34\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-34\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the company must track configuration changes on its AWS resources and record a history of API calls made to these resources.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.\u003c/p\u003e\n\u003cp\u003eB. Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.\u003c/p\u003e\n\u003cp\u003eC. Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.\u003c/p\u003e\n\u003cp\u003eD. Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-37\"\u003eQuestion #37\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-37\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The company needs to create a strategy to access and administer the instances remotely and securely. The company needs to implement a repeatable process that works with native AWS services and follows the AWS Well-Architected Framework.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements with the LEAST operational overhead?\u003c/p\u003e\n\u003cp\u003eA. Use the EC2 serial console to directly access the terminal interface of each instance for administration.\u003c/p\u003e\n\u003cp\u003eB. Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager Session Manager to establish a remote SSH session.\u003c/p\u003e\n\u003cp\u003eC. Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion host in a public subnet to provide a tunnel for administration of each instance.\u003c/p\u003e\n\u003cp\u003eD. Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises machines to connect directly to the instances by using SSH keys across the VPN tunnel.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\u003c/details\u003e\n\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-50\"\u003eQuestion #50\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-50\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The workload is powered by third-party software. The company needs to patch the third-party software on all EC2 instances as quickly as possible to remediate a critical security vulnerability.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Create an AWS Lambda function to apply the patch to all EC2 instances.\u003c/p\u003e\n\u003cp\u003eB. Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.\u003c/p\u003e\n\u003cp\u003eC. Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.\u003c/p\u003e\n\u003cp\u003eD. Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eD\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-56\"\u003eQuestion #56\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-56\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway in the ca-central-1 Region as a public interface for its backend microservice APIs. Third-party services consume the APIs securely. The company wants to design its API Gateway URL with the company's domain name and corresponding certificate so that the third-party services can use HTTPS.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Create stage variables in API Gateway with Name=\"Endpoint-URL\" and Value=\"Company Domain Name\" to overwrite the default URL. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM).\u003c/p\u003e\n\u003cp\u003eB. Create Route 53 DNS records with the company's domain name. Point the alias record to the Regional API Gateway stage endpoint. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region.\u003c/p\u003e\n\u003cp\u003eC. Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.\u003c/p\u003e\n\u003cp\u003eD. Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region. Attach the certificate to the API Gateway APIs. Create Route 53 DNS records with the company's domain name. Point an A record to the company's domain name.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-64\"\u003eQuestion #64\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-64\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has more than 5 TB of file data on Windows file servers that run on premises. Users and applications interact with the data each day.\nThe company is moving its Windows workloads to AWS. As the company continues this process, the company requires access to AWS and on-premises file storage with minimum latency. The company needs a solution that minimizes operational overhead and requires no significant changes to the existing file access patterns. The company uses an AWS Site-to-Site VPN connection for connectivity to AWS.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data to FSx for Windows File Server. Reconfigure the workloads to use FSx for Windows File Server on AWS.\u003c/p\u003e\n\u003cp\u003eB. Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to the S3 File Gateway. Reconfigure the on-premises workloads and the cloud workloads to use the S3 File Gateway.\u003c/p\u003e\n\u003cp\u003eC. Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to Amazon S3. Reconfigure the workloads to use either Amazon S3 directly or the S3 File Gateway. depending on each workload's location.\u003c/p\u003e\n\u003cp\u003eD. Deploy and configure Amazon FSx for Windows File Server on AWS. Deploy and configure an Amazon FSx File Gateway on premises. Move the on-premises file data to the FSx File Gateway. Configure the cloud workloads to use FSx for Windows File Server on AWS. Configure the on-premises workloads to use the FSx File Gateway.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eD\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-66\"\u003eQuestion #66\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-66\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has an application that generates a large number of files, each approximately 5 MB in size. The files are stored in Amazon S3. Company policy requires the files to be stored for 4 years before they can be deleted. Immediate accessibility is always required as the files contain critical business data that is not easy to reproduce. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days.\u003c/p\u003e\n\u003cp\u003eWhich storage solution is MOST cost-effective?\u003c/p\u003e\n\u003cp\u003eA. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Glacier 30 days from object creation. Delete the files 4 years after object creation.\u003c/p\u003e\n\u003cp\u003eB. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days from object creation. Delete the files 4 years after object creation.\u003c/p\u003e\n\u003cp\u003eC. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Delete the files 4 years after object creation.\u003c/p\u003e\n\u003cp\u003eD. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Move the files to S3 Glacier 4 years after object creation.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-68\"\u003eQuestion #68\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-68\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA solutions architect is designing a new hybrid architecture to extend a company's on-premises infrastructure to AWS. The company requires a highly available connection with consistent low latency to an AWS Region. The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails.\u003c/p\u003e\n\u003cp\u003eWhat should the solutions architect do to meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the primary Direct Connect connection fails.\u003c/p\u003e\n\u003cp\u003eB. Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN tunnel for private connectivity and as a backup if the primary VPN connection fails.\u003c/p\u003e\n\u003cp\u003eC. Provision an AWS Direct Connect connection to a Region. Provision a second Direct Connect connection to the same Region as a backup if the primary Direct Connect connection fails.\u003c/p\u003e\n\u003cp\u003eD. Provision an AWS Direct Connect connection to a Region. Use the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-71\"\u003eQuestion #71\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-71\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company runs a shopping application that uses Amazon DynamoDB to store customer information. In case of data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour.\u003c/p\u003e\n\u003cp\u003eWhat should the solutions architect recommend to meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Configure DynamoDB global tables. For RPO recovery, point the application to a different AWS Region.\u003c/p\u003e\n\u003cp\u003eB. Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.\u003c/p\u003e\n\u003cp\u003eC. Export the DynamoDB data to Amazon S3 Glacier on a daily basis. For RPO recovery, import the data from S3 Glacier to DynamoDB.\u003c/p\u003e\n\u003cp\u003eD. Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the DynamoDB table by using the EBS snapshot.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB \u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-82\"\u003eQuestion #82\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-82\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company hosts its web applications in the AWS Cloud. The company configures Elastic Load Balancers to use certificates that are imported into AWS Certificate Manager (ACM). The company's security team must be notified 30 days before the expiration of each certificate.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect recommend to meet this requirement?\u003c/p\u003e\n\u003cp\u003eA. Add a rule in ACM to publish a custom message to an Amazon Simple Notification Service (Amazon SNS) topic every day, beginning 30 days before any certificate will expire.\u003c/p\u003e\n\u003cp\u003eB. Create an AWS Config rule that checks for certificates that will expire within 30 days. Configure Amazon EventBridge (Amazon CloudWatch Events) to invoke a custom alert by way of Amazon Simple Notification Service (Amazon SNS) when AWS Config reports a noncompliant resource.\u003c/p\u003e\n\u003cp\u003eC. Use AWS Trusted Advisor to check for certificates that will expire within 30 days. Create an Amazon CloudWatch alarm that is based on Trusted Advisor metrics for check status changes. Configure the alarm to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS).\u003c/p\u003e\n\u003cp\u003eD. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect any certificates that will expire within 30 days. Configure the rule to invoke an AWS Lambda function. Configure the Lambda function to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS).\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-83\"\u003eQuestion #83\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-83\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company's dynamic website is hosted using on-premises servers in the United States. The company is launching its product in Europe, and it wants to optimize site loading times for new European users. The site's backend must remain in the United States. The product is being launched in a few days, and an immediate solution is needed.\u003c/p\u003e\n\u003cp\u003eWhat should the solutions architect recommend?\u003c/p\u003e\n\u003cp\u003eA. Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.\u003c/p\u003e\n\u003cp\u003eB. Move the website to Amazon S3. Use Cross-Region Replication between Regions.\u003c/p\u003e\n\u003cp\u003eC. Use Amazon CloudFront with a custom origin pointing to the on-premises servers.\u003c/p\u003e\n\u003cp\u003eD. Use an Amazon Route 53 geoproximity routing policy pointing to on-premises servers.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-84\"\u003eQuestion #84\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-84\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company wants to reduce the cost of its existing three-tier web architecture. The web, application, and database servers are running on Amazon EC2 instances for the development, test, and production environments. The EC2 instances average 30% CPU utilization during peak hours and 10% CPU utilization during non-peak hours.\u003c/p\u003e\n\u003cp\u003eThe production EC2 instances run 24 hours a day. The development and test EC2 instances run for at least 8 hours each day. The company plans to implement automation to stop the development and test EC2 instances when they are not in use.\u003c/p\u003e\n\u003cp\u003eWhich EC2 instance purchasing solution will meet the company's requirements MOST cost-effectively?\u003c/p\u003e\n\u003cp\u003eA. Use Spot Instances for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.\u003c/p\u003e\n\u003cp\u003eB. Use Reserved Instances for the production EC2 instances. Use On-Demand Instances for the development and test EC2 instances.\u003c/p\u003e\n\u003cp\u003eC. Use Spot blocks for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.\u003c/p\u003e\n\u003cp\u003eD. Use On-Demand Instances for the production EC2 instances. Use Spot blocks for the development and test EC2 instances.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-88\"\u003eQuestion #88\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-88\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA survey company has gathered data for several years from areas in the United States. The company hosts the data in an Amazon S3 bucket that is 3 TB in size and growing. The company has started to share the data with a European marketing firm that has S3 buckets. The company wants to ensure that its data transfer costs remain as low as possible.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Configure the Requester Pays feature on the company's S3 bucket.\u003c/p\u003e\n\u003cp\u003eB. Configure S3 Cross-Region Replication from the company's S3 bucket to one of the marketing firm's S3 buckets.\u003c/p\u003e\n\u003cp\u003eC. Configure cross-account access for the marketing firm so that the marketing firm has access to the company's S3 bucket.\u003c/p\u003e\n\u003cp\u003eD. Configure the company's S3 bucket to use S3 Intelligent-Tiering. Sync the S3 bucket to one of the marketing firm's S3 buckets.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-93\"\u003eQuestion #93\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-93\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company runs an on-premises application that is powered by a MySQL database. The company is migrating the application to AWS to increase the application's elasticity and availability.\u003c/p\u003e\n\u003cp\u003eThe current architecture shows heavy read activity on the database during times of normal operation. Every 4 hours, the company's development team pulls a full export of the production database to populate a database in the staging environment. During this period, users experience unacceptable application latency. The development team is unable to use the staging environment until the procedure completes.\u003c/p\u003e\n\u003cp\u003eA solutions architect must recommend replacement architecture that alleviates the application latency issue. The replacement architecture also must give the development team the ability to continue using the staging environment without delay.\u003c/p\u003e\n\u003cp\u003eWhich solution meets these requirements?\u003c/p\u003e\n\u003cp\u003eA. Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility.\u003c/p\u003e\n\u003cp\u003eB. Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to create the staging database on-demand.\u003c/p\u003e\n\u003cp\u003eC. Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Use the standby instance for the staging database.\u003c/p\u003e\n\u003cp\u003eD. Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-101\"\u003eQuestion #101\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-101\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA solutions architect is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates.\u003c/p\u003e\n\u003cp\u003eWhat should the solutions architect do to enable Internet access for the private subnets?\u003c/p\u003e\n\u003cp\u003eA. Create three NAT gateways, one for each public subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT gateway in its AZ.\u003c/p\u003e\n\u003cp\u003eB. Create three NAT instances, one for each private subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT instance in its AZ.\u003c/p\u003e\n\u003cp\u003eC. Create a second internet gateway on one of the private subnets. Update the route table for the private subnets that forward non-VPC traffic to the private internet gateway.\u003c/p\u003e\n\u003cp\u003eD. Create an egress-only internet gateway on one of the public subnets. Update the route table for the private subnets that forward non-VPC traffic to the egress-only Internet gateway.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA \u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-113\"\u003eQuestion #113\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-113\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company uses 50 TB of data for reporting. The company wants to move this data from on premises to AWS. A custom application in the company’s data center runs a weekly data transformation job. The company plans to pause the application until the data transfer is complete and needs to begin the transfer process as soon as possible.\u003c/p\u003e\n\u003cp\u003eThe data center does not have any available network bandwidth for additional workloads. A solutions architect must transfer the data and must configure the transformation job to continue to run in the AWS Cloud.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements with the LEAST operational overhead?\u003c/p\u003e\n\u003cp\u003eA. Use AWS DataSync to move the data. Create a custom transformation job by using AWS Glue.\u003c/p\u003e\n\u003cp\u003eB. Order an AWS Snowcone device to move the data. Deploy the transformation application to the device.\u003c/p\u003e\n\u003cp\u003eC. Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue.\u003c/p\u003e\n\u003cp\u003eD. Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data to the device. Create a new EC2 instance on AWS to run the transformation application.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-116\"\u003eQuestion #116\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-116\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company uses a popular content management system (CMS) for its corporate website. However, the required patching and maintenance are burdensome. The company is redesigning its website and wants a new solution. The website will be updated four times a year and does not need to have any dynamic content available. The solution must provide high scalability and enhanced security.\u003c/p\u003e\n\u003cp\u003eWhich combination of changes will meet these requirements with the LEAST operational overhead? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Configure Amazon CloudFront in front of the website to use HTTPS functionality.\u003c/p\u003e\n\u003cp\u003eB. Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality.\u003c/p\u003e\n\u003cp\u003eC. Create and deploy an AWS Lambda function to manage and serve the website content.\u003c/p\u003e\n\u003cp\u003eD. Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.\u003c/p\u003e\n\u003cp\u003eE. Create the new website. Deploy the website by using an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA, D\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-119\"\u003eQuestion #119\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-119\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA global company is using Amazon API Gateway to design REST APIs for its loyalty club users in the us-east-1 Region and the ap-southeast-2 Region. A solutions architect must design a solution to protect these API Gateway managed REST APIs across multiple accounts from SQL injection and cross-site scripting attacks.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements with the LEAST amount of administrative effort?\u003c/p\u003e\n\u003cp\u003eA. Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage.\u003c/p\u003e\n\u003cp\u003eB. Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules.\u003c/p\u003e\n\u003cp\u003eC. Set up AWS Shield in bath Regions. Associate Regional web ACLs with an API stage.\u003c/p\u003e\n\u003cp\u003eD. Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-120\"\u003eQuestion #120\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-120\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has implemented a self-managed DNS solution on three Amazon EC2 instances behind a Network Load Balancer (NLB) in the us-west-2 Region. Most of the company's users are located in the United States and Europe. The company wants to improve the performance and availability of the solution. The company launches and configures three EC2 instances in the eu-west-1 Region and adds the EC2 instances as targets for a new NLB.\u003c/p\u003e\n\u003cp\u003eWhich solution can the company use to route traffic to all the EC2 instances?\u003c/p\u003e\n\u003cp\u003eA. Create an Amazon Route 53 geolocation routing policy to route requests to one of the two NLBs. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution’s origin.\u003c/p\u003e\n\u003cp\u003eB. Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in us-west-2 and eu-west-1. Add the two NLBs as endpoints for the endpoint groups.\u003c/p\u003e\n\u003cp\u003eC. Attach Elastic IP addresses to the six EC2 instances. Create an Amazon Route 53 geolocation routing policy to route requests to one of the six EC2 instances. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution's origin.\u003c/p\u003e\n\u003cp\u003eD. Replace the two NLBs with two Application Load Balancers (ALBs). Create an Amazon Route 53 latency routing policy to route requests to one of the two ALBs. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution’s origin.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-125\"\u003eQuestion #125\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-125\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company runs its two-tier ecommerce website on AWS. The web tier consists of a load balancer that sends traffic to Amazon EC2 instances. The database tier uses an Amazon RDS DB instance. The EC2 instances and the RDS DB instance should not be exposed to the public internet. The EC2 instances require internet access to complete payment processing of orders through a third-party web service. The application must be highly available.\u003c/p\u003e\n\u003cp\u003eWhich combination of configuration options will meet these requirements? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-AZ DB instance in private subnets.\u003c/p\u003e\n\u003cp\u003eB. Configure a VPC with two private subnets and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the private subnets.\u003c/p\u003e\n\u003cp\u003eC. Use an Auto Scaling group to launch the EC2 instances in public subnets across two Availability Zones. Deploy an RDS Multi-AZ DB instance in private subnets.\u003c/p\u003e\n\u003cp\u003eD. Configure a VPC with one public subnet, one private subnet, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnet.\u003c/p\u003e\n\u003cp\u003eE. Configure a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnets.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA, D\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-129\"\u003eQuestion #129\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-129\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is running a multi-tier web application on premises. The web application is containerized and runs on a number of Linux hosts connected to a PostgreSQL database that contains user records. The operational overhead of maintaining the infrastructure and capacity planning is limiting the company's growth. A solutions architect must improve the application's infrastructure.\u003c/p\u003e\n\u003cp\u003eWhich combination of actions should the solutions architect take to accomplish this? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Migrate the PostgreSQL database to Amazon Aurora.\u003c/p\u003e\n\u003cp\u003eB. Migrate the web application to be hosted on Amazon EC2 instances.\u003c/p\u003e\n\u003cp\u003eC. Set up an Amazon CloudFront distribution for the web application content.\u003c/p\u003e\n\u003cp\u003eD. Set up Amazon ElastiCache between the web application and the PostgreSQL database.\u003c/p\u003e\n\u003cp\u003eE. Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS).\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA, E\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-134\"\u003eQuestion #134\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-134\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company wants to move its application to a serverless solution. The serverless solution needs to analyze existing and new data by using SL. The company stores the data in an Amazon S3 bucket. The data requires encryption and must be replicated to a different AWS Region.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements with the LEAST operational overhead?\u003c/p\u003e\n\u003cp\u003eA. Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region kays (SSE-KMS). Use Amazon Athena to query the data.\u003c/p\u003e\n\u003cp\u003eB. Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region keys (SSE-KMS). Use Amazon RDS to query the data.\u003c/p\u003e\n\u003cp\u003eC. Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon Athena to query the data.\u003c/p\u003e\n\u003cp\u003eD. Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon RDS to query the data.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-139\"\u003eQuestion #139\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-139\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon QuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket.\u003c/p\u003e\n\u003cp\u003eThe reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3 bucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker Pipelines.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to meet these requirements with the LEAST operational overhead?\u003c/p\u003e\n\u003cp\u003eA. Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.\u003c/p\u003e\n\u003cp\u003eB. Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.\u003c/p\u003e\n\u003cp\u003eC. Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.\u003c/p\u003e\n\u003cp\u003eD. Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eD \u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-140\"\u003eQuestion #140\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-140\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA solutions architect needs to help a company optimize the cost of running an application on AWS. The application will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for compute within the architecture.\u003c/p\u003e\n\u003cp\u003eThe EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic and unpredictable. Workloads that run on EC2 instances can be interrupted at any time. The application front end will run on Fargate, and Lambda will serve the API layer. The front-end utilization and API layer utilization will be predictable over the course of the next year.\u003c/p\u003e\n\u003cp\u003eWhich combination of purchasing options will provide the MOST cost-effective solution for hosting this application? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Use Spot Instances for the data ingestion layer\u003c/p\u003e\n\u003cp\u003eB. Use On-Demand Instances for the data ingestion layer\u003c/p\u003e\n\u003cp\u003eC. Purchase a 1-year Compute Savings Plan for the front end and API layer.\u003c/p\u003e\n\u003cp\u003eD. Purchase 1-year All Upfront Reserved instances for the data ingestion layer.\u003c/p\u003e\n\u003cp\u003eE. Purchase a 1-year EC2 instance Savings Plan for the front end and API layer.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA, C\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-143\"\u003eQuestion #143\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-143\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company wants to migrate its existing on-premises monolithic application to AWS. The company wants to keep as much of the front-end code and the backend code as possible. However, the company wants to break the application into smaller applications. A different team will manage each application. The company needs a highly scalable solution that minimizes operational overhead.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Host the application on AWS Lambda. Integrate the application with Amazon API Gateway.\u003c/p\u003e\n\u003cp\u003eB. Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API that is integrated with AWS Lambda.\u003c/p\u003e\n\u003cp\u003eC. Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2 instances in an Auto Scaling group as targets.\u003c/p\u003e\n\u003cp\u003eD. Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eD\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-144\"\u003eQuestion #144\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-144\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company recently started using Amazon Aurora as the data store for its global ecommerce application. When large reports are run, developers report that the ecommerce application is performing poorly. After reviewing metrics in Amazon CloudWatch, a solutions architect finds that the ReadIOPS and CPUUtilizalion metrics are spiking when monthly reports run.\u003c/p\u003e\n\u003cp\u003eWhat is the MOST cost-effective solution?\u003c/p\u003e\n\u003cp\u003eA. Migrate the monthly reporting to Amazon Redshift.\u003c/p\u003e\n\u003cp\u003eB. Migrate the monthly reporting to an Aurora Replica.\u003c/p\u003e\n\u003cp\u003eC. Migrate the Aurora database to a larger instance class.\u003c/p\u003e\n\u003cp\u003eD. Increase the Provisioned IOPS on the Aurora instance.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-145\"\u003eQuestion #145\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-145\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. The analytics software is written in PHP and uses a MySQL database. The analytics software, the web server that provides PHP, and the database server are all hosted on the EC2 instance. The application is showing signs of performance degradation during busy times and is presenting 5xx errors. The company needs to make the application scale seamlessly.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements MOST cost-effectively?\u003c/p\u003e\n\u003cp\u003eA. Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load to each EC2 instance.\u003c/p\u003e\n\u003cp\u003eB. Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances.\u003c/p\u003e\n\u003cp\u003eC. Migrate the database to an Amazon Aurora MySQL DB instance. Create an AWS Lambda function to stop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization surpasses 75%.\u003c/p\u003e\n\u003cp\u003eD. Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web application. Apply the AMI to a launch template. Create an Auto Scaling group with the launch template Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eD\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-150\"\u003eQuestion #150\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-150\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is migrating an application from on-premises servers to Amazon EC2 instances. As part of the migration design requirements, a solutions architect must implement infrastructure metric alarms. The company does not need to take action if CPU utilization increases to more than 50% for a short burst of time. However, if the CPU utilization increases to more than 50% and read IOPS on the disk are high at the same time, the company needs to act as soon as possible. The solutions architect also must reduce false alarms.\u003c/p\u003e\n\u003cp\u003eWhat should the solutions architect do to meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Create Amazon CloudWatch composite alarms where possible.\u003c/p\u003e\n\u003cp\u003eB. Create Amazon CloudWatch dashboards to visualize the metrics and react to issues quickly.\u003c/p\u003e\n\u003cp\u003eC. Create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm.\u003c/p\u003e\n\u003cp\u003eD. Create single Amazon CloudWatch metric alarms with multiple metric thresholds where possible.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-151\"\u003eQuestion #151\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-151\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company wants to migrate its on-premises data center to AWS. According to the company's compliance requirements, the company can use only the ap-northeast-3 Region. Company administrators are not permitted to connect VPCs to the internet.\u003c/p\u003e\n\u003cp\u003eWhich solutions will meet these requirements? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3.\u003c/p\u003e\n\u003cp\u003eB. Use rules in AWS WAF to prevent internet access. Deny access to all AWS Regions except ap-northeast-3 in the AWS account settings.\u003c/p\u003e\n\u003cp\u003eC. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.\u003c/p\u003e\n\u003cp\u003eD. Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0. Create an IAM policy for each user to prevent the use of any AWS Region other than ap-northeast-3.\u003c/p\u003e\n\u003cp\u003eE. Use AWS Config to activate managed rules to detect and alert for internet gateways and to detect and alert for new resources deployed outside of ap-northeast-3.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA, C\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-157\"\u003eQuestion #157\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-157\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company stores data in an Amazon Aurora PostgreSQL DB cluster. The company must store all the data for 5 years and must delete all the data after 5 years. The company also must indefinitely keep audit logs of actions that are performed within the database. Currently, the company has automated backups configured for Aurora.\u003c/p\u003e\n\u003cp\u003eWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Take a manual snapshot of the DB cluster.\u003c/p\u003e\n\u003cp\u003eB. Create a lifecycle policy for the automated backups.\u003c/p\u003e\n\u003cp\u003eC. Configure automated backup retention for 5 years.\u003c/p\u003e\n\u003cp\u003eD. Configure an Amazon CloudWatch Logs export for the DB cluster.\u003c/p\u003e\n\u003cp\u003eE. Use AWS Backup to take the backups and to keep the backups for 5 years.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eD, E\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-158\"\u003eQuestion #158\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-158\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA solutions architect is optimizing a website for an upcoming musical event. Videos of the performances will be streamed in real time and then will be available on demand. The event is expected to attract a global online audience.\u003c/p\u003e\n\u003cp\u003eWhich service will improve the performance of both the real-time and on-demand streaming?\u003c/p\u003e\n\u003cp\u003eA. Amazon CloudFront\u003c/p\u003e\n\u003cp\u003eB. AWS Global Accelerator\u003c/p\u003e\n\u003cp\u003eC. Amazon Route 53\u003c/p\u003e\n\u003cp\u003eD. Amazon S3 Transfer Acceleration\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-159\"\u003eQuestion #159\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-159\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS Lambda. The application’s traffic recently spiked due to fraudulent requests from botnets.\u003c/p\u003e\n\u003cp\u003eWhich steps should a solutions architect take to block requests from unauthorized users? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Create a usage plan with an API key that is shared with genuine users only.\u003c/p\u003e\n\u003cp\u003eB. Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses.\u003c/p\u003e\n\u003cp\u003eC. Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.\u003c/p\u003e\n\u003cp\u003eD. Convert the existing public API to a private API. Update the DNS records to redirect users to the new API endpoint.\u003c/p\u003e\n\u003cp\u003eE. Create an IAM role for each user attempting to access the API. A user will assume the role when making the API call.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA, C\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-172\"\u003eQuestion #172\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-172\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA solutions architect is creating a new Amazon CloudFront distribution for an application. Some of the information submitted by users is sensitive. The application uses HTTPS but needs another layer of security. The sensitive information should be protected throughout the entire application stack, and access to the information should be restricted to certain applications.\u003c/p\u003e\n\u003cp\u003eWhich action should the solutions architect take?\u003c/p\u003e\n\u003cp\u003eA. Configure a CloudFront signed URL.\u003c/p\u003e\n\u003cp\u003eB. Configure a CloudFront signed cookie.\u003c/p\u003e\n\u003cp\u003eC. Configure a CloudFront field-level encryption profile.\u003c/p\u003e\n\u003cp\u003eD. Configure CloudFront and set the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol Policy.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-182\"\u003eQuestion #182\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-182\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company wants to migrate its MySQL database from on premises to AWS. The company recently experienced a database outage that significantly impacted the business. To ensure this does not happen again, the company wants a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes.\u003c/p\u003e\n\u003cp\u003eWhich solution meets these requirements?\u003c/p\u003e\n\u003cp\u003eA. Create an Amazon RDS DB instance with synchronous replication to three nodes in three Availability Zones.\u003c/p\u003e\n\u003cp\u003eB. Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data.\u003c/p\u003e\n\u003cp\u003eC. Create an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data.\u003c/p\u003e\n\u003cp\u003eD. Create an Amazon EC2 instance with a MySQL engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-184\"\u003eQuestion #184\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-184\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has an AWS account used for software engineering. The AWS account has access to the company’s on-premises data center through a pair of AWS Direct Connect connections. All non-VPC traffic routes to the virtual private gateway.\u003c/p\u003e\n\u003cp\u003eA development team recently created an AWS Lambda function through the console. The development team needs to allow the function to access a database that runs in a private subnet in the company’s data center.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Configure the Lambda function to run in the VPC with the appropriate security group.\u003c/p\u003e\n\u003cp\u003eB. Set up a VPN connection from AWS to the data center. Route the traffic from the Lambda function through the VPN.\u003c/p\u003e\n\u003cp\u003eC. Update the route tables in the VPC to allow the Lambda function to access the on-premises data center through Direct Connect.\u003c/p\u003e\n\u003cp\u003eD. Create an Elastic IP address. Configure the Lambda function to send traffic through the Elastic IP address without an elastic network interface.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-188\"\u003eQuestion #188\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-188\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company uses Amazon S3 as its data lake. The company has a new partner that must use SFTP to upload data files. A solutions architect needs to implement a highly available SFTP solution that minimizes operational overhead.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible endpoint. Choose the S3 data lake as the destination.\u003c/p\u003e\n\u003cp\u003eB. Use Amazon S3 File Gateway as an SFTP server. Expose the S3 File Gateway endpoint URL to the new partner. Share the S3 File Gateway endpoint with the new partner.\u003c/p\u003e\n\u003cp\u003eC. Launch an Amazon EC2 instance in a private subnet in a VPInstruct the new partner to upload files to the EC2 instance by using a VPN. Run a cron job script, on the EC2 instance to upload files to the S3 data lake.\u003c/p\u003e\n\u003cp\u003eD. Launch Amazon EC2 instances in a private subnet in a VPC. Place a Network Load Balancer (NLB) in front of the EC2 instances. Create an SFTP listener port for the NLB. Share the NLB hostname with the new partner. Run a cron job script on the EC2 instances to upload files to the S3 data lake.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-189\"\u003eQuestion #189\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-189\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company needs to store contract documents. A contract lasts for 5 years. During the 5-year period, the company must ensure that the documents cannot be overwritten or deleted. The company needs to encrypt the documents at rest and rotate the encryption keys automatically every year.\u003c/p\u003e\n\u003cp\u003eWhich combination of steps should a solutions architect take to meet these requirements with the LEAST operational overhead? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Store the documents in Amazon S3. Use S3 Object Lock in governance mode.\u003c/p\u003e\n\u003cp\u003eB. Store the documents in Amazon S3. Use S3 Object Lock in compliance mode.\u003c/p\u003e\n\u003cp\u003eC. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure key rotation.\u003c/p\u003e\n\u003cp\u003eD. Use server-side encryption with AWS Key Management Service (AWS KMS) customer managed keys. Configure key rotation.\u003c/p\u003e\n\u003cp\u003eE. Use server-side encryption with AWS Key Management Service (AWS KMS) customer provided (imported) keys. Configure key rotation.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB, D\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-190\"\u003eQuestion #190\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-190\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has a web application that is based on Java and PHP. The company plans to move the application from on premises to AWS. The company needs the ability to test new site features frequently. The company also needs a highly available and managed solution that requires minimum operational overhead.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Create an Amazon S3 bucket. Enable static web hosting on the S3 bucket. Upload the static content to the S3 bucket. Use AWS Lambda to process all dynamic content.\u003c/p\u003e\n\u003cp\u003eB. Deploy the web application to an AWS Elastic Beanstalk environment. Use URL swapping to switch between multiple Elastic Beanstalk environments for feature testing.\u003c/p\u003e\n\u003cp\u003eC. Deploy the web application to Amazon EC2 instances that are configured with Java and PHP. Use Auto Scaling groups and an Application Load Balancer to manage the website’s availability.\u003c/p\u003e\n\u003cp\u003eD. Containerize the web application. Deploy the web application to Amazon EC2 instances. Use the AWS Load Balancer Controller to dynamically route traffic between containers that contain the new site features for testing.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-194\"\u003eQuestion #194\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-194\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company needs to run a critical application on AWS. The company needs to use Amazon EC2 for the application’s database. The database must be highly available and must fail over automatically if a disruptive event occurs.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Set up database replication.\u003c/p\u003e\n\u003cp\u003eB. Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use AWS CloudFormation to automate provisioning of the EC2 instance if a disruptive event occurs.\u003c/p\u003e\n\u003cp\u003eC. Launch two EC2 instances, each in a different AWS Region. Install the database on both EC2 instances. Set up database replication. Fail over the database to a second Region.\u003c/p\u003e\n\u003cp\u003eD. Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use EC2 automatic recovery to recover the instance if a disruptive event occurs.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-201\"\u003eQuestion #201\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-201\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is developing a marketing communications service that targets mobile app users. The company needs to send confirmation messages with Short Message Service (SMS) to its users. The users must be able to reply to the SMS messages. The company must store the responses for a year for analysis.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to process the responses.\u003c/p\u003e\n\u003cp\u003eB. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.\u003c/p\u003e\n\u003cp\u003eC. Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda to process the responses.\u003c/p\u003e\n\u003cp\u003eD. Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-208\"\u003eQuestion #208\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-208\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must ensure that no API calls and no data are routed through public internet routes. Only the EC2 instance can have access to upload data to the S3 bucket.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.\u003c/p\u003e\n\u003cp\u003eB. Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is located. Attach appropriate security groups to the endpoint. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.\u003c/p\u003e\n\u003cp\u003eC. Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3 bucket’s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.\u003c/p\u003e\n\u003cp\u003eD. Use the AWS provided, publicly available ip-ranges.json file to obtain the private IP address of the S3 bucket’s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-215\"\u003eQuestion #215\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-215\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has 700 TB of backup data stored in network attached storage (NAS) in its data center. This backup data need to be accessible for infrequent regulatory requests and must be retained 7 years. The company has decided to migrate this backup data from its data center to AWS. The migration must be complete within 1 month. The company has 500 Mbps of dedicated bandwidth on its public internet connection available for data transfer.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to migrate and store the data at the LOWEST cost?\u003c/p\u003e\n\u003cp\u003eA. Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.\u003c/p\u003e\n\u003cp\u003eB. Deploy a VPN connection between the data center and Amazon VPC. Use the AWS CLI to copy the data from on premises to Amazon S3 Glacier.\u003c/p\u003e\n\u003cp\u003eC. Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.\u003c/p\u003e\n\u003cp\u003eD. Use AWS DataSync to transfer the data and deploy a DataSync agent on premises. Use the DataSync task to copy files from the on-premises NAS storage to Amazon S3 Glacier.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-217\"\u003eQuestion #217\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-217\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company runs a global web application on Amazon EC2 instances behind an Application Load Balancer. The application stores data in Amazon Aurora. The company needs to create a disaster recovery solution and can tolerate up to 30 minutes of downtime and potential data loss. The solution does not need to handle the load when the primary infrastructure is healthy.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in a second AWS Region.\u003c/p\u003e\n\u003cp\u003eB. Host a scaled-down deployment of the application in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora Replica in the second Region.\u003c/p\u003e\n\u003cp\u003eC. Replicate the primary infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora database that is restored from the latest snapshot.\u003c/p\u003e\n\u003cp\u003eD. Back up data with AWS Backup. Use the backup to create the required infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-passive failover. Create an Aurora second primary instance in the second Region.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-218\"\u003eQuestion #218\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-218\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has a web server running on an Amazon EC2 instance in a public subnet with an Elastic IP address. The default security group is assigned to the EC2 instance. The default network ACL has been modified to block all traffic. A solutions architect needs to make the web server accessible from everywhere on port 443.\u003c/p\u003e\n\u003cp\u003eWhich combination of steps will accomplish this task? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.\u003c/p\u003e\n\u003cp\u003eB. Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0.\u003c/p\u003e\n\u003cp\u003eC. Update the network ACL to allow TCP port 443 from source 0.0.0.0/0.\u003c/p\u003e\n\u003cp\u003eD. Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0.\u003c/p\u003e\n\u003cp\u003eE. Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA, E \u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-219\"\u003eQuestion #219\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-219\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company’s application is having performance issues. The application is stateful and needs to complete in-memory tasks on Amazon EC2 instances. The company used AWS CloudFormation to deploy infrastructure and used the M5 EC2 instance family. As traffic increased, the application performance degraded. Users are reporting delays when the users attempt to access the application.\u003c/p\u003e\n\u003cp\u003eWhich solution will resolve these issues in the MOST operationally efficient way?\u003c/p\u003e\n\u003cp\u003eA. Replace the EC2 instances with T3 EC2 instances that run in an Auto Scaling group. Make the changes by using the AWS Management Console.\u003c/p\u003e\n\u003cp\u003eB. Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group. Increase the desired capacity and the maximum capacity of the Auto Scaling group manually when an increase is necessary.\u003c/p\u003e\n\u003cp\u003eC. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon CloudWatch built-in EC2 memory metrics to track the application performance for future capacity planning.\u003c/p\u003e\n\u003cp\u003eD. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eD\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-223\"\u003eQuestion #223\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-223\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has deployed a Java Spring Boot application as a pod that runs on Amazon Elastic Kubernetes Service (Amazon EKS) in private subnets. The application needs to write data to an Amazon DynamoDB table. A solutions architect must ensure that the application can interact with the DynamoDB table without exposing traffic to the internet.\u003c/p\u003e\n\u003cp\u003eWhich combination of steps should the solutions architect take to accomplish this goal? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Attach an IAM role that has sufficient privileges to the EKS pod.\u003c/p\u003e\n\u003cp\u003eB. Attach an IAM user that has sufficient privileges to the EKS pod.\u003c/p\u003e\n\u003cp\u003eC. Allow outbound connectivity to the DynamoDB table through the private subnets’ network ACLs.\u003c/p\u003e\n\u003cp\u003eD. Create a VPC endpoint for DynamoDB.\u003c/p\u003e\n\u003cp\u003eE. Embed the access keys in the Java Spring Boot code.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA, D\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-224\"\u003eQuestion #224\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-224\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company recently migrated its web application to AWS by rehosting the application on Amazon EC2 instances in a single AWS Region. The company wants to redesign its application architecture to be highly available and fault tolerant. Traffic must reach all running EC2 instances randomly.\u003c/p\u003e\n\u003cp\u003eWhich combination of steps should the company take to meet these requirements? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Create an Amazon Route 53 failover routing policy.\u003c/p\u003e\n\u003cp\u003eB. Create an Amazon Route 53 weighted routing policy.\u003c/p\u003e\n\u003cp\u003eC. Create an Amazon Route 53 multivalue answer routing policy.\u003c/p\u003e\n\u003cp\u003eD. Launch three EC2 instances: two instances in one Availability Zone and one instance in another Availability Zone.\u003c/p\u003e\n\u003cp\u003eE. Launch four EC2 instances: two instances in one Availability Zone and two instances in another Availability Zone.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC, E\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-225\"\u003eQuestion #225\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-225\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA media company collects and analyzes user activity data on premises. The company wants to migrate this capability to AWS. The user activity data store will continue to grow and will be petabytes in size. The company needs to build a highly available data ingestion solution that facilitates on-demand analytics of existing data and new data with SQL.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements with the LEAST operational overhead?\u003c/p\u003e\n\u003cp\u003eA. Send activity data to an Amazon Kinesis data stream. Configure the stream to deliver the data to an Amazon S3 bucket.\u003c/p\u003e\n\u003cp\u003eB. Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver the data to an Amazon Redshift cluster.\u003c/p\u003e\n\u003cp\u003eC. Place activity data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on the data as the data arrives in the S3 bucket.\u003c/p\u003e\n\u003cp\u003eD. Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability Zones. Configure the service to forward data to an Amazon RDS Multi-AZ database.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-233\"\u003eQuestion #233\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-233\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has three VPCs named Development, Testing, and Production in the us-east-1 Region. The three VPCs need to be connected to an on-premises data center and are designed to be separate to maintain security and prevent any resource sharing. A solutions architect needs to find a scalable and secure solution.\u003c/p\u003e\n\u003cp\u003eWhat should the solutions architect recommend?\u003c/p\u003e\n\u003cp\u003eA. Create an AWS Direct Connect connection and a VPN connection for each VPC to connect back to the data center.\u003c/p\u003e\n\u003cp\u003eB. Create VPC peers from all the VPCs to the Production VPC. Use an AWS Direct Connect connection from the Production VPC back to the data center.\u003c/p\u003e\n\u003cp\u003eC. Connect VPN connections from all the VPCs to a VPN in the Production VPC. Use a VPN connection from the Production VPC back to the data center.\u003c/p\u003e\n\u003cp\u003eD. Create a new VPC called Network. Within the Network VPC, create an AWS Transit Gateway with an AWS Direct Connect connection back to the data center. Attach all the other VPCs to the Network VPC.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-234\"\u003eQuestion #234\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-234\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is building a new web-based customer relationship management application. The application will use several Amazon EC2 instances that are backed by Amazon Elastic Block Store (Amazon EBS) volumes behind an Application Load Balancer (ALB). The application will also use an Amazon Aurora database. All data for the application must be encrypted at rest and in transit.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Use AWS Key Management Service (AWS KMS) certificates on the ALB to encrypt data in transit. Use AWS Certificate Manager (ACM) to encrypt the EBS volumes and Aurora database storage at rest.\u003c/p\u003e\n\u003cp\u003eB. Use the AWS root account to log in to the AWS Management Console. Upload the company’s encryption certificates. While in the root account, select the option to turn on encryption for all data at rest and in transit for the account.\u003c/p\u003e\n\u003cp\u003eC. Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database storage at rest. Attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in transit.\u003c/p\u003e\n\u003cp\u003eD. Use BitLocker to encrypt all data at rest. Import the company’s TLS certificate keys to AWS Key Management Service (AWS KMS) Attach the KMS keys to the ALB to encrypt data in transit.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-266\"\u003eQuestion #266\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-266\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups configured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect traffic to healthy endpoints.\u003c/p\u003e\n\u003cp\u003eWhich solution meets these requirements?\u003c/p\u003e\n\u003cp\u003eA. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.\u003c/p\u003e\n\u003cp\u003eB. Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.\u003c/p\u003e\n\u003cp\u003eC. Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.\u003c/p\u003e\n\u003cp\u003eD. Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-276\"\u003eQuestion #276\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-276\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An Amazon RDS for Oracle instance is the application’s data layer that uses Oracle-specific PL/SQL functions. Traffic to the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the RDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and defines the minimum healthy instance count only. The company predicts that traffic will continue to increase at a steady but unpredictable rate before leveling off.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to ensure the system can automatically scale for the increased traffic? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Configure storage Auto Scaling on the RDS for Oracle instance.\u003c/p\u003e\n\u003cp\u003eB. Migrate the database to Amazon Aurora to use Auto Scaling storage.\u003c/p\u003e\n\u003cp\u003eC. Configure an alarm on the RDS for Oracle instance for low free storage space.\u003c/p\u003e\n\u003cp\u003eD. Configure the Auto Scaling group to use the average CPU as the scaling metric.\u003c/p\u003e\n\u003cp\u003eE. Configure the Auto Scaling group to use the average free memory as the scaling metric.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA, D\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-287\"\u003eQuestion #287\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-287\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company wants to use specific features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers.\u003c/p\u003e\n\u003cp\u003eHow should a solutions architect design the architecture to meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.\u003c/p\u003e\n\u003cp\u003eB. Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.\u003c/p\u003e\n\u003cp\u003eC. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.\u003c/p\u003e\n\u003cp\u003eD. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-295\"\u003eQuestion #295\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-295\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eAn ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally identifiable information (PII). The company wants to use the data in three applications. Only one of the applications needs to process the PII. The PII must be removed before the other two applications process the data.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements with the LEAST operational overhead?\u003c/p\u003e\n\u003cp\u003eA. Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and process the data that each application requests.\u003c/p\u003e\n\u003cp\u003eB. Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.\u003c/p\u003e\n\u003cp\u003eC. Process the data and store the transformed data in three separate Amazon S3 buckets so that each application has its own custom dataset. Point each application to its respective S3 bucket.\u003c/p\u003e\n\u003cp\u003eD. Process the data and store the transformed data in three separate Amazon DynamoDB tables so that each application has its own custom dataset. Point each application to its respective DynamoDB table.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-327\"\u003eQuestion #327\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-327\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party’s URL. Other internet traffic must be blocked.\u003c/p\u003e\n\u003cp\u003eWhich solution meets these requirements?\u003c/p\u003e\n\u003cp\u003eA. Update the route table for the private subnet to route the outbound traffic to an AWS Network Firewall firewall. Configure domain list rule groups.\u003c/p\u003e\n\u003cp\u003eB. Set up an AWS WAF web ACL. Create a custom set of rules that filter traffic requests based on source and destination IP address range sets.\u003c/p\u003e\n\u003cp\u003eC. Implement strict inbound security group rules. Configure an outbound rule that allows traffic only to the authorized software repositories on the internet by specifying the URLs.\u003c/p\u003e\n\u003cp\u003eD. Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound traffic to the ALB. Use a URL-based rule listener in the ALB’s target group for outbound access to the internet.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-328\"\u003eQuestion #328\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-328\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously.\u003c/p\u003e\n\u003cp\u003eThe company is expecting a significant and sudden increase in the number of sales requests during events for the launch of new products.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect recommend to ensure that all the requests are processed successfully?\u003c/p\u003e\n\u003cp\u003eA. Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in traffic.\u003c/p\u003e\n\u003cp\u003eB. Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network traffic.\u003c/p\u003e\n\u003cp\u003eC. Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce traffic for the API to handle.\u003c/p\u003e\n\u003cp\u003eD. Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eD\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-333\"\u003eQuestion #333\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-333\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company’s application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of every month at midnight, the application becomes much slower when the month-end financial calculation batch runs. This causes the CPU utilization of the EC2 instances to immediately peak to 100%, which disrupts the application.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?\u003c/p\u003e\n\u003cp\u003eA. Configure an Amazon CloudFront distribution in front of the ALB.\u003c/p\u003e\n\u003cp\u003eB. Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.\u003c/p\u003e\n\u003cp\u003eC. Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.\u003c/p\u003e\n\u003cp\u003eD. Configure Amazon ElastiCache to remove some of the workload from the EC2 instances.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-335\"\u003eQuestion #335\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-335\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand.\u003c/p\u003e\n\u003cp\u003eWhich solution meets these requirements?\u003c/p\u003e\n\u003cp\u003eA. Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step Functions to replace the AMI in the Auto Scaling group.\u003c/p\u003e\n\u003cp\u003eB. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.\u003c/p\u003e\n\u003cp\u003eC. Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM). Create an AWS Lambda function that modifies the AMI in the Auto Scaling group.\u003c/p\u003e\n\u003cp\u003eD. Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure Auto Scaling group capacity limits as an event source in EventBridge.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-342\"\u003eQuestion #342\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-342\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA transaction processing company has weekly scripted batch jobs that run on Amazon EC2 instances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, but the baseline CPU utilization that is noted on each run is at least 60%. The company needs to provision the capacity 30 minutes before the jobs run.\u003c/p\u003e\n\u003cp\u003eCurrently, engineers complete this task by manually modifying the Auto Scaling group parameters. The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts. The company needs an automated way to modify the Auto Scaling group’s desired capacity.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements with the LEAST operational overhead?\u003c/p\u003e\n\u003cp\u003eA. Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based on the CPU utilization metric. Set the target value for the metric to 60%.\u003c/p\u003e\n\u003cp\u003eB. Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired capacity, minimum capacity, and maximum capacity. Set the recurrence to weekly. Set the start time to 30 minutes before the batch jobs run.\u003c/p\u003e\n\u003cp\u003eC. Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based on forecast. Set the scaling metric to CPU utilization. Set the target value for the metric to 60%. In the policy, set the instances to pre-launch 30 minutes before the jobs run.\u003c/p\u003e\n\u003cp\u003eD. Create an Amazon EventBridge event to invoke an AWS Lambda function when the CPU utilization metric value for the Auto Scaling group reaches 60%. Configure the Lambda function to increase the Auto Scaling group’s desired capacity and maximum capacity by 20%.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-344\"\u003eQuestion #344\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-344\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements with the FEWEST changes to the code?\u003c/p\u003e\n\u003cp\u003eA. Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.\u003c/p\u003e\n\u003cp\u003eB. Use Amazon EventBridge to post large messages from the application instead of Amazon SQS.\u003c/p\u003e\n\u003cp\u003eC. Change the limit in Amazon SQS to handle messages that are larger than 256 KB.\u003c/p\u003e\n\u003cp\u003eD. Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS). Configure Amazon SQS to reference this location in the messages.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-346\"\u003eQuestion #346\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-346\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has an aging network-attached storage (NAS) array in its data center. The NAS array presents SMB shares and NFS shares to client workstations. The company does not want to purchase a new NAS array. The company also does not want to incur the cost of renewing the NAS array’s support contract. Some of the data is accessed frequently, but much of the data is inactive.\u003c/p\u003e\n\u003cp\u003eA solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 Lifecycle policies, and maintains the same look and feel for the client workstations. The solutions architect has identified AWS Storage Gateway as part of the solution.\u003c/p\u003e\n\u003cp\u003eWhich type of storage gateway should the solutions architect provision to meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Volume Gateway\u003c/p\u003e\n\u003cp\u003eB. Tape Gateway\u003c/p\u003e\n\u003cp\u003eC. Amazon FSx File Gateway\u003c/p\u003e\n\u003cp\u003eD. Amazon S3 File Gateway\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eD\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-349\"\u003eQuestion #349\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-349\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company stores confidential data in an Amazon Aurora PostgreSQL database in the ap-southeast-3 Region. The database is encrypted with an AWS Key Management Service (AWS KMS) customer managed key. The company was recently acquired and must securely share a backup of the database with the acquiring company’s AWS account in ap-southeast-3.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new snapshot with the acquiring company’s AWS account.\u003c/p\u003e\n\u003cp\u003eB. Create a database snapshot. Add the acquiring company’s AWS account to the KMS key policy. Share the snapshot with the acquiring company’s AWS account.\u003c/p\u003e\n\u003cp\u003eC. Create a database snapshot that uses a different AWS managed KMS key. Add the acquiring company’s AWS account to the KMS key alias. Share the snapshot with the acquiring company's AWS account.\u003c/p\u003e\n\u003cp\u003eD. Create a database snapshot. Download the database snapshot. Upload the database snapshot to an Amazon S3 bucket. Update the S3 bucket policy to allow access from the acquiring company’s AWS account.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-353\"\u003eQuestion #353\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-353\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company hosts a three-tier web application on Amazon EC2 instances in a single Availability Zone. The web application uses a self-managed MySQL database that is hosted on an EC2 instance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL database currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects traffic of 1,000 IOPS for both reads and writes at peak traffic.\u003c/p\u003e\n\u003cp\u003eThe company wants to minimize any disruptions, stabilize performance, and reduce costs while retaining the capacity for double the IOPS. The company wants to move the database tier to a fully managed solution that is highly available and fault tolerant.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements MOST cost-effectively?\u003c/p\u003e\n\u003cp\u003eA. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block Express EBS volume.\u003c/p\u003e\n\u003cp\u003eB. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.\u003c/p\u003e\n\u003cp\u003eC. Use Amazon S3 Intelligent-Tiering access tiers.\u003c/p\u003e\n\u003cp\u003eD. Use two large EC2 instances to host the database in active-passive mode.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-363\"\u003eQuestion #363\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-363\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is building a game system that needs to send unique events to separate leaderboard, matchmaking, and authentication services concurrently. The company needs an AWS event-driven system that guarantees the order of the events.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Amazon EventBridge event bus\u003c/p\u003e\n\u003cp\u003eB. Amazon Simple Notification Service (Amazon SNS) FIFO topics\u003c/p\u003e\n\u003cp\u003eC. Amazon Simple Notification Service (Amazon SNS) standard topics\u003c/p\u003e\n\u003cp\u003eD. Amazon Simple Queue Service (Amazon SQS) FIFO queues\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-364\"\u003eQuestion #364\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-364\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA hospital is designing a new application that gathers symptoms from patients. The hospital has decided to use Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) in the architecture.\u003c/p\u003e\n\u003cp\u003eA solutions architect is reviewing the infrastructure design. Data must be encrypted at rest and in transit. Only authorized personnel of the hospital should be able to access the data.\u003c/p\u003e\n\u003cp\u003eWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Turn on server-side encryption on the SQS components. Update the default key policy to restrict key usage to a set of authorized principals.\u003c/p\u003e\n\u003cp\u003eB. Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals.\u003c/p\u003e\n\u003cp\u003eC. Turn on encryption on the SNS components. Update the default key policy to restrict key usage to a set of authorized principals. Set a condition in the topic policy to allow only encrypted connections over TLS.\u003c/p\u003e\n\u003cp\u003eD. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.\u003c/p\u003e\n\u003cp\u003eE. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply an IAM policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB, D \u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-366\"\u003eQuestion #366\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-366\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company’s web application consists of an Amazon API Gateway API in front of an AWS Lambda function and an Amazon DynamoDB database. The Lambda function handles the business logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito user pools to identify the individual users of the application. A solutions architect needs to update the application so that only users who have a subscription can access premium content.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet this requirement with the LEAST operational overhead?\u003c/p\u003e\n\u003cp\u003eA. Enable API caching and throttling on the API Gateway API.\u003c/p\u003e\n\u003cp\u003eB. Set up AWS WAF on the API Gateway API. Create a rule to filter users who have a subscription.\u003c/p\u003e\n\u003cp\u003eC. Apply fine-grained IAM permissions to the premium content in the DynamoDB table.\u003c/p\u003e\n\u003cp\u003eD. Implement API usage plans and API keys to limit the access of users who do not have a subscription.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eD\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-369\"\u003eQuestion #369\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-369\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has migrated an application to Amazon EC2 Linux instances. One of these EC2 instances runs several 1-hour tasks on a schedule. These tasks were written by different teams and have no common programming language. The company is concerned about performance and scalability while these tasks run on a single instance. A solutions architect needs to implement a solution to resolve these concerns.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements with the LEAST operational overhead?\u003c/p\u003e\n\u003cp\u003eA. Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events).\u003c/p\u003e\n\u003cp\u003eB. Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to run the tasks as jobs.\u003c/p\u003e\n\u003cp\u003eC. Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events).\u003c/p\u003e\n\u003cp\u003eD. Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto Scaling group with the AMI to run multiple copies of the instance.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-370\"\u003eQuestion #370\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-370\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company runs a public three-tier web application in a VPC. The application runs on Amazon EC2 instances across multiple Availability Zones. The EC2 instances that run in private subnets need to communicate with a license server over the internet. The company needs a managed solution that minimizes operational maintenance.\u003c/p\u003e\n\u003cp\u003eWhich solution meets these requirements?\u003c/p\u003e\n\u003cp\u003eA. Provision a NAT instance in a public subnet. Modify each private subnet's route table with a default route that points to the NAT instance.\u003c/p\u003e\n\u003cp\u003eB. Provision a NAT instance in a private subnet. Modify each private subnet's route table with a default route that points to the NAT instance.\u003c/p\u003e\n\u003cp\u003eC. Provision a NAT gateway in a public subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.\u003c/p\u003e\n\u003cp\u003eD. Provision a NAT gateway in a private subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-371\"\u003eQuestion #371\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-371\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company needs to create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host a digital media streaming application. The EKS cluster will use a managed node group that is backed by Amazon Elastic Block Store (Amazon EBS) volumes for storage. The company must encrypt all data at rest by using a customer managed key that is stored in AWS Key Management Service (AWS KMS).\u003c/p\u003e\n\u003cp\u003eWhich combination of actions will meet this requirement with the LEAST operational overhead? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Use a Kubernetes plugin that uses the customer managed key to perform data encryption.\u003c/p\u003e\n\u003cp\u003eB. After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the customer managed key.\u003c/p\u003e\n\u003cp\u003eC. Enable EBS encryption by default in the AWS Region where the EKS cluster will be created. Select the customer managed key as the default key.\u003c/p\u003e\n\u003cp\u003eD. Create the EKS cluster. Create an IAM role that has a policy that grants permission to the customer managed key. Associate the role with the EKS cluster.\u003c/p\u003e\n\u003cp\u003eE. Store the customer managed key as a Kubernetes secret in the EKS cluster. Use the customer managed key to encrypt the EBS volumes.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC, D\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-379\"\u003eQuestion #379\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-379\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company hosts a frontend application that uses an Amazon API Gateway API backend that is integrated with AWS Lambda. When the API receives requests, the Lambda function loads many libraries. Then the Lambda function connects to an Amazon RDS database, processes the data, and returns the data to the frontend application. The company wants to ensure that response latency is as low as possible for all its users with the fewest number of changes to the company's operations.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Establish a connection between the frontend application and the database to make queries faster by bypassing the API.\u003c/p\u003e\n\u003cp\u003eB. Configure provisioned concurrency for the Lambda function that handles the requests.\u003c/p\u003e\n\u003cp\u003eC. Cache the results of the queries in Amazon S3 for faster retrieval of similar datasets.\u003c/p\u003e\n\u003cp\u003eD. Increase the size of the database to increase the number of connections Lambda can establish at one time.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-385\"\u003eQuestion #385\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-385\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA solutions architect is creating a new VPC design. There are two public subnets for the load balancer, two private subnets for web servers, and two private subnets for MySQL. The web servers use only HTTPS. The solutions architect has already created a security group for the load balancer allowing port 443 from 0.0.0.0/0. Company policy requires that each resource has the least access required to still be able to perform its tasks.\u003c/p\u003e\n\u003cp\u003eWhich additional configuration strategy should the solutions architect use to meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Create a security group for the web servers and allow port 443 from 0.0.0.0/0. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.\u003c/p\u003e\n\u003cp\u003eB. Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.\u003c/p\u003e\n\u003cp\u003eC. Create a security group for the web servers and allow port 443 from the load balancer. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.\u003c/p\u003e\n\u003cp\u003eD. Create a network ACL for the web servers and allow port 443 from the load balancer. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-388\"\u003eQuestion #388\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-388\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is deploying a two-tier web application in a VPC. The web tier is using an Amazon EC2 Auto Scaling group with public subnets that span multiple Availability Zones. The database tier consists of an Amazon RDS for MySQL DB instance in separate private subnets. The web tier requires access to the database to retrieve product information.\u003c/p\u003e\n\u003cp\u003eThe web application is not working as intended. The web application reports that it cannot connect to the database. The database is confirmed to be up and running. All configurations for the network ACLs, security groups, and route tables are still in their default states.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect recommend to fix the application?\u003c/p\u003e\n\u003cp\u003eA. Add an explicit rule to the private subnet’s network ACL to allow traffic from the web tier’s EC2 instances.\u003c/p\u003e\n\u003cp\u003eB. Add a route in the VPC route table to allow traffic between the web tier’s EC2 instances and the database tier.\u003c/p\u003e\n\u003cp\u003eC. Deploy the web tier's EC2 instances and the database tier’s RDS instance into two separate VPCs, and configure VPC peering.\u003c/p\u003e\n\u003cp\u003eD. Add an inbound rule to the security group of the database tier’s RDS instance to allow traffic from the web tiers security group.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eD\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-390\"\u003eQuestion #390\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-390\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company hosts a three-tier ecommerce application on a fleet of Amazon EC2 instances. The instances run in an Auto Scaling group behind an Application Load Balancer (ALB). All ecommerce data is stored in an Amazon RDS for MariaDB Multi-AZ DB instance.\u003c/p\u003e\n\u003cp\u003eThe company wants to optimize customer session management during transactions. The application must store session data durably.\u003c/p\u003e\n\u003cp\u003eWhich solutions will meet these requirements? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Turn on the sticky sessions feature (session affinity) on the ALB.\u003c/p\u003e\n\u003cp\u003eB. Use an Amazon DynamoDB table to store customer session information.\u003c/p\u003e\n\u003cp\u003eC. Deploy an Amazon Cognito user pool to manage user session information.\u003c/p\u003e\n\u003cp\u003eD. Deploy an Amazon ElastiCache for Redis cluster to store customer session information.\u003c/p\u003e\n\u003cp\u003eE. Use AWS Systems Manager Application Manager in the application to manage user session information.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA, D\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-391\"\u003eQuestion #391\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-391\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company needs a backup strategy for its three-tier stateless web application. The web application runs on Amazon EC2 instances in an Auto Scaling group with a dynamic scaling policy that is configured to respond to scaling events. The database tier runs on Amazon RDS for PostgreSQL. The web application does not require temporary local storage on the EC2 instances. The company’s recovery point objective (RPO) is 2 hours.\u003c/p\u003e\n\u003cp\u003eThe backup strategy must maximize scalability and optimize resource utilization for this environment.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and database every 2 hours to meet the RPO.\u003c/p\u003e\n\u003cp\u003eB. Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS) snapshots. Enable automated backups in Amazon RDS to meet the RPO.\u003c/p\u003e\n\u003cp\u003eC. Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.\u003c/p\u003e\n\u003cp\u003eD. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances every 2 hours. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-399\"\u003eQuestion #399\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-399\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA financial company hosts a web application on AWS. The application uses an Amazon API Gateway Regional API endpoint to give users the ability to retrieve current stock prices. The company’s security team has noticed an increase in the number of API requests. The security team is concerned that HTTP flood attacks might take the application offline.\u003c/p\u003e\n\u003cp\u003eA solutions architect must design a solution to protect the application from this type of attack.\u003c/p\u003e\n\u003cp\u003eWhich solution meets these requirements with the LEAST operational overhead?\u003c/p\u003e\n\u003cp\u003eA. Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24 hours.\u003c/p\u003e\n\u003cp\u003eB. Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.\u003c/p\u003e\n\u003cp\u003eC. Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when the predefined rate is reached.\u003c/p\u003e\n\u003cp\u003eD. Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway Regional API endpoint. Create an AWS Lambda function to block requests from IP addresses that exceed the predefined rate.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-402\"\u003eQuestion #402\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-402\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company needs to ingest and handle large amounts of streaming data that its application generates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is configured with default settings. Every other day, the application consumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the data that the application sends to Kinesis Data Streams.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to resolve this issue?\u003c/p\u003e\n\u003cp\u003eA. Update the Kinesis Data Streams default settings by modifying the data retention period.\u003c/p\u003e\n\u003cp\u003eB. Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.\u003c/p\u003e\n\u003cp\u003eC. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.\u003c/p\u003e\n\u003cp\u003eD. Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-406\"\u003eQuestion #406\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-406\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA solutions architect is designing a two-tiered architecture that includes a public subnet and a database subnet. The web servers in the public subnet must be open to the internet on port 443. The Amazon RDS for MySQL DB instance in the database subnet must be accessible only to the web servers on port 3306.\u003c/p\u003e\n\u003cp\u003eWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Create a network ACL for the public subnet. Add a rule to deny outbound traffic to 0.0.0.0/0 on port 3306.\u003c/p\u003e\n\u003cp\u003eB. Create a security group for the DB instance. Add a rule to allow traffic from the public subnet CIDR block on port 3306.\u003c/p\u003e\n\u003cp\u003eC. Create a security group for the web servers in the public subnet. Add a rule to allow traffic from 0.0.0.0/0 on port 443.\u003c/p\u003e\n\u003cp\u003eD. Create a security group for the DB instance. Add a rule to allow traffic from the web servers’ security group on port 3306.\u003c/p\u003e\n\u003cp\u003eE. Create a security group for the DB instance. Add a rule to deny all traffic except traffic from the web servers’ security group on port 3306.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC, D\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-407\"\u003eQuestion #407\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-407\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is using Site-to-Site VPN connections for secure connectivity to its AWS Cloud resources from on premises. Due to an increase in traffic across the\nVPN connections to the Amazon EC2 instances, users are experiencing slower VPN connectivity.\u003c/p\u003e\n\u003cp\u003eWhich solution will improve the VPN throughput?\u003c/p\u003e\n\u003cp\u003eA. Implement multiple customer gateways for the same network to scale the throughput.\u003c/p\u003e\n\u003cp\u003eB. Use a transit gateway with equal cost multipath routing and add additional VPN tunnels.\u003c/p\u003e\n\u003cp\u003eC. Configure a virtual private gateway with equal cost multipath routing and multiple channels.\u003c/p\u003e\n\u003cp\u003eD. Increase the number of tunnels in the VPN configuration to scale the throughput beyond the default limit.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-410\"\u003eQuestion #410\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-410\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to the EBS volumes is encrypted at rest.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet this requirement?\u003c/p\u003e\n\u003cp\u003eA. Create an IAM role that specifies EBS encryption. Attach the role to the EC2 instances.\u003c/p\u003e\n\u003cp\u003eB. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.\u003c/p\u003e\n\u003cp\u003eC. Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that require encryption at the EBS level.\u003c/p\u003e\n\u003cp\u003eD. Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption in the account. Ensure that the key policy is active.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB \u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-414\"\u003eQuestion #414\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-414\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has a business system that generates hundreds of reports each day. The business system saves the reports to a network share in CSV format. The company needs to store this data in the AWS Cloud in near-real time for analysis.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements with the LEAST administrative overhead?\u003c/p\u003e\n\u003cp\u003eA. Use AWS DataSync to transfer the files to Amazon S3. Create a scheduled task that runs at the end of each day.\u003c/p\u003e\n\u003cp\u003eB. Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.\u003c/p\u003e\n\u003cp\u003eC. Use AWS DataSync to transfer the files to Amazon S3. Create an application that uses the DataSync API in the automation workflow.\u003c/p\u003e\n\u003cp\u003eD. Deploy an AWS Transfer for SFTP endpoint. Create a script that checks for new files on the network share and uploads the new files by using SFTP.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-419\"\u003eQuestion #419\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-419\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 workloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that prevents any resources from being created in any other Region. A security policy requires the company to encrypt all data at rest.\u003c/p\u003e\n\u003cp\u003eAn audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) volumes for EC2 instances without encrypting the volumes. The company wants any new EC2 instances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS volumes. The company wants a solution that will have minimal effect on employees who create EBS volumes.\u003c/p\u003e\n\u003cp\u003eWhich combination of steps will meet these requirements? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key.\u003c/p\u003e\n\u003cp\u003eB. Create an IAM permission boundary. Attach the permission boundary to the root organizational unit (OU). Define the boundary to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.\u003c/p\u003e\n\u003cp\u003eC. Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.\u003c/p\u003e\n\u003cp\u003eD. Update the IAM policies for each account to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.\u003c/p\u003e\n\u003cp\u003eE. In the Organizations management account, specify the Default EBS volume encryption setting.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC, E\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-420\"\u003eQuestion #420\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-420\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company wants to use an Amazon RDS for PostgreSQL DB cluster to simplify time-consuming database administrative tasks for production database workloads. The company wants to ensure that its database is highly available and will provide automatic failover support in most scenarios in less than 40 seconds. The company wants to offload reads off of the primary instance and keep costs as low as possible.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Use an Amazon RDS Multi-AZ DB instance deployment. Create one read replica and point the read workload to the read replica.\u003c/p\u003e\n\u003cp\u003eB. Use an Amazon RDS Multi-AZ DB duster deployment Create two read replicas and point the read workload to the read replicas.\u003c/p\u003e\n\u003cp\u003eC. Use an Amazon RDS Multi-AZ DB instance deployment. Point the read workload to the secondary instances in the Multi-AZ pair.\u003c/p\u003e\n\u003cp\u003eD. Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eD \u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-434\"\u003eQuestion #434\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-434\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in anotherAWS Region with minimal downtime.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to meet these requirements with the LEAST amount of downtime?\u003c/p\u003e\n\u003cp\u003eA. Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.\u003c/p\u003e\n\u003cp\u003eB. Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be launched when needed Configure DNS failover to point to the new disaster recovery Region's load balancer.\u003c/p\u003e\n\u003cp\u003eC. Create an AWS CloudFormation template to create EC2 instances and a load balancer to be launched when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.\u003c/p\u003e\n\u003cp\u003eD. Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-439\"\u003eQuestion #439\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-439\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA solutions architect configured a VPC that has a small range of IP addresses. The number of Amazon EC2 instances that are in the VPC is increasing, and there is an insufficient number of IP addresses for future workloads.\u003c/p\u003e\n\u003cp\u003eWhich solution resolves this issue with the LEAST operational overhead?\u003c/p\u003e\n\u003cp\u003eA. Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC. Create new resources in the new subnets by using the new CIDR.\u003c/p\u003e\n\u003cp\u003eB. Create a second VPC with additional subnets. Use a peering connection to connect the second VPC with the first VPC. Update the routes and create new resources in the subnets of the second VPC.\u003c/p\u003e\n\u003cp\u003eC. Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first VPC. Update the routes of the transit gateway and VPCs. Create new resources in the subnets of the second VPC.\u003c/p\u003e\n\u003cp\u003eD. Create a second VPC. Create a Site-to-Site VPN connection between the first VPC and the second VPC by using a VPN-hosted solution on Amazon EC2 and a virtual private gateway. Update the route between VPCs to the traffic through the VPN. Create new resources in the subnets of the second VPC.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-440\"\u003eQuestion #440\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-440\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company used an Amazon RDS for MySQL DB instance during application testing. Before terminating the DB instance at the end of the test cycle, a solutions architect created two backups. The solutions architect created the first backup by using the mysqldump utility to create a database dump. The solutions architect created the second backup by enabling the final DB snapshot option on RDS termination.\u003c/p\u003e\n\u003cp\u003eThe company is now planning for a new test cycle and wants to create a new DB instance from the most recent backup. The company has chosen a MySQL-compatible edition ofAmazon Aurora to host the DB instance.\u003c/p\u003e\n\u003cp\u003eWhich solutions will create the new DB instance? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Import the RDS snapshot directly into Aurora.\u003c/p\u003e\n\u003cp\u003eB. Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.\u003c/p\u003e\n\u003cp\u003eC. Upload the database dump to Amazon S3. Then import the database dump into Aurora.\u003c/p\u003e\n\u003cp\u003eD. Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora.\u003c/p\u003e\n\u003cp\u003eE. Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS DMS) to import the database dump into Aurora.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA, C\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-442\"\u003eQuestion #442\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-442\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has two VPCs that are located in the us-west-2 Region within the same AWS account. The company needs to allow network traffic between these\nVPCs. Approximately 500 GB of data transfer will occur between the VPCs each month.\u003c/p\u003e\n\u003cp\u003eWhat is the MOST cost-effective solution to connect these VPCs?\u003c/p\u003e\n\u003cp\u003eA. Implement AWS Transit Gateway to connect the VPCs. Update the route tables of each VPC to use the transit gateway for inter-VPC communication.\u003c/p\u003e\n\u003cp\u003eB. Implement an AWS Site-to-Site VPN tunnel between the VPCs. Update the route tables of each VPC to use the VPN tunnel for inter-VPC communication.\u003c/p\u003e\n\u003cp\u003eC. Set up a VPC peering connection between the VPCs. Update the route tables of each VPC to use the VPC peering connection for inter-VPC communication.\u003c/p\u003e\n\u003cp\u003eD. Set up a 1 GB AWS Direct Connect connection between the VPCs. Update the route tables of each VPC to use the Direct Connect connection for inter-VPC communication.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-443\"\u003eQuestion #443\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-443\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company wants to host a scalable web application on AWS. The application will be accessed by users from different geographic regions of the world. Application users will be able to download and upload unique data up to gigabytes in size. The development team wants a cost-effective solution to minimize upload and download latency and maximize performance.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to accomplish this?\u003c/p\u003e\n\u003cp\u003eA. Use Amazon S3 with Transfer Acceleration to host the application.\u003c/p\u003e\n\u003cp\u003eB. Use Amazon S3 with CacheControl headers to host the application.\u003c/p\u003e\n\u003cp\u003eC. Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.\u003c/p\u003e\n\u003cp\u003eD. Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-443-1\"\u003eQuestion #443\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-443-1\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company wants to host a scalable web application on AWS. The application will be accessed by users from different geographic regions of the world. Application users will be able to download and upload unique data up to gigabytes in size. The development team wants a cost-effective solution to minimize upload and download latency and maximize performance.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to accomplish this?\u003c/p\u003e\n\u003cp\u003eA. Use Amazon S3 with Transfer Acceleration to host the application.\u003c/p\u003e\n\u003cp\u003eB. Use Amazon S3 with CacheControl headers to host the application.\u003c/p\u003e\n\u003cp\u003eC. Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.\u003c/p\u003e\n\u003cp\u003eD. Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC \u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-447\"\u003eQuestion #447\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-447\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API Gateway. The company wants to deploy the application across multiple AWS Regions to provide Regional failover capabilities.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to route traffic to multiple Regions?\u003c/p\u003e\n\u003cp\u003eA. Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.\u003c/p\u003e\n\u003cp\u003eB. Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health checks to route traffic.\u003c/p\u003e\n\u003cp\u003eC. Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region. Configure the transit gateway to route requests.\u003c/p\u003e\n\u003cp\u003eD. Create an Application Load Balancer in the primary Region. Set the target group to point to the API Gateway endpoint hostnames in each Region.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-448\"\u003eQuestion #448\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-448\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has two VPCs named Management and Production. The Management VPC uses VPNs through a customer gateway to connect to a single device in the data center. The Production VPC uses a virtual private gateway with two attached AWS Direct Connect connections. The Management and Production VPCs both use a single VPC peering connection to allow communication between the applications.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to mitigate any single point of failure in this architecture?\u003c/p\u003e\n\u003cp\u003eA. Add a set of VPNs between the Management and Production VPCs.\u003c/p\u003e\n\u003cp\u003eB. Add a second virtual private gateway and attach it to the Management VPC.\u003c/p\u003e\n\u003cp\u003eC. Add a second set of VPNs to the Management VPC from a second customer gateway device.\u003c/p\u003e\n\u003cp\u003eD. Add a second VPC peering connection between the Management VPC and the Production VPC.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC \u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-450\"\u003eQuestion #450\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-450\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has a three-tier web application that is in a single server. The company wants to migrate the application to the AWS Cloud. The company also wants the application to align with the AWS Well-Architected Framework and to be consistent with AWS recommended best practices for security, scalability, and resiliency.\u003c/p\u003e\n\u003cp\u003eWhich combination of solutions will meet these requirements? (Choose three.)\u003c/p\u003e\n\u003cp\u003eA. Create a VPC across two Availability Zones with the application's existing architecture. Host the application with existing architecture on an Amazon EC2 instance in a private subnet in each Availability Zone with EC2 Auto Scaling groups. Secure the EC2 instance with security groups and network access control lists (network ACLs).\u003c/p\u003e\n\u003cp\u003eB. Set up security groups and network access control lists (network ACLs) to control access to the database layer. Set up a single Amazon RDS database in a private subnet.\u003c/p\u003e\n\u003cp\u003eC. Create a VPC across two Availability Zones. Refactor the application to host the web tier, application tier, and database tier. Host each tier on its own private subnet with Auto Scaling groups for the web tier and application tier.\u003c/p\u003e\n\u003cp\u003eD. Use a single Amazon RDS database. Allow database access only from the application tier security group.\u003c/p\u003e\n\u003cp\u003eE. Use Elastic Load Balancers in front of the web tier. Control access by using security groups containing references to each layer's security groups.\u003c/p\u003e\n\u003cp\u003eF. Use an Amazon RDS database Multi-AZ cluster deployment in private subnets. Allow database access only from application tier security groups.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC, E, F \u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-451\"\u003eQuestion #451\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-451\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is migrating its applications and databases to the AWS Cloud. The company will use Amazon Elastic Container Service (Amazon ECS), AWS Direct Connect, and Amazon RDS.\u003c/p\u003e\n\u003cp\u003eWhich activities will be managed by the company's operational team? (Choose three.)\u003c/p\u003e\n\u003cp\u003eA. Management of the Amazon RDS infrastructure layer, operating system, and platforms\u003c/p\u003e\n\u003cp\u003eB. Creation of an Amazon RDS DB instance and configuring the scheduled maintenance window\u003c/p\u003e\n\u003cp\u003eC. Configuration of additional software components on Amazon ECS for monitoring, patch management, log management, and host intrusion detection\u003c/p\u003e\n\u003cp\u003eD. Installation of patches for all minor and major database versions for Amazon RDS\u003c/p\u003e\n\u003cp\u003eE. Ensure the physical security of the Amazon RDS infrastructure in the data center\u003c/p\u003e\n\u003cp\u003eF. Encryption of the data that moves in transit through Direct Connect\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB, C, F\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-455\"\u003eQuestion #455\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-455\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company uses AWS Organizations. The company wants to operate some of its AWS accounts with different budgets. The company wants to receive alerts and automatically prevent provisioning of additional resources on AWS accounts when the allocated budget threshold is met during a specific period.\u003c/p\u003e\n\u003cp\u003eWhich combination of solutions will meet these requirements? (Choose three.)\u003c/p\u003e\n\u003cp\u003eA. Use AWS Budgets to create a budget. Set the budget amount under the Cost and Usage Reports section of the required AWS accounts.\u003c/p\u003e\n\u003cp\u003eB. Use AWS Budgets to create a budget. Set the budget amount under the Billing dashboards of the required AWS accounts.\u003c/p\u003e\n\u003cp\u003eC. Create an IAM user for AWS Budgets to run budget actions with the required permissions.\u003c/p\u003e\n\u003cp\u003eD. Create an IAM role for AWS Budgets to run budget actions with the required permissions.\u003c/p\u003e\n\u003cp\u003eE. Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate config rule to prevent provisioning of additional resources.\u003c/p\u003e\n\u003cp\u003eF. Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate service control policy (SCP) to prevent provisioning of additional resources.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB, D, F\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-457\"\u003eQuestion #457\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-457\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company that uses AWS is building an application to transfer data to a product manufacturer. The company has its own identity provider (IdP). The company wants the IdP to authenticate application users while the users use the application to transfer data. The company must use Applicability Statement 2 (AS2) protocol.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Use AWS DataSync to transfer the data. Create an AWS Lambda function for IdP authentication.\u003c/p\u003e\n\u003cp\u003eB. Use Amazon AppFlow flows to transfer the data. Create an Amazon Elastic Container Service (Amazon ECS) task for IdP authentication.\u003c/p\u003e\n\u003cp\u003eC. Use AWS Transfer Family to transfer the data. Create an AWS Lambda function for IdP authentication.\u003c/p\u003e\n\u003cp\u003eD. Use AWS Storage Gateway to transfer the data. Create an Amazon Cognito identity pool for IdP authentication.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-471\"\u003eQuestion #471\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-471\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is creating an application that runs on containers in a VPC. The application stores and accesses data in an Amazon S3 bucket. During the development phase, the application will store and access 1 TB of data in Amazon S3 each day. The company wants to minimize costs and wants to prevent traffic from traversing the internet whenever possible.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Enable S3 Intelligent-Tiering for the S3 bucket\u003c/p\u003e\n\u003cp\u003eB. Enable S3 Transfer Acceleration for the S3 bucket\u003c/p\u003e\n\u003cp\u003eC. Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in the VPC\u003c/p\u003e\n\u003cp\u003eD. Create an interface endpoint for Amazon S3 in the VPC. Associate this endpoint with all route tables in the VPC\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-468\"\u003eQuestion #468\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-468\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is developing a microservices application that will provide a search catalog for customers. The company must use REST APIs to present the frontend of the application to users. The REST APIs must access the backend services that the company hosts in containers in private VPC subnets.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.\u003c/p\u003e\n\u003cp\u003eB. Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.\u003c/p\u003e\n\u003cp\u003eC. Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS.\u003c/p\u003e\n\u003cp\u003eD. Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-474\"\u003eQuestion #474\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-474\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has multiple VPCs across AWS Regions to support and run workloads that are isolated from workloads in other Regions. Because of a recent application launch requirement, the company’s VPCs must communicate with all other VPCs across all Regions.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements with the LEAST amount of administrative effort?\u003c/p\u003e\n\u003cp\u003eA. Use VPC peering to manage VPC communication in a single Region. Use VPC peering across Regions to manage VPC communications.\u003c/p\u003e\n\u003cp\u003eB. Use AWS Direct Connect gateways across all Regions to connect VPCs across regions and manage VPC communications.\u003c/p\u003e\n\u003cp\u003eC. Use AWS Transit Gateway to manage VPC communication in a single Region and Transit Gateway peering across Regions to manage VPC communications.\u003c/p\u003e\n\u003cp\u003eD. Use AWS PrivateLink across all Regions to connect VPCs across Regions and manage VPC communications\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-475\"\u003eQuestion #475\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-475\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is designing a containerized application that will use Amazon Elastic Container Service (Amazon ECS). The application needs to access a shared file system that is highly durable and can recover data to another AWS Region with a recovery point objective (RPO) of 8 hours. The file system needs to provide a mount target m each Availability Zone within a Region.\u003c/p\u003e\n\u003cp\u003eA solutions architect wants to use AWS Backup to manage the replication to another Region.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Amazon FSx for Windows File Server with a Multi-AZ deployment\nB. Amazon FSx for NetApp ONTAP with a Multi-AZ deployment\nC. Amazon Elastic File System (Amazon EFS) with the Standard storage class\nD. Amazon FSx for OpenZFS\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-476\"\u003eQuestion #476\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-476\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is expecting rapid growth in the near future. A solutions architect needs to configure existing users and grant permissions to new users on AWS. The solutions architect has decided to create IAM groups. The solutions architect will add the new users to IAM groups based on department.\u003c/p\u003e\n\u003cp\u003eWhich additional action is the MOST secure way to grant permissions to the new users?\u003c/p\u003e\n\u003cp\u003eA. Apply service control policies (SCPs) to manage access permissions\u003c/p\u003e\n\u003cp\u003eB. Create IAM roles that have least privilege permission. Attach the roles to the IAM groups\u003c/p\u003e\n\u003cp\u003eC. Create an IAM policy that grants least privilege permission. Attach the policy to the IAM groups\u003c/p\u003e\n\u003cp\u003eD. Create IAM roles. Associate the roles with a permissions boundary that defines the maximum permissions\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-479\"\u003eQuestion #479\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-479\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company is making a prototype of the infrastructure for its new website by manually provisioning the necessary infrastructure. This infrastructure includes an Auto Scaling group, an Application Load Balancer and an Amazon RDS database. After the configuration has been thoroughly validated, the company wants the capability to immediately deploy the infrastructure for development and production use in two Availability Zones in an automated fashion.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect recommend to meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Use AWS Systems Manager to replicate and provision the prototype infrastructure in two Availability Zones\u003c/p\u003e\n\u003cp\u003eB. Define the infrastructure as a template by using the prototype infrastructure as a guide. Deploy the infrastructure with AWS CloudFormation.\u003c/p\u003e\n\u003cp\u003eC. Use AWS Config to record the inventory of resources that are used in the prototype infrastructure. Use AWS Config to deploy the prototype infrastructure into two Availability Zones.\u003c/p\u003e\n\u003cp\u003eD. Use AWS Elastic Beanstalk and configure it to use an automated reference to the prototype infrastructure to automatically deploy new environments in two Availability Zones.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-492\"\u003eQuestion #492\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-492\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has multiple AWS accounts for development work. Some staff consistently use oversized Amazon EC2 instances, which causes the company to exceed the yearly budget for the development accounts. The company wants to centrally restrict the creation of AWS resources in these accounts.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements with the LEAST development effort?\u003c/p\u003e\n\u003cp\u003eA. Develop AWS Systems Manager templates that use an approved EC2 creation process. Use the approved Systems Manager templates to provision EC2 instances.\u003c/p\u003e\n\u003cp\u003eB. Use AWS Organizations to organize the accounts into organizational units (OUs). Define and attach a service control policy (SCP) to control the usage of EC2 instance types.\u003c/p\u003e\n\u003cp\u003eC. Configure an Amazon EventBridge rule that invokes an AWS Lambda function when an EC2 instance is created. Stop disallowed EC2 instance types.\u003c/p\u003e\n\u003cp\u003eD. Set up AWS Service Catalog products for the staff to create the allowed EC2 instance types. Ensure that staff can deploy EC2 instances only by using the Service Catalog products.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eB\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-500\"\u003eQuestion #500\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-500\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has multiple Windows file servers on premises. The company wants to migrate and consolidate its files into an Amazon FSx for Windows File Server file system. File permissions must be preserved to ensure that access rights do not change.\u003c/p\u003e\n\u003cp\u003eWhich solutions will meet these requirements? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Deploy AWS DataSync agents on premises. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.\u003c/p\u003e\n\u003cp\u003eB. Copy the shares on each file server into Amazon S3 buckets by using the AWS CLI. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.\u003c/p\u003e\n\u003cp\u003eC. Remove the drives from each file server. Ship the drives to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.\u003c/p\u003e\n\u003cp\u003eD. Order an AWS Snowcone device. Connect the device to the on-premises network. Launch AWS DataSync agents on the device. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.\u003c/p\u003e\n\u003cp\u003eE. Order an AWS Snowball Edge Storage Optimized device. Connect the device to the on-premises network. Copy data to the device by using the AWS CLI. Ship the device back to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA, D\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-502\"\u003eQuestion #502\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-502\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company runs a website that uses a content management system (CMS) on Amazon EC2. The CMS runs on a single EC2 instance and uses an Amazon Aurora MySQL Multi-AZ DB instance for the data tier. Website images are stored on an Amazon Elastic Block Store (Amazon EBS) volume that is mounted inside the EC2 instance.\u003c/p\u003e\n\u003cp\u003eWhich combination of actions should a solutions architect take to improve the performance and resilience of the website? (Choose two.)\u003c/p\u003e\n\u003cp\u003eA. Move the website images into an Amazon S3 bucket that is mounted on every EC2 instance\u003c/p\u003e\n\u003cp\u003eB. Share the website images by using an NFS share from the primary EC2 instance. Mount this share on the other EC2 instances.\u003c/p\u003e\n\u003cp\u003eC. Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.\u003c/p\u003e\n\u003cp\u003eD. Create an Amazon Machine Image (AMI) from the existing EC2 instance. Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group. Configure the Auto Scaling group to maintain a minimum of two instances. Configure an accelerator in AWS Global Accelerator for the website\u003c/p\u003e\n\u003cp\u003eE. Create an Amazon Machine Image (AMI) from the existing EC2 instance. Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group. Configure the Auto Scaling group to maintain a minimum of two instances. Configure an Amazon CloudFront distribution for the website.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA, E\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-504\"\u003eQuestion #504\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-504\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company needs to connect several VPCs in the us-east-1 Region that span hundreds of AWS accounts. The company's networking team has its own AWS account to manage the cloud network.\u003c/p\u003e\n\u003cp\u003eWhat is the MOST operationally efficient solution to connect the VPCs?\u003c/p\u003e\n\u003cp\u003eA. Set up VPC peering connections between each VPC. Update each associated subnet’s route table\u003c/p\u003e\n\u003cp\u003eB. Configure a NAT gateway and an internet gateway in each VPC to connect each VPC through the internet\u003c/p\u003e\n\u003cp\u003eC. Create an AWS Transit Gateway in the networking team’s AWS account. Configure static routes from each VPC.\u003c/p\u003e\n\u003cp\u003eD. Deploy VPN gateways in each VPC. Create a transit VPC in the networking team’s AWS account to connect to each VPC.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-544\"\u003eQuestion #544\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-544\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA retail company uses a regional Amazon API Gateway API for its public REST APIs. The API Gateway endpoint is a custom domain name that points to an Amazon Route 53 alias record. A solutions architect needs to create a solution that has minimal effects on customers and minimal data loss to release the new version of APIs.\u003c/p\u003e\n\u003cp\u003eWhich solution will meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Create a canary release deployment stage for API Gateway. Deploy the latest API version. Point an appropriate percentage of traffic to the canary stage. After API verification, promote the canary stage to the production stage.\u003c/p\u003e\n\u003cp\u003eB. Create a new API Gateway endpoint with a new version of the API in OpenAPI YAML file format. Use the import-to-update operation in merge mode into the API in API Gateway. Deploy the new version of the API to the production stage.\u003c/p\u003e\n\u003cp\u003eC. Create a new API Gateway endpoint with a new version of the API in OpenAPI JSON file format. Use the import-to-update operation in overwrite mode into the API in API Gateway. Deploy the new version of the API to the production stage.\u003c/p\u003e\n\u003cp\u003eD. Create a new API Gateway endpoint with new versions of the API definitions. Create a custom domain name for the new API Gateway API. Point the Route 53 alias record to the new API Gateway API custom domain name.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003chr\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch3 id=\"question-558\"\u003eQuestion #558\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-558\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has two VPCs that are located in the us-west-2 Region within the same AWS account. The company needs to allow network traffic between these VPCs. Approximately 500 GB of data transfer will occur between the VPCs each month.\u003c/p\u003e\n\u003cp\u003eWhat is the MOST cost-effective solution to connect these VPCs?\u003c/p\u003e\n\u003cp\u003eA. Implement AWS Transit Gateway to connect the VPCs. Update the route tables of each VPC to use the transit gateway for inter-VPC communication.\u003c/p\u003e\n\u003cp\u003eB. Implement an AWS Site-to-Site VPN tunnel between the VPCs. Update the route tables of each VPC to use the VPN tunnel for inter-VPC communication.\u003c/p\u003e\n\u003cp\u003eC. Set up a VPC peering connection between the VPCs. Update the route tables of each VPC to use the VPC peering connection for inter-VPC communication.\u003c/p\u003e\n\u003cp\u003eD. Set up a 1 GB AWS Direct Connect connection between the VPCs. Update the route tables of each VPC to use the Direct Connect connection for inter-VPC communication.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eC\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cbr\u003e\n\u003ch3 id=\"question-608\"\u003eQuestion #608\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#question-608\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA company has an application that serves clients that are deployed in more than 20.000 retail storefront locations around the world. The application consists of backend web services that are exposed over HTTPS on port 443. The application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The retail locations communicate with the web application over the public internet. The company allows each retail location to register the IP address that the retail location has been allocated by its local ISP.\u003c/p\u003e\n\u003cp\u003eThe company's security team recommends to increase the security of the application endpoint by restricting access to only the IP addresses registered by the retail locations.\u003c/p\u003e\n\u003cp\u003eWhat should a solutions architect do to meet these requirements?\u003c/p\u003e\n\u003cp\u003eA. Associate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter traffic. Update the IP addresses in the rule to include the registered IP addresses.\u003c/p\u003e\n\u003cp\u003eB. Deploy AWS Firewall Manager to manage the ALConfigure firewall rules to restrict traffic to the ALModify the firewall rules to include the registered IP addresses.\u003c/p\u003e\n\u003cp\u003eC. Store the IP addresses in an Amazon DynamoDB table. Configure an AWS Lambda authorization function on the ALB to validate that incoming requests are from the registered IP addresses.\u003c/p\u003e\n\u003cp\u003eD. Configure the network ACL on the subnet that contains the public interface of the ALB. Update the ingress rules on the network ACL with entries for each of the registered IP addresses.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow Answer\u003c/summary\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/details\u003e\n\u003cbr\u003e\n\u003ch2 id=\"references\"\u003eReferences\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#references\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate-saa-c03/\"\u003ehttps://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate-saa-c03/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"rfaecb1r4pxpahsrighqpfi","title":"Dev","desc":"","updated":1666489926104,"created":1666489926104,"custom":{"nav_order":0,"permalink":"/"},"fname":"dev","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"5f3d9b993bc1c60c0e10119b00a45c49","links":[],"anchors":{},"children":["vmcgt1bw13lwsobet50r3vl","pj0dbesw2qm5j1tub9f34pg","e40hlh2wukm75vo4t8r9c7p","7xwnx64er01irhu8wg1q5zm","4buyekecqhzdg87aqxmh1rz","n1xmefdyaaztqb8wmsmqtjj","d8p9jq81rc0pfbbuzz7prah","yq2839iijurmzkrwkqwo9ps","51sfhirev9l2r8bef9utfb9","xuw6kyksesfaatsmt3gctyi","njc2zpmxz8izls7ogwdj20h","d3ma1d2r59fhejrl6bn9g0e","tgd9pt1fu1j4ejm4g09vlx4","5jlkrg4hx6dpy5tr55grkh6","ockg1gb78gpshwty5zjmvue","owsm2ti22dslhpk9oduhhku","52x2zgp7jpygai1h0jpv0t8","2d5oqog2rsdfdogne54tq9t"],"parent":null,"data":{},"body":""},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":".","selfContained":true,"name":"Dendron"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["dev","about"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Golbang Hacker","description":"Personal Knowledge Space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"search","assetsPrefix":"/pks-publish","siteUrl":"https://b2sdev.github.io","theme":"dark","siteFaviconPath":"favicon.ico","siteIndex":"dev"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"csls1iecmzveuq2w6ym65k0"},"buildId":"MBpMZiPEoVJLwa6Uvfsj2","assetPrefix":"/pks-publish","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>